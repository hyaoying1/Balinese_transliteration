{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import editdistance\n",
    "\n",
    "from PIL import Image\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "import timm  \n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 39\n",
      "Training size: 13972; Validation size: 1050\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "base_dir = os.getcwd()\n",
    "\n",
    "# Same paths as your original code\n",
    "ground_truth_path = os.path.join(base_dir, 'balinese_transliteration_train.txt') \n",
    "images_dir        = os.path.join(base_dir, 'balinese_word_train')\n",
    "\n",
    "filenames = []\n",
    "labels    = []\n",
    "\n",
    "with open(ground_truth_path, 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        line = line.strip()\n",
    "        if line:  # Ensure the line is not empty\n",
    "            parts = line.split(';')\n",
    "            if len(parts) == 2:\n",
    "                filename, label = parts\n",
    "                label = label.lower()\n",
    "                filenames.append(filename)\n",
    "                labels.append(label)\n",
    "            else:\n",
    "                print(f\"Skipping malformed line: {line}\")\n",
    "\n",
    "data = pd.DataFrame({\n",
    "    'filename': filenames,\n",
    "    'label': labels\n",
    "})\n",
    "\n",
    "label_counts = data['label'].value_counts()\n",
    "\n",
    "all_text = ''.join(data['label'])\n",
    "unique_chars = sorted(list(set(all_text)))\n",
    "\n",
    "# Create character->index starting from 1\n",
    "char_to_idx = {char: idx + 1 for idx, char in enumerate(unique_chars)}\n",
    "# Add special tokens\n",
    "char_to_idx['<PAD>'] = 0\n",
    "char_to_idx['<UNK>'] = len(char_to_idx)\n",
    "char_to_idx['<SOS>'] = len(char_to_idx)\n",
    "char_to_idx['<EOS>'] = len(char_to_idx)\n",
    "\n",
    "# Reverse mapping\n",
    "idx_to_char = {v: k for k, v in char_to_idx.items()}\n",
    "\n",
    "vocab_size = len(char_to_idx)\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "\n",
    "def encode_label(label, char_to_idx, max_length):\n",
    "    \"\"\"\n",
    "    Converts a label (string) into a list of indices with <SOS>, <EOS>, padding, etc.\n",
    "    \"\"\"\n",
    "    encoded = (\n",
    "        [char_to_idx['<SOS>']] +\n",
    "        [char_to_idx.get(ch, char_to_idx['<UNK>']) for ch in label] +\n",
    "        [char_to_idx['<EOS>']]\n",
    "    )\n",
    "    # Pad if needed\n",
    "    if len(encoded) < max_length:\n",
    "        encoded += [char_to_idx['<PAD>']] * (max_length - len(encoded))\n",
    "    else:\n",
    "        encoded = encoded[:max_length]\n",
    "    return encoded\n",
    "\n",
    "max_label_length = max(len(label) for label in data['label']) + 2  # +2 for <SOS> and <EOS>\n",
    "data['encoded_label'] = data['label'].apply(lambda x: encode_label(x, char_to_idx, max_label_length))\n",
    "data['label_length']  = data['label'].apply(len)\n",
    "\n",
    "rare_labels = label_counts[label_counts < 3].index  # NEW: words that appear <3 times\n",
    "\n",
    "def custom_split(df, rare_label_list, test_size=0.1, random_state=42):\n",
    "    # Separate rare words from frequent ones\n",
    "    rare_df     = df[df['label'].isin(rare_label_list)]\n",
    "    non_rare_df = df[~df['label'].isin(rare_label_list)]\n",
    "\n",
    "    #  train/val split for non-rare\n",
    "    train_nr, val_nr = train_test_split(non_rare_df, test_size=test_size, \n",
    "                                        random_state=random_state)\n",
    "\n",
    "    # Combine rare samples entirely into training\n",
    "    train_df = pd.concat([train_nr, rare_df], ignore_index=True)\n",
    "    # Shuffle after combining\n",
    "    train_df = train_df.sample(frac=1, random_state=random_state).reset_index(drop=True)\n",
    "\n",
    "    val_df = val_nr.reset_index(drop=True)\n",
    "    return train_df, val_df\n",
    "\n",
    "# Call custom_split instead of direct train_test_split\n",
    "train_data, val_data = custom_split(data, rare_labels, test_size=0.1, random_state=42) \n",
    "\n",
    "print(f\"Training size: {len(train_data)}; Validation size: {len(val_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BalineseDataset(Dataset):\n",
    "    def __init__(self, df, images_dir, transform=None):\n",
    "        self.data       = df.reset_index(drop=True)\n",
    "        self.images_dir = images_dir\n",
    "        self.transform  = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name     = self.data.loc[idx, 'filename']\n",
    "        label        = self.data.loc[idx, 'encoded_label']\n",
    "        label_length = self.data.loc[idx, 'label_length']\n",
    "\n",
    "        img_path = os.path.join(self.images_dir, img_name)\n",
    "        image    = Image.open(img_path).convert('RGB')\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        label = torch.tensor(label, dtype=torch.long)\n",
    "        return image, label, torch.tensor(label_length, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=(0.5, 0.5, 0.5),\n",
    "        std=(0.5, 0.5, 0.5)\n",
    "    )\n",
    "])\n",
    "\n",
    "train_dataset = BalineseDataset(train_data, images_dir, transform=transform)\n",
    "val_dataset   = BalineseDataset(val_data,   images_dir, transform=transform)\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,  num_workers=2, pin_memory=True)\n",
    "val_loader   = DataLoader(val_dataset,   batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/30\n",
      "  Batch 50/437 - Loss: 2.4494\n",
      "  Batch 100/437 - Loss: 2.3068\n",
      "  Batch 150/437 - Loss: 2.0392\n",
      "  Batch 200/437 - Loss: 1.8417\n",
      "  Batch 250/437 - Loss: 1.7403\n",
      "  Batch 300/437 - Loss: 1.7022\n",
      "  Batch 350/437 - Loss: 0.9931\n",
      "  Batch 400/437 - Loss: 1.3448\n",
      "Sample 1:\n",
      "Predicted: ,<EOS>\n",
      "Target   : ,<EOS>\n",
      "\n",
      "Sample 2:\n",
      "Predicted: suura<EOS>\n",
      "Target   : swaha<EOS>\n",
      "\n",
      "Sample 3:\n",
      "Predicted: suaha<EOS>\n",
      "Target   : swaha<EOS>\n",
      "\n",
      "\n",
      "=== Top 5 Worst Samples by CER ===\n",
      "[1] CER: 1.778\n",
      "   Predicted: .<EOS><EOS><EOS><EOS>\n",
      "   Ground Truth: ring<EOS>\n",
      "\n",
      "[2] CER: 1.700\n",
      "   Predicted: aa<EOS><EOS><EOS><EOS>\n",
      "   Ground Truth: irung<EOS>\n",
      "\n",
      "[3] CER: 1.500\n",
      "   Predicted: aasa<EOS><EOS><EOS><EOS>\n",
      "   Ground Truth: tanhana<EOS>\n",
      "\n",
      "[4] CER: 1.471\n",
      "   Predicted: pwiiiign<EOS><EOS><EOS><EOS><EOS>\n",
      "   Ground Truth: awighnamastu<EOS>\n",
      "\n",
      "[5] CER: 1.300\n",
      "   Predicted: a<EOS><EOS>aa<EOS>\n",
      "   Ground Truth: panti<EOS>\n",
      "\n",
      "[1/30] Train Loss: 1.8378, Train CER: 0.6011 | Val Loss: 0.8633, Val CER: 0.2567\n",
      "\n",
      "Epoch 2/30\n",
      "  Batch 50/437 - Loss: 0.7274\n",
      "  Batch 100/437 - Loss: 1.1416\n",
      "  Batch 150/437 - Loss: 0.6361\n",
      "  Batch 200/437 - Loss: 0.8656\n",
      "  Batch 250/437 - Loss: 0.7729\n",
      "  Batch 300/437 - Loss: 0.9456\n",
      "  Batch 350/437 - Loss: 0.5864\n",
      "  Batch 400/437 - Loss: 0.7534\n",
      "Sample 1:\n",
      "Predicted: ,<EOS>\n",
      "Target   : ,<EOS>\n",
      "\n",
      "Sample 2:\n",
      "Predicted: sua<EOS>a<EOS>\n",
      "Target   : swaha<EOS>\n",
      "\n",
      "Sample 3:\n",
      "Predicted: suaha<EOS>\n",
      "Target   : swaha<EOS>\n",
      "\n",
      "\n",
      "=== Top 5 Worst Samples by CER ===\n",
      "[1] CER: 1.333\n",
      "   Predicted: ki<EOS><EOS><EOS>\n",
      "   Ground Truth: nang<EOS>\n",
      "\n",
      "[2] CER: 1.222\n",
      "   Predicted: ta<EOS><EOS><EOS>\n",
      "   Ground Truth: wara<EOS>\n",
      "\n",
      "[3] CER: 1.111\n",
      "   Predicted: si<EOS><EOS><EOS>\n",
      "   Ground Truth: sira<EOS>\n",
      "\n",
      "[4] CER: 1.111\n",
      "   Predicted: ra<EOS><EOS><EOS>\n",
      "   Ground Truth: rasa<EOS>\n",
      "\n",
      "[5] CER: 1.111\n",
      "   Predicted: ri<EOS><EOS><EOS>\n",
      "   Ground Truth: riya<EOS>\n",
      "\n",
      "[2/30] Train Loss: 0.8684, Train CER: 0.2619 | Val Loss: 0.4827, Val CER: 0.1384\n",
      "\n",
      "Epoch 3/30\n",
      "  Batch 50/437 - Loss: 0.7661\n",
      "  Batch 100/437 - Loss: 0.6100\n",
      "  Batch 150/437 - Loss: 0.5568\n",
      "  Batch 200/437 - Loss: 0.8699\n",
      "  Batch 250/437 - Loss: 0.6523\n",
      "  Batch 300/437 - Loss: 0.3564\n",
      "  Batch 350/437 - Loss: 0.4965\n",
      "  Batch 400/437 - Loss: 0.3933\n",
      "Sample 1:\n",
      "Predicted: ,<EOS>\n",
      "Target   : ,<EOS>\n",
      "\n",
      "Sample 2:\n",
      "Predicted: swaha<EOS>\n",
      "Target   : swaha<EOS>\n",
      "\n",
      "Sample 3:\n",
      "Predicted: swaha<EOS>\n",
      "Target   : swaha<EOS>\n",
      "\n",
      "\n",
      "=== Top 5 Worst Samples by CER ===\n",
      "[1] CER: 1.600\n",
      "   Predicted: ra<EOS><EOS><EOS><EOS>\n",
      "   Ground Truth: panti<EOS>\n",
      "\n",
      "[2] CER: 1.375\n",
      "   Predicted: a<EOS><EOS><EOS>\n",
      "   Ground Truth: nga<EOS>\n",
      "\n",
      "[3] CER: 1.375\n",
      "   Predicted: a<EOS><EOS><EOS>\n",
      "   Ground Truth: sha<EOS>\n",
      "\n",
      "[4] CER: 1.250\n",
      "   Predicted: e<EOS><EOS><EOS>\n",
      "   Ground Truth: eda<EOS>\n",
      "\n",
      "[5] CER: 1.176\n",
      "   Predicted: awilemes<EOS>s<EOS><EOS><EOS>\n",
      "   Ground Truth: awighnamastu<EOS>\n",
      "\n",
      "[3/30] Train Loss: 0.5774, Train CER: 0.1641 | Val Loss: 0.3277, Val CER: 0.0906\n",
      "\n",
      "Epoch 4/30\n",
      "  Batch 50/437 - Loss: 0.3927\n",
      "  Batch 100/437 - Loss: 0.3482\n",
      "  Batch 150/437 - Loss: 0.2933\n",
      "  Batch 200/437 - Loss: 0.3986\n",
      "  Batch 250/437 - Loss: 0.7211\n",
      "  Batch 300/437 - Loss: 0.5535\n",
      "  Batch 350/437 - Loss: 0.5622\n",
      "  Batch 400/437 - Loss: 0.4882\n",
      "Sample 1:\n",
      "Predicted: ,<EOS>\n",
      "Target   : ,<EOS>\n",
      "\n",
      "Sample 2:\n",
      "Predicted: swaha<EOS>\n",
      "Target   : swaha<EOS>\n",
      "\n",
      "Sample 3:\n",
      "Predicted: aaswah\n",
      "Target   : swaha<EOS>\n",
      "\n",
      "\n",
      "=== Top 5 Worst Samples by CER ===\n",
      "[1] CER: 1.333\n",
      "   Predicted: ran<EOS>a<EOS><EOS><EOS>\n",
      "   Ground Truth: rendang<EOS>\n",
      "\n",
      "[2] CER: 1.200\n",
      "   Predicted: .<EOS><EOS>ta<EOS>\n",
      "   Ground Truth: panti<EOS>\n",
      "\n",
      "[3] CER: 1.100\n",
      "   Predicted: ira<EOS><EOS><EOS>\n",
      "   Ground Truth: irung<EOS>\n",
      "\n",
      "[4] CER: 1.000\n",
      "   Predicted: nw\n",
      "   Ground Truth: 3<EOS>\n",
      "\n",
      "[5] CER: 1.000\n",
      "   Predicted: maka\n",
      "   Ground Truth: iki<EOS>\n",
      "\n",
      "[4/30] Train Loss: 0.4452, Train CER: 0.1204 | Val Loss: 0.3022, Val CER: 0.0831\n",
      "\n",
      "Epoch 5/30\n",
      "  Batch 50/437 - Loss: 0.1409\n",
      "  Batch 100/437 - Loss: 0.2332\n",
      "  Batch 150/437 - Loss: 0.6286\n",
      "  Batch 200/437 - Loss: 0.4222\n",
      "  Batch 250/437 - Loss: 0.1876\n",
      "  Batch 300/437 - Loss: 0.5199\n",
      "  Batch 350/437 - Loss: 0.3913\n",
      "  Batch 400/437 - Loss: 0.5438\n",
      "Sample 1:\n",
      "Predicted: ,<EOS>\n",
      "Target   : ,<EOS>\n",
      "\n",
      "Sample 2:\n",
      "Predicted: swaha<EOS>\n",
      "Target   : swaha<EOS>\n",
      "\n",
      "Sample 3:\n",
      "Predicted: seh<EOS>a<EOS>\n",
      "Target   : swaha<EOS>\n",
      "\n",
      "\n",
      "=== Top 5 Worst Samples by CER ===\n",
      "[1] CER: 1.200\n",
      "   Predicted: jut<EOS><EOS><EOS>\n",
      "   Ground Truth: jagat<EOS>\n",
      "\n",
      "[2] CER: 1.167\n",
      "   Predicted: ngken<EOS><EOS><EOS>\n",
      "   Ground Truth: langkir<EOS>\n",
      "\n",
      "[3] CER: 1.111\n",
      "   Predicted: ri<EOS><EOS><EOS>\n",
      "   Ground Truth: riya<EOS>\n",
      "\n",
      "[4] CER: 1.100\n",
      "   Predicted: .a<EOS>t<EOS><EOS>\n",
      "   Ground Truth: panti<EOS>\n",
      "\n",
      "[5] CER: 1.083\n",
      "   Predicted: bangn<EOS><EOS><EOS>\n",
      "   Ground Truth: babakan<EOS>\n",
      "\n",
      "[5/30] Train Loss: 0.3530, Train CER: 0.0937 | Val Loss: 0.2873, Val CER: 0.0789\n",
      "\n",
      "Epoch 6/30\n",
      "  Batch 50/437 - Loss: 0.3346\n",
      "  Batch 100/437 - Loss: 0.2817\n",
      "  Batch 150/437 - Loss: 0.4492\n",
      "  Batch 200/437 - Loss: 0.2528\n",
      "  Batch 250/437 - Loss: 0.3276\n",
      "  Batch 300/437 - Loss: 0.1189\n",
      "  Batch 350/437 - Loss: 0.2263\n",
      "  Batch 400/437 - Loss: 0.3309\n",
      "Sample 1:\n",
      "Predicted: ,<EOS>\n",
      "Target   : ,<EOS>\n",
      "\n",
      "Sample 2:\n",
      "Predicted: swaha<EOS>\n",
      "Target   : swaha<EOS>\n",
      "\n",
      "Sample 3:\n",
      "Predicted: swaha<EOS>\n",
      "Target   : swaha<EOS>\n",
      "\n",
      "\n",
      "=== Top 5 Worst Samples by CER ===\n",
      "[1] CER: 2.100\n",
      "   Predicted: .<EOS><EOS><EOS><EOS><EOS>\n",
      "   Ground Truth: panti<EOS>\n",
      "\n",
      "[2] CER: 1.200\n",
      "   Predicted: hin<EOS><EOS><EOS>\n",
      "   Ground Truth: gring<EOS>\n",
      "\n",
      "[3] CER: 1.083\n",
      "   Predicted: takak<EOS><EOS><EOS>\n",
      "   Ground Truth: tanhana<EOS>\n",
      "\n",
      "[4] CER: 1.000\n",
      "   Predicted: nga\n",
      "   Ground Truth: la<EOS>\n",
      "\n",
      "[5] CER: 1.000\n",
      "   Predicted: wad\n",
      "   Ground Truth: di<EOS>\n",
      "\n",
      "[6/30] Train Loss: 0.2898, Train CER: 0.0768 | Val Loss: 0.2421, Val CER: 0.0677\n",
      "\n",
      "Epoch 7/30\n",
      "  Batch 50/437 - Loss: 0.2887\n",
      "  Batch 100/437 - Loss: 0.1442\n",
      "  Batch 150/437 - Loss: 0.2454\n",
      "  Batch 200/437 - Loss: 0.0544\n",
      "  Batch 250/437 - Loss: 0.4502\n",
      "  Batch 300/437 - Loss: 0.1071\n",
      "  Batch 350/437 - Loss: 0.5828\n",
      "  Batch 400/437 - Loss: 0.2059\n",
      "Sample 1:\n",
      "Predicted: ,<EOS>\n",
      "Target   : ,<EOS>\n",
      "\n",
      "Sample 2:\n",
      "Predicted: swaha<EOS>\n",
      "Target   : swaha<EOS>\n",
      "\n",
      "Sample 3:\n",
      "Predicted: swaha<EOS>\n",
      "Target   : swaha<EOS>\n",
      "\n",
      "\n",
      "=== Top 5 Worst Samples by CER ===\n",
      "[1] CER: 1.111\n",
      "   Predicted: su<EOS><EOS><EOS>\n",
      "   Ground Truth: susu<EOS>\n",
      "\n",
      "[2] CER: 1.100\n",
      "   Predicted: wan<EOS><EOS><EOS>\n",
      "   Ground Truth: warna<EOS>\n",
      "\n",
      "[3] CER: 1.000\n",
      "   Predicted: banan<EOS><EOS><EOS>\n",
      "   Ground Truth: tanhana<EOS>\n",
      "\n",
      "[4] CER: 1.000\n",
      "   Predicted: wac\n",
      "   Ground Truth: di<EOS>\n",
      "\n",
      "[5] CER: 1.000\n",
      "   Predicted: iru<EOS><EOS><EOS>\n",
      "   Ground Truth: irung<EOS>\n",
      "\n",
      "[7/30] Train Loss: 0.2530, Train CER: 0.0670 | Val Loss: 0.2631, Val CER: 0.0676\n",
      "No improvement for 1 epoch(s).\n",
      "\n",
      "Epoch 8/30\n",
      "  Batch 50/437 - Loss: 0.3269\n",
      "  Batch 100/437 - Loss: 0.1038\n",
      "  Batch 150/437 - Loss: 0.1883\n",
      "  Batch 200/437 - Loss: 0.3173\n",
      "  Batch 250/437 - Loss: 0.1934\n",
      "  Batch 300/437 - Loss: 0.2391\n",
      "  Batch 350/437 - Loss: 0.3326\n",
      "  Batch 400/437 - Loss: 0.4689\n",
      "Sample 1:\n",
      "Predicted: ,<EOS>\n",
      "Target   : ,<EOS>\n",
      "\n",
      "Sample 2:\n",
      "Predicted: swaha<EOS>\n",
      "Target   : swaha<EOS>\n",
      "\n",
      "Sample 3:\n",
      "Predicted: swaha<EOS>\n",
      "Target   : swaha<EOS>\n",
      "\n",
      "\n",
      "=== Top 5 Worst Samples by CER ===\n",
      "[1] CER: 1.417\n",
      "   Predicted: taka<EOS><EOS><EOS><EOS>\n",
      "   Ground Truth: tanhana<EOS>\n",
      "\n",
      "[2] CER: 1.200\n",
      "   Predicted: rum<EOS><EOS><EOS>\n",
      "   Ground Truth: utama<EOS>\n",
      "\n",
      "[3] CER: 1.100\n",
      "   Predicted: ira<EOS><EOS><EOS>\n",
      "   Ground Truth: irung<EOS>\n",
      "\n",
      "[4] CER: 1.083\n",
      "   Predicted: daari<EOS><EOS><EOS>\n",
      "   Ground Truth: bhatari<EOS>\n",
      "\n",
      "[5] CER: 1.000\n",
      "   Predicted: wad\n",
      "   Ground Truth: di<EOS>\n",
      "\n",
      "[8/30] Train Loss: 0.2556, Train CER: 0.0659 | Val Loss: 0.2461, Val CER: 0.0597\n",
      "No improvement for 2 epoch(s).\n",
      "\n",
      "Epoch 9/30\n",
      "  Batch 50/437 - Loss: 0.0388\n",
      "  Batch 100/437 - Loss: 0.1744\n",
      "  Batch 150/437 - Loss: 0.2599\n",
      "  Batch 200/437 - Loss: 0.1197\n",
      "  Batch 250/437 - Loss: 0.2760\n",
      "  Batch 300/437 - Loss: 0.1367\n",
      "  Batch 350/437 - Loss: 0.1312\n",
      "  Batch 400/437 - Loss: 0.1437\n",
      "Sample 1:\n",
      "Predicted: ,<EOS>\n",
      "Target   : ,<EOS>\n",
      "\n",
      "Sample 2:\n",
      "Predicted: swaha<EOS>\n",
      "Target   : swaha<EOS>\n",
      "\n",
      "Sample 3:\n",
      "Predicted: swaha<EOS>\n",
      "Target   : swaha<EOS>\n",
      "\n",
      "\n",
      "=== Top 5 Worst Samples by CER ===\n",
      "[1] CER: 1.333\n",
      "   Predicted: ki<EOS><EOS><EOS>\n",
      "   Ground Truth: nang<EOS>\n",
      "\n",
      "[2] CER: 1.300\n",
      "   Predicted: tmb<EOS><EOS><EOS>\n",
      "   Ground Truth: utama<EOS>\n",
      "\n",
      "[3] CER: 1.000\n",
      "   Predicted: wad\n",
      "   Ground Truth: di<EOS>\n",
      "\n",
      "[4] CER: 1.000\n",
      "   Predicted: pa\n",
      "   Ground Truth: .<EOS>\n",
      "\n",
      "[5] CER: 1.000\n",
      "   Predicted: bi\n",
      "   Ground Truth: 1<EOS>\n",
      "\n",
      "[9/30] Train Loss: 0.1936, Train CER: 0.0504 | Val Loss: 0.2742, Val CER: 0.0691\n",
      "No improvement for 3 epoch(s).\n",
      "Early stopping triggered after 3 epochs without improvement on validation loss.\n"
     ]
    }
   ],
   "source": [
    "class ViTEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple ViT encoder that extracts patch embeddings as [batch_size, num_patches, hidden_dim].\n",
    "    \"\"\"\n",
    "    def __init__(self, model_name=\"vit_large_patch16_224\", pretrained=True):\n",
    "        super(ViTEncoder, self).__init__()\n",
    "        self.vit = timm.create_model(model_name, pretrained=pretrained)\n",
    "        self.vit.head = nn.Identity()\n",
    "        self.encoder_dim = self.vit.embed_dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        :param x: [batch_size, 3, 224, 224]\n",
    "        :return:  [batch_size, num_patches, encoder_dim]\n",
    "        \"\"\"\n",
    "        # forward_features usually returns [B, hidden_dim, H', W'] or [B, hidden_dim]\n",
    "        feats = self.vit.forward_features(x)  # [B, hidden_dim, H', W'] for vit_large_patch16_224\n",
    "\n",
    "        # Flatten the spatial dimensions\n",
    "        if feats.dim() == 4:  # [B, C, H, W]\n",
    "            b, c, h, w = feats.shape\n",
    "            feats = feats.permute(0, 2, 3, 1).reshape(b, -1, c)  # [B, H*W, C]\n",
    "\n",
    "        return feats\n",
    "\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, encoder_dim, decoder_dim, attention_dim):\n",
    "        super(Attention, self).__init__()\n",
    "        self.encoder_att = nn.Linear(encoder_dim, attention_dim)  # transform encoder output\n",
    "        self.decoder_att = nn.Linear(decoder_dim, attention_dim)  # transform decoder hidden\n",
    "        self.full_att    = nn.Linear(attention_dim, 1)\n",
    "        self.relu        = nn.ReLU()\n",
    "        self.softmax     = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, encoder_out, decoder_hidden):\n",
    "        \"\"\"\n",
    "        encoder_out:    [batch_size, num_patches, encoder_dim]\n",
    "        decoder_hidden: [batch_size, decoder_dim]\n",
    "        \"\"\"\n",
    "        att1 = self.encoder_att(encoder_out)                  # [batch_size, num_patches, attention_dim]\n",
    "        att2 = self.decoder_att(decoder_hidden).unsqueeze(1)  # [batch_size, 1, attention_dim]\n",
    "\n",
    "        # sum -> relu -> full_att -> squeeze -> softmax\n",
    "        att  = self.full_att(self.relu(att1 + att2)).squeeze(2)  # [batch_size, num_patches]\n",
    "        alpha = self.softmax(att)\n",
    "        # Weighted sum of the encoder_out\n",
    "        attention_weighted_encoding = (encoder_out * alpha.unsqueeze(2)).sum(dim=1)  # [batch_size, encoder_dim]\n",
    "        return attention_weighted_encoding, alpha\n",
    "\n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, attention_dim, embed_dim, decoder_dim, vocab_size, encoder_dim=768, teacher_forcing_ratio=0.5):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "\n",
    "        self.attention     = Attention(encoder_dim, decoder_dim, attention_dim)\n",
    "        self.embedding     = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.dropout       = nn.Dropout(p=0.5)\n",
    "        self.lstm = nn.LSTMCell(embed_dim + encoder_dim, decoder_dim)\n",
    "        self.init_h = nn.Linear(encoder_dim, decoder_dim)\n",
    "        self.init_c = nn.Linear(encoder_dim, decoder_dim)\n",
    "        self.f_beta  = nn.Linear(decoder_dim, encoder_dim)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.fc = nn.Linear(decoder_dim, vocab_size)\n",
    "        self.teacher_forcing_ratio = teacher_forcing_ratio\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        self.embedding.weight.data.uniform_(-0.1, 0.1)\n",
    "        self.fc.bias.data.fill_(0)\n",
    "        self.fc.weight.data.uniform_(-0.1, 0.1)\n",
    "\n",
    "    def init_hidden_state(self, encoder_out):\n",
    "        mean_encoder_out = encoder_out.mean(dim=1)  # [batch_size, encoder_dim]\n",
    "        h = self.init_h(mean_encoder_out)         # [batch_size, decoder_dim]\n",
    "        c = self.init_c(mean_encoder_out)         # [batch_size, decoder_dim]\n",
    "        return (h, c)\n",
    "\n",
    "    def forward(self, encoder_out, encoded_captions, caption_lengths):\n",
    "        \"\"\"\n",
    "        encoder_out:      [batch_size, num_patches, encoder_dim]\n",
    "        encoded_captions: [batch_size, max_label_length]\n",
    "        caption_lengths:  [batch_size, 1]\n",
    "        \"\"\"\n",
    "        caption_lengths, sort_ind = caption_lengths.squeeze(1).sort(dim=0, descending=True)\n",
    "        encoder_out      = encoder_out[sort_ind]\n",
    "        encoded_captions = encoded_captions[sort_ind]\n",
    "\n",
    "        embeddings = self.embedding(encoded_captions)\n",
    "\n",
    "        # Initialize hidden states\n",
    "        h, c = self.init_hidden_state(encoder_out)\n",
    "\n",
    "        decode_lengths    = (caption_lengths - 1).tolist()\n",
    "        max_decode_length = max(decode_lengths)\n",
    "\n",
    "        batch_size = encoder_out.size(0)\n",
    "        vocab_size = self.fc.out_features\n",
    "\n",
    "        predictions = torch.zeros(batch_size, max_decode_length, vocab_size, device=encoder_out.device)\n",
    "        alphas      = torch.zeros(batch_size, max_decode_length, encoder_out.size(1), device=encoder_out.device)\n",
    "\n",
    "        # We'll feed the first token from the input (<SOS>) or from the previous prediction\n",
    "        prev_tokens = encoded_captions[:, 0].clone()\n",
    "\n",
    "        for t in range(max_decode_length):\n",
    "            batch_size_t = sum([l > t for l in decode_lengths])\n",
    "\n",
    "            attention_weighted_encoding, alpha = self.attention(\n",
    "                encoder_out[:batch_size_t],\n",
    "                h[:batch_size_t]\n",
    "            )\n",
    "\n",
    "            # Apply gating\n",
    "            gate = self.sigmoid(self.f_beta(h[:batch_size_t]))\n",
    "            attention_weighted_encoding = gate * attention_weighted_encoding\n",
    "\n",
    "            # Teacher forcing?\n",
    "            use_teacher_forcing = (torch.rand(1).item() < self.teacher_forcing_ratio)\n",
    "            if use_teacher_forcing:\n",
    "                current_input = embeddings[:batch_size_t, t, :]\n",
    "            else:\n",
    "                current_input = self.embedding(prev_tokens[:batch_size_t].detach())\n",
    "\n",
    "            h_next, c_next = self.lstm(\n",
    "                torch.cat([current_input, attention_weighted_encoding], dim=1),\n",
    "                (h[:batch_size_t], c[:batch_size_t])\n",
    "            )\n",
    "\n",
    "            preds = self.fc(self.dropout(h_next))\n",
    "            predictions[:batch_size_t, t, :] = preds\n",
    "            alphas[:batch_size_t, t, :]      = alpha\n",
    "\n",
    "            _, next_tokens = preds.max(dim=1)\n",
    "            prev_tokens_ = prev_tokens.clone()\n",
    "            prev_tokens_[:batch_size_t] = next_tokens.detach()\n",
    "            prev_tokens = prev_tokens_\n",
    "\n",
    "            h_new = torch.zeros_like(h)\n",
    "            c_new = torch.zeros_like(c)\n",
    "\n",
    "            h_new[:batch_size_t] = h_next\n",
    "            c_new[:batch_size_t] = c_next\n",
    "\n",
    "            h, c = h_new, c_new\n",
    "\n",
    "        return predictions, encoded_captions, decode_lengths, alphas, sort_ind\n",
    "\n",
    "\n",
    "encoder = ViTEncoder(model_name=\"vit_large_patch16_224\", pretrained=True)\n",
    "decoder = DecoderRNN(\n",
    "    attention_dim=256,\n",
    "    embed_dim=256,\n",
    "    decoder_dim=512,\n",
    "    vocab_size=vocab_size,\n",
    "    encoder_dim=encoder.encoder_dim,  # typically 768 for ViT-B/16\n",
    "    teacher_forcing_ratio=0.5\n",
    ")\n",
    "\n",
    "encoder = encoder.to(device)\n",
    "decoder = decoder.to(device)\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=char_to_idx['<PAD>'])\n",
    "encoder_optimizer = optim.Adam(encoder.parameters(), lr=1e-4)\n",
    "decoder_optimizer = optim.Adam(decoder.parameters(), lr=4e-4)\n",
    "\n",
    "\n",
    "class ImageCaptioningTrainer:\n",
    "    def __init__(self, encoder, decoder, \n",
    "                 criterion, encoder_optimizer, decoder_optimizer, \n",
    "                 train_loader, val_loader, device, \n",
    "                 char_to_idx, idx_to_char, max_label_length):\n",
    "        self.encoder = encoder.to(device)\n",
    "        self.decoder = decoder.to(device)\n",
    "        self.criterion = criterion\n",
    "        self.encoder_optimizer = encoder_optimizer\n",
    "        self.decoder_optimizer = decoder_optimizer\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader   = val_loader\n",
    "        self.device = device\n",
    "        self.char_to_idx = char_to_idx\n",
    "        self.idx_to_char = idx_to_char\n",
    "        self.max_label_length = max_label_length\n",
    "\n",
    "    def fit(self, num_epochs, early_stopping_patience=5):\n",
    "        \"\"\"\n",
    "        Train the model for 'num_epochs' epochs.\n",
    "        If val_loss doesn't improve for 'early_stopping_patience' consecutive epochs,\n",
    "        we stop training early.\n",
    "        \"\"\"\n",
    "        best_val_loss = float(\"inf\")  # track minimum validation loss seen\n",
    "        no_improvement_epochs = 0     # epochs since last improvement\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
    "            train_loss, train_cer = self.train_one_epoch()\n",
    "            val_loss,   val_cer   = self.validate_one_epoch(top_n=5)\n",
    "\n",
    "            print(f\"[{epoch+1}/{num_epochs}] \"\n",
    "                  f\"Train Loss: {train_loss:.4f}, Train CER: {train_cer:.4f} | \"\n",
    "                  f\"Val Loss: {val_loss:.4f}, Val CER: {val_cer:.4f}\")\n",
    "\n",
    "            # Early Stopping Check\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                no_improvement_epochs = 0\n",
    "            else:\n",
    "                # No improvement this epoch\n",
    "                no_improvement_epochs += 1\n",
    "                print(f\"No improvement for {no_improvement_epochs} epoch(s).\")\n",
    "\n",
    "                if no_improvement_epochs >= early_stopping_patience:\n",
    "                    print(f\"Early stopping triggered after {no_improvement_epochs} epochs \"\n",
    "                          f\"without improvement on validation loss.\")\n",
    "                    break\n",
    "\n",
    "    def train_one_epoch(self):\n",
    "        self.encoder.train()\n",
    "        self.decoder.train()\n",
    "        running_loss = 0.0\n",
    "        total_edit_distance = 0\n",
    "        total_ref_length = 0\n",
    "\n",
    "        for batch_idx, (images, labels, label_lengths) in enumerate(self.train_loader):\n",
    "            images = images.to(self.device, non_blocking=True)\n",
    "            labels = labels.to(self.device, non_blocking=True)\n",
    "            label_lengths = label_lengths.to(self.device, non_blocking=True)\n",
    "\n",
    "            self.encoder_optimizer.zero_grad()\n",
    "            self.decoder_optimizer.zero_grad()\n",
    "\n",
    "            encoder_out = self.encoder(images)\n",
    "            caption_lengths = torch.tensor(\n",
    "                [self.max_label_length] * labels.size(0)\n",
    "            ).unsqueeze(1).to(self.device)\n",
    "\n",
    "            outputs, encoded_captions, decode_lengths, alphas, sort_ind = self.decoder(\n",
    "                encoder_out, labels, caption_lengths\n",
    "            )\n",
    "\n",
    "            # Targets = encoded captions without the <SOS>\n",
    "            targets = encoded_captions[:, 1:]\n",
    "\n",
    "            # Flatten for loss\n",
    "            outputs_flat = outputs.view(-1, self.decoder.fc.out_features)\n",
    "            targets_flat = targets.contiguous().view(-1)\n",
    "\n",
    "            loss = self.criterion(outputs_flat, targets_flat)\n",
    "            loss.backward()\n",
    "\n",
    "            self.decoder_optimizer.step()\n",
    "            self.encoder_optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            # Compute CER\n",
    "            batch_size = labels.size(0)\n",
    "            _, preds_flat = torch.max(outputs_flat, dim=1)\n",
    "            preds_seq = preds_flat.view(batch_size, -1)\n",
    "\n",
    "            for i in range(batch_size):\n",
    "                pred_indices = preds_seq[i].detach().cpu().numpy()\n",
    "                target_indices = targets[i].detach().cpu().numpy()\n",
    "\n",
    "                mask = (target_indices != self.char_to_idx['<PAD>'])\n",
    "                pred_indices   = pred_indices[mask]\n",
    "                target_indices = target_indices[mask]\n",
    "\n",
    "                pred_str   = ''.join([self.idx_to_char.get(idx, '') for idx in pred_indices])\n",
    "                target_str = ''.join([self.idx_to_char.get(idx, '') for idx in target_indices])\n",
    "\n",
    "                edit_dist = editdistance.eval(pred_str, target_str)\n",
    "                total_edit_distance += edit_dist\n",
    "                total_ref_length    += len(target_str)\n",
    "\n",
    "            # Optional: Print intermediate training stats\n",
    "            if (batch_idx + 1) % 50 == 0:\n",
    "                print(f\"  Batch {batch_idx + 1}/{len(self.train_loader)} - Loss: {loss.item():.4f}\")\n",
    "\n",
    "        avg_loss = running_loss / len(self.train_loader)\n",
    "        avg_cer  = total_edit_distance / total_ref_length if total_ref_length > 0 else 0.0\n",
    "        return avg_loss, avg_cer\n",
    "\n",
    "    def validate_one_epoch(self, top_n=5):\n",
    "        self.encoder.eval()\n",
    "        self.decoder.eval()\n",
    "        running_loss = 0.0\n",
    "        total_edit_distance = 0\n",
    "        total_ref_length = 0\n",
    "\n",
    "        sample_cer_info = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (images, labels, label_lengths) in enumerate(self.val_loader):\n",
    "                images = images.to(self.device, non_blocking=True)\n",
    "                labels = labels.to(self.device, non_blocking=True)\n",
    "                label_lengths = label_lengths.to(self.device, non_blocking=True)\n",
    "\n",
    "                encoder_out = self.encoder(images)\n",
    "                caption_lengths = torch.tensor(\n",
    "                    [self.max_label_length] * labels.size(0)\n",
    "                ).unsqueeze(1).to(self.device)\n",
    "\n",
    "                outputs, encoded_captions, decode_lengths, alphas, sort_ind = self.decoder(\n",
    "                    encoder_out, labels, caption_lengths\n",
    "                )\n",
    "\n",
    "                targets = encoded_captions[:, 1:]\n",
    "\n",
    "                outputs_flat = outputs.view(-1, self.decoder.fc.out_features)\n",
    "                targets_flat = targets.contiguous().view(-1)\n",
    "\n",
    "                loss = self.criterion(outputs_flat, targets_flat)\n",
    "                running_loss += loss.item()\n",
    "\n",
    "                batch_size = labels.size(0)\n",
    "                _, preds_flat = torch.max(outputs_flat, dim=1)\n",
    "                preds_seq = preds_flat.view(batch_size, -1)\n",
    "\n",
    "                for i in range(batch_size):\n",
    "                    pred_indices = preds_seq[i].detach().cpu().numpy()\n",
    "                    target_indices = targets[i].detach().cpu().numpy()\n",
    "\n",
    "                    mask = (target_indices != self.char_to_idx['<PAD>'])\n",
    "                    pred_indices   = pred_indices[mask]\n",
    "                    target_indices = target_indices[mask]\n",
    "\n",
    "                    pred_str   = ''.join(self.idx_to_char.get(idx, '') for idx in pred_indices)\n",
    "                    target_str = ''.join(self.idx_to_char.get(idx, '') for idx in target_indices)\n",
    "\n",
    "                    edit_dist = editdistance.eval(pred_str, target_str)\n",
    "                    ref_len   = len(target_str)\n",
    "                    cer       = edit_dist / ref_len if ref_len > 0 else 0\n",
    "\n",
    "                    total_edit_distance += edit_dist\n",
    "                    total_ref_length    += ref_len\n",
    "\n",
    "                    sample_cer_info.append({\n",
    "                        \"pred\": pred_str,\n",
    "                        \"gt\": target_str,\n",
    "                        \"cer\": cer\n",
    "                    })\n",
    "\n",
    "                    # Print a few samples from the first batch\n",
    "                    # if batch_idx == 0 and i < 3:\n",
    "                    #     print(f\"Sample {i + 1}:\")\n",
    "                    #     print(f\"Predicted: {pred_str}\")\n",
    "                    #     print(f\"Target   : {target_str}\\n\")\n",
    "\n",
    "        avg_loss = running_loss / len(self.val_loader)\n",
    "        avg_cer  = total_edit_distance / total_ref_length if total_ref_length > 0 else 0.0\n",
    "\n",
    "        # Sort by CER descending\n",
    "        sample_cer_info.sort(key=lambda x: x[\"cer\"], reverse=True)\n",
    "        worst_samples = sample_cer_info[:top_n]\n",
    "\n",
    "        print(f\"\\n=== Top {top_n} Worst Samples by CER ===\")\n",
    "        for idx, sample in enumerate(worst_samples):\n",
    "            print(f\"[{idx+1}] CER: {sample['cer']:.3f}\")\n",
    "            print(f\"   Predicted: {sample['pred']}\")\n",
    "            print(f\"   Ground Truth: {sample['gt']}\\n\")\n",
    "\n",
    "        return avg_loss, avg_cer\n",
    "\n",
    "\n",
    "trainer = ImageCaptioningTrainer(\n",
    "    encoder=encoder,\n",
    "    decoder=decoder,\n",
    "    criterion=criterion,\n",
    "    encoder_optimizer=encoder_optimizer,\n",
    "    decoder_optimizer=decoder_optimizer,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    device=device,\n",
    "    char_to_idx=char_to_idx,\n",
    "    idx_to_char=idx_to_char,\n",
    "    max_label_length=max_label_length\n",
    ")\n",
    "\n",
    "num_epochs = 30\n",
    "trainer.fit(num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n",
      "['BalineseDataset', 'DataLoader', 'Dataset', 'Image', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', 'editdistance', 'evaluate_test_set', 'np', 'os', 'pd', 'torch', 'transforms']\n"
     ]
    }
   ],
   "source": [
    "import test_balinese_model\n",
    "print(dir(test_balinese_model))\n",
    "from test_balinese_model import evaluate_test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ground_truth_path = os.path.join(os.getcwd(), 'balinese_transliteration_test.txt')\n",
    "test_images_dir        = os.path.join(os.getcwd(), 'balinese_word_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unknown characters in test labels: set()\n",
      "\n",
      "=== Sample predictions (first 5) ===\n",
      "Image: test1.png\n",
      "Predicted: ,\n",
      "Ground Truth: ,\n",
      "\n",
      "Image: test2.png\n",
      "Predicted: biak\n",
      "Ground Truth: biakta\n",
      "\n",
      "Image: test3.png\n",
      "Predicted: antah\n",
      "Ground Truth: ngantah\n",
      "\n",
      "Image: test4.png\n",
      "Predicted: sarina\n",
      "Ground Truth: sarira\n",
      "\n",
      "Image: test5.png\n",
      "Predicted: yu\n",
      "Ground Truth: yu\n",
      "\n",
      "Global CER on test set: 0.1409\n",
      "\n",
      "Top 5 highest CER results:\n",
      "1) Image: test10070.png\n",
      "   CER: 8.0000\n",
      "   Predicted       : .patatnia\n",
      "   Ground Truth    : .\n",
      "\n",
      "2) Image: test1330.png\n",
      "   CER: 7.0000\n",
      "   Predicted       : santiaa\n",
      "   Ground Truth    : .\n",
      "\n",
      "3) Image: test580.png\n",
      "   CER: 6.0000\n",
      "   Predicted       : .aattia\n",
      "   Ground Truth    : .\n",
      "\n",
      "4) Image: test1335.png\n",
      "   CER: 5.0000\n",
      "   Predicted       : panti\n",
      "   Ground Truth    : .\n",
      "\n",
      "5) Image: test6297.png\n",
      "   CER: 5.0000\n",
      "   Predicted       : panti\n",
      "   Ground Truth    : .\n",
      "\n",
      "Final Test CER: 0.1409\n"
     ]
    }
   ],
   "source": [
    "test_cer = evaluate_test_set(\n",
    "    encoder=encoder,\n",
    "    decoder=decoder,\n",
    "    device=device,\n",
    "    char_to_idx=char_to_idx,\n",
    "    idx_to_char=idx_to_char,\n",
    "    max_label_length=max_label_length,\n",
    "    test_ground_truth_path=test_ground_truth_path,\n",
    "    test_images_dir=test_images_dir\n",
    ")\n",
    "\n",
    "print(f\"Final Test CER: {test_cer:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_ground_truth_path = os.path.join(base_dir, 'balinese_transliteration_test.txt')\n",
    "# test_images_dir        = os.path.join(base_dir, 'balinese_word_test')\n",
    "\n",
    "# test_filenames = []\n",
    "# test_labels    = []\n",
    "\n",
    "# with open(test_ground_truth_path, 'r', encoding='utf-8') as file:\n",
    "#     for line in file:\n",
    "#         line = line.strip()\n",
    "#         if line:\n",
    "#             parts = line.split(';')\n",
    "#             if len(parts) == 2:\n",
    "#                 filename, label = parts\n",
    "#                 label = label.lower()\n",
    "#                 test_filenames.append(filename)\n",
    "#                 test_labels.append(label)\n",
    "#             else:\n",
    "#                 print(f\"Skipping malformed line: {line}\")\n",
    "\n",
    "# test_data = pd.DataFrame({\n",
    "#     'filename': test_filenames,\n",
    "#     'label': test_labels\n",
    "# })\n",
    "\n",
    "# # Check for unknown chars in test set\n",
    "# test_chars = set(''.join(test_data['label']))\n",
    "# unknown_chars = test_chars - set(char_to_idx.keys())\n",
    "# print(f\"Unknown characters in test labels: {unknown_chars}\")\n",
    "\n",
    "# # Encode test labels\n",
    "# max_label_length_test = max(len(lbl) for lbl in test_data['label']) + 2\n",
    "# def encode_label_test(label, char_to_idx, max_length):\n",
    "#     encoded = (\n",
    "#         [char_to_idx['<SOS>']] +\n",
    "#         [char_to_idx.get(ch, char_to_idx['<UNK>']) for ch in label] +\n",
    "#         [char_to_idx['<EOS>']]\n",
    "#     )\n",
    "#     if len(encoded) > max_length:\n",
    "#         encoded = encoded[:max_length]\n",
    "#     else:\n",
    "#         encoded += [char_to_idx['<PAD>']] * (max_length - len(encoded))\n",
    "#     return encoded\n",
    "\n",
    "# test_data['encoded_label'] = test_data['label'].apply(lambda x: encode_label_test(x, char_to_idx, max_label_length_test))\n",
    "# test_data['label_length']  = test_data['label'].apply(len)\n",
    "\n",
    "# test_transform = transforms.Compose([\n",
    "#     transforms.Resize((224, 224)),\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize(\n",
    "#         mean=(0.5, 0.5, 0.5),\n",
    "#         std=(0.5, 0.5, 0.5)\n",
    "#     )\n",
    "# ])\n",
    "\n",
    "# test_dataset = BalineseDataset(test_data, test_images_dir, transform=test_transform)\n",
    "# test_loader  = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=0)\n",
    "\n",
    "\n",
    "# def inference(encoder, decoder, data_loader, device, char_to_idx, idx_to_char, max_seq_length, test_data):\n",
    "#     encoder.eval()\n",
    "#     decoder.eval()\n",
    "\n",
    "#     eos_idx = char_to_idx['<EOS>']\n",
    "#     results = []\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         for batch_idx, (images, labels, label_lengths) in enumerate(data_loader):\n",
    "#             images = images.to(device, non_blocking=True)\n",
    "#             labels = labels.to(device, non_blocking=True)\n",
    "\n",
    "#             batch_size  = images.size(0)\n",
    "#             encoder_out = encoder(images)  # [B, num_patches, encoder_dim]\n",
    "\n",
    "#             # Init LSTM state\n",
    "#             h, c = decoder.init_hidden_state(encoder_out)\n",
    "\n",
    "\n",
    "#             # Start tokens\n",
    "#             inputs = torch.full(\n",
    "#                 (batch_size,),\n",
    "#                 fill_value=char_to_idx['<SOS>'],\n",
    "#                 dtype=torch.long,\n",
    "#                 device=device\n",
    "#             )\n",
    "\n",
    "#             all_preds = []\n",
    "\n",
    "#             for _ in range(max_seq_length):\n",
    "#                 embeddings = decoder.embedding(inputs)  # [batch_size, embed_dim]\n",
    "            \n",
    "#                 # Attention\n",
    "#                 attention_weighted_encoding, alpha = decoder.attention(encoder_out, h)\n",
    "            \n",
    "#                 # Gating\n",
    "#                 gate = decoder.sigmoid(decoder.f_beta(h))\n",
    "#                 attention_weighted_encoding = gate * attention_weighted_encoding\n",
    "            \n",
    "#                 h, c = decoder.lstm(\n",
    "#                     torch.cat([embeddings, attention_weighted_encoding], dim=1),\n",
    "#                     (h, c))\n",
    "            \n",
    "#                 # Predict next token\n",
    "#                 preds = decoder.fc(decoder.dropout(h))  # [batch_size, vocab_size]\n",
    "#                 _, preds_idx = preds.max(dim=1)\n",
    "            \n",
    "#                 all_preds.append(preds_idx.cpu().numpy())\n",
    "#                 inputs = preds_idx\n",
    "\n",
    "\n",
    "#             # Convert shape from [max_seq_length, batch_size] -> [batch_size, max_seq_length]\n",
    "#             all_preds = np.array(all_preds).T\n",
    "\n",
    "#             for i in range(batch_size):\n",
    "#                 pred_indices = all_preds[i]\n",
    "\n",
    "#                 # Stop at <EOS> if present\n",
    "#                 if eos_idx in pred_indices:\n",
    "#                     first_eos = np.where(pred_indices == eos_idx)[0][0]\n",
    "#                     pred_indices = pred_indices[:first_eos]\n",
    "\n",
    "#                 pred_chars = [idx_to_char.get(idx, '') for idx in pred_indices]\n",
    "#                 pred_str   = ''.join(pred_chars)\n",
    "\n",
    "#                 # Ground truth\n",
    "#                 label_indices = labels[i].detach().cpu().numpy()\n",
    "#                 # remove <SOS>\n",
    "#                 label_indices = label_indices[1:]\n",
    "#                 if eos_idx in label_indices:\n",
    "#                     eos_pos = np.where(label_indices == eos_idx)[0][0]\n",
    "#                     label_indices = label_indices[:eos_pos]\n",
    "#                 else:\n",
    "#                     label_indices = label_indices[label_indices != char_to_idx['<PAD>']]\n",
    "\n",
    "#                 label_chars = [idx_to_char.get(idx, '') for idx in label_indices]\n",
    "#                 label_str   = ''.join(label_chars)\n",
    "\n",
    "#                 global_idx = batch_idx * batch_size + i\n",
    "#                 image_filename = test_data.iloc[global_idx]['filename']\n",
    "\n",
    "#                 results.append({\n",
    "#                     'image_filename': image_filename,\n",
    "#                     'predicted_caption': pred_str,\n",
    "#                     'ground_truth_caption': label_str\n",
    "#                 })\n",
    "\n",
    "#     return results\n",
    "\n",
    "# test_results = inference(\n",
    "#     encoder=encoder,\n",
    "#     decoder=decoder,\n",
    "#     data_loader=test_loader,\n",
    "#     device=device,\n",
    "#     char_to_idx=char_to_idx,\n",
    "#     idx_to_char=idx_to_char,\n",
    "#     max_seq_length=max_label_length_test,\n",
    "#     test_data=test_data\n",
    "# )\n",
    "\n",
    "# # Print first few\n",
    "# for r in test_results[:5]:\n",
    "#     print(\"Image:\", r['image_filename'])\n",
    "#     print(\"Predicted:\", r['predicted_caption'])\n",
    "#     print(\"Ground Truth:\", r['ground_truth_caption'])\n",
    "#     print()\n",
    "\n",
    "\n",
    "# def calculate_global_cer(results):\n",
    "#     total_ed   = 0\n",
    "#     total_refs = 0\n",
    "#     for r in results:\n",
    "#         ref = r['ground_truth_caption']\n",
    "#         hyp = r['predicted_caption']\n",
    "#         dist = editdistance.eval(ref, hyp)\n",
    "#         total_ed   += dist\n",
    "#         total_refs += len(ref)\n",
    "#     if total_refs == 0:\n",
    "#         return 0.0\n",
    "#     return total_ed / total_refs\n",
    "\n",
    "# global_cer = calculate_global_cer(test_results)\n",
    "# print(f\"Global CER on test set: {global_cer:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n = 5\n",
    "\n",
    "# results_with_cer = []\n",
    "# for r in test_results:\n",
    "#     ref = r['ground_truth_caption']\n",
    "#     hyp = r['predicted_caption']\n",
    "#     dist = editdistance.eval(ref, hyp)\n",
    "#     length = len(ref)\n",
    "#     cer = dist / length if length > 0 else 0\n",
    "#     new_r = r.copy()\n",
    "#     new_r['cer'] = cer\n",
    "#     results_with_cer.append(new_r)\n",
    "\n",
    "# # 2. Sort by CER in descending order\n",
    "# results_with_cer.sort(key=lambda x: x['cer'], reverse=True)\n",
    "\n",
    "# # 3. Print the top N highest CER\n",
    "# print(f\"\\nTop {n} highest CER results:\")\n",
    "# for i, r in enumerate(results_with_cer[:n], start=1):\n",
    "#     print(f\"{i}) Image: {r['image_filename']}\")\n",
    "#     print(f\"   CER: {r['cer']:.4f}\")\n",
    "#     print(f\"   Predicted       : {r['predicted_caption']}\")\n",
    "#     print(f\"   Ground Truth    : {r['ground_truth_caption']}\")\n",
    "#     print()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6660678,
     "sourceId": 10741146,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30887,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
