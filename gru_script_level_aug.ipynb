{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52ee18c4-7d20-4008-a971-06f1bd0f39e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import editdistance\n",
    "import time \n",
    "import random\n",
    "\n",
    "from PIL import Image\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "\n",
    "import timm  \n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "522fd34f-1004-43be-89ba-2c97189e573a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from augmentations.script_level.Information_extraction import information_extraction\n",
    "from augmentations.script_level.transformation import flag_judge, identify_reference_corner\n",
    "from augmentations.script_level.transformation import bezier_transformation, affine_transformation, L2A_transformation\n",
    "from augmentations.script_level import script_aug\n",
    "from augmentations.script_level.script_aug import new_local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e01607ec-0914-4dff-8b89-831dd9f04e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import  sys\n",
    "# # sys.path.insert(0, '/fs04/qb36/lontar_project/code/gru_aug')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96f12cf6-64ef-4626-9f39-9812dca91ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0db252f6-2b6c-432a-9035-0ae2156a123c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base project path\n",
    "BASE_DIR = \"/projects/qb36/lontar_project\"\n",
    "DATA_DIR = os.path.join(BASE_DIR, \"data\")\n",
    "\n",
    "# Define file paths\n",
    "ground_truth_path = os.path.join(DATA_DIR, 'balinese_transliteration_train.txt')\n",
    "images_dir = os.path.join(DATA_DIR, 'balinese_word_train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "324fc09b-fa47-4a5f-9ec9-9bb6abd60a1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 39\n",
      "Training size: 13972; Validation size: 1050\n"
     ]
    }
   ],
   "source": [
    "# base_dir = os.getcwd()\n",
    "\n",
    "# # Same paths as your original code\n",
    "# ground_truth_path = os.path.join(base_dir, 'balinese_transliteration_train.txt') \n",
    "# images_dir        = os.path.join(base_dir, 'balinese_word_train')\n",
    "\n",
    "\n",
    "filenames = []\n",
    "labels    = []\n",
    "\n",
    "with open(ground_truth_path, 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        line = line.strip()\n",
    "        if line:  # Ensure the line is not empty\n",
    "            parts = line.split(';')\n",
    "            if len(parts) == 2:\n",
    "                filename, label = parts\n",
    "                label = label.lower()\n",
    "                filenames.append(filename)\n",
    "                labels.append(label)\n",
    "            else:\n",
    "                print(f\"Skipping malformed line: {line}\")\n",
    "\n",
    "data = pd.DataFrame({\n",
    "    'filename': filenames,\n",
    "    'label': labels\n",
    "})\n",
    "\n",
    "label_counts = data['label'].value_counts()\n",
    "\n",
    "all_text = ''.join(data['label'])\n",
    "unique_chars = sorted(list(set(all_text)))\n",
    "\n",
    "# Create character->index starting from 1\n",
    "char_to_idx = {char: idx + 1 for idx, char in enumerate(unique_chars)}\n",
    "# Add special tokens\n",
    "char_to_idx['<PAD>'] = 0\n",
    "char_to_idx['<UNK>'] = len(char_to_idx)\n",
    "char_to_idx['<SOS>'] = len(char_to_idx)\n",
    "char_to_idx['<EOS>'] = len(char_to_idx)\n",
    "\n",
    "# Reverse mapping\n",
    "idx_to_char = {v: k for k, v in char_to_idx.items()}\n",
    "\n",
    "vocab_size = len(char_to_idx)\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "\n",
    "def encode_label(label, char_to_idx, max_length):\n",
    "    \"\"\"\n",
    "    Converts a label (string) into a list of indices with <SOS>, <EOS>, padding, etc.\n",
    "    \"\"\"\n",
    "    encoded = (\n",
    "        [char_to_idx['<SOS>']] +\n",
    "        [char_to_idx.get(ch, char_to_idx['<UNK>']) for ch in label] +\n",
    "        [char_to_idx['<EOS>']]\n",
    "    )\n",
    "    # Pad if needed\n",
    "    if len(encoded) < max_length:\n",
    "        encoded += [char_to_idx['<PAD>']] * (max_length - len(encoded))\n",
    "    else:\n",
    "        encoded = encoded[:max_length]\n",
    "    return encoded\n",
    "\n",
    "max_label_length = max(len(label) for label in data['label']) + 2  # +2 for <SOS> and <EOS>\n",
    "data['encoded_label'] = data['label'].apply(lambda x: encode_label(x, char_to_idx, max_label_length))\n",
    "data['label_length']  = data['label'].apply(len)\n",
    "\n",
    "rare_labels = label_counts[label_counts < 3].index  # NEW: words that appear <3 times\n",
    "\n",
    "def custom_split(df, rare_label_list, test_size=0.1, random_state=42):\n",
    "    # Separate rare words from frequent ones\n",
    "    rare_df     = df[df['label'].isin(rare_label_list)]\n",
    "    non_rare_df = df[~df['label'].isin(rare_label_list)]\n",
    "\n",
    "    #  train/val split for non-rare\n",
    "    train_nr, val_nr = train_test_split(non_rare_df, test_size=test_size, \n",
    "                                        random_state=random_state)\n",
    "\n",
    "    # Combine rare samples entirely into training\n",
    "    train_df = pd.concat([train_nr, rare_df], ignore_index=True)\n",
    "    # Shuffle after combining\n",
    "    train_df = train_df.sample(frac=1, random_state=random_state).reset_index(drop=True)\n",
    "\n",
    "    val_df = val_nr.reset_index(drop=True)\n",
    "    return train_df, val_df\n",
    "\n",
    "# Call custom_split instead of direct train_test_split\n",
    "train_data, val_data = custom_split(data, rare_labels, test_size=0.1, random_state=42) \n",
    "\n",
    "print(f\"Training size: {len(train_data)}; Validation size: {len(val_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d307eca6-2574-4032-80d7-844b8e58184d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BalineseDataset(Dataset):\n",
    "    def __init__(self, df, images_dir, transform=None):\n",
    "        self.data       = df.reset_index(drop=True)\n",
    "        self.images_dir = images_dir\n",
    "        self.transform  = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name     = self.data.loc[idx, 'filename']\n",
    "        label        = self.data.loc[idx, 'encoded_label']\n",
    "        label_length = self.data.loc[idx, 'label_length']\n",
    "\n",
    "        img_path = os.path.join(self.images_dir, img_name)\n",
    "        image    = Image.open(img_path).convert('RGB')\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        label = torch.tensor(label, dtype=torch.long)\n",
    "        return image, label, torch.tensor(label_length, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa20aa2f-f9b5-44d7-b8f6-6b15775878b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScriptLevelAugment:\n",
    "    \"\"\"\n",
    "    Wraps the ICFHR'22 script-level augmentation as a torchvision-style transform.\n",
    "    Only applies augmentation with probability `prob`.\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 prob=0.5,\n",
    "                 stroke_radius=2,\n",
    "                 k1_control_field_corner=0.6,\n",
    "                 k2_control_field_third_bezier=0.6,\n",
    "                 segment=2):\n",
    "        self.prob = prob\n",
    "        self.stroke_radius = stroke_radius\n",
    "        self.k1_control_field_corner = k1_control_field_corner\n",
    "        self.k2_control_field_third_bezier = k2_control_field_third_bezier\n",
    "        self.segment = segment\n",
    "\n",
    "    def __call__(self, pil_img):\n",
    "        if random.random() > self.prob:\n",
    "            return pil_img\n",
    "        # Convert PIL Image to numpy array (RGB)\n",
    "        img = np.array(pil_img.convert(\"RGB\"))\n",
    "        # Run augmentation (get list of augmented images, pick the first)\n",
    "        aug_imgs = new_local(\n",
    "            img, \n",
    "            times=1,\n",
    "            stroke_radius=self.stroke_radius,\n",
    "            k1_control_field_corner=self.k1_control_field_corner,\n",
    "            k2_control_field_third_bezier=self.k2_control_field_third_bezier,\n",
    "            segment=self.segment\n",
    "        )\n",
    "        # Defensive: fallback if augmentation fails\n",
    "        if not aug_imgs or not isinstance(aug_imgs[0], np.ndarray):\n",
    "            return pil_img\n",
    "        # Convert back to PIL Image\n",
    "        aug_pil = Image.fromarray(aug_imgs[0].astype(np.uint8)).convert(\"RGB\")\n",
    "        return aug_pil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bbb03afe-e10e-45eb-b348-1351e71f38d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_transform = transforms.Compose([\n",
    "#     transforms.Resize((224, 224)),\n",
    "#     transforms.RandomRotation(degrees=5),\n",
    "#     transforms.RandomAffine(degrees=0, translate=(0.05, 0.05)),\n",
    "#     transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.RandomErasing(p=0.5, scale=(0.01, 0.05), ratio=(0.3, 3.3)),\n",
    "#     transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)),\n",
    "# ])\n",
    "\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    ScriptLevelAugment(prob=0.3),  # 30% chance augment\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomRotation(degrees=5),\n",
    "    transforms.RandomAffine(degrees=0, translate=(0.05, 0.05)),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.RandomErasing(p=0.5, scale=(0.01, 0.05), ratio=(0.3, 3.3)),\n",
    "    transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)),\n",
    "])\n",
    "\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)),\n",
    "])\n",
    "\n",
    "train_dataset = BalineseDataset(train_data, images_dir, transform=train_transform)\n",
    "val_dataset   = BalineseDataset(val_data,   images_dir, transform=val_transform)\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,  num_workers=2, pin_memory=True)\n",
    "val_loader   = DataLoader(val_dataset,   batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9d2653d9-3262-42fd-8a83-f4b71abae491",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet18Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Encoder that uses a pretrained ResNet18 to extract features of shape \n",
    "    [B, H*W, C], which the DecoderRNN can then attend over.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, pretrained=True):\n",
    "        super(ResNet18Encoder, self).__init__()\n",
    "        resnet = models.resnet18(pretrained=pretrained)\n",
    "\n",
    "        # Remove the classification (fc) layer\n",
    "        modules = list(resnet.children())[:-2]  # remove the avgpool & fc\n",
    "        self.cnn = nn.Sequential(*modules)\n",
    "\n",
    "        # last convolutional block outputs 512 channels\n",
    "        self.encoder_dim = 512\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Input shape:  x -> [batch_size, 3, 224, 224]\n",
    "        Output shape: -> [batch_size, num_patches, encoder_dim]\n",
    "                       where num_patches = H' * W' from the final feature map\n",
    "        \"\"\"\n",
    "        # pass through ResNet (up to layer4)\n",
    "        features = self.cnn(x)  # [B, 512, H', W']\n",
    "\n",
    "        # Flatten the spatial dims\n",
    "        # shape => [B, 512, H', W'] -> [B, H'*W', 512]\n",
    "        b, c, h, w = features.shape\n",
    "        features = features.permute(0, 2, 3, 1)   # [B, H', W', C]\n",
    "        features = features.reshape(b, -1, c)     # [B, H'*W', C]\n",
    "\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7d1c0aac-ffb1-46dd-b122-741af90faf0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViTEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple ViT encoder that extracts patch embeddings as [batch_size, num_patches, hidden_dim].\n",
    "    We'll use timm to load a pretrained ViT. Then we use .forward_features() to get a\n",
    "    feature map of shape [B, C, H', W'] for many timm ViT models, which we flatten.\n",
    "    \"\"\"\n",
    "    def __init__(self, model_name, pretrained=True):\n",
    "        super(ViTEncoder, self).__init__()\n",
    "        self.vit = timm.create_model(model_name, pretrained=pretrained)\n",
    "        # Remove or replace the classification head\n",
    "        self.vit.head = nn.Identity()\n",
    "\n",
    "        # timm's ViT typically has an embed_dim attribute\n",
    "        self.encoder_dim = self.vit.embed_dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        :param x: [batch_size, 3, 224, 224]\n",
    "        :return:  [batch_size, num_patches, encoder_dim]\n",
    "        \"\"\"\n",
    "        # forward_features usually returns [B, hidden_dim, H', W'] or [B, hidden_dim]\n",
    "        feats = self.vit.forward_features(x)  # [B, hidden_dim, 14, 14] for vit_base_patch16_224\n",
    "\n",
    "        # Flatten the spatial dimensions\n",
    "        if feats.dim() == 4:  # [B, C, H, W]\n",
    "            b, c, h, w = feats.shape\n",
    "            feats = feats.permute(0, 2, 3, 1).reshape(b, -1, c)  # => [B, H*W, C]\n",
    "\n",
    "        return feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "591ff7ca-4284-4662-bf71-5962fe8793f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwinEncoder(nn.Module):\n",
    "    def __init__(self, model_name=\"swin_small_patch4_window7_224\", pretrained=True):\n",
    "        \"\"\"\n",
    "        A simple Swin Transformer encoder that extracts patch embeddings\n",
    "        as [batch_size, num_patches, hidden_dim]. We'll use timm to load \n",
    "        a pretrained Swin model, remove its classification head, then flatten.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.swin = timm.create_model(model_name, pretrained=pretrained)\n",
    "        self.swin.head = nn.Identity()\n",
    "\n",
    "        # We'll assign encoder_dim dynamically after forward\n",
    "        self.encoder_dim = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        :param x: [batch_size, 3, 224, 224]\n",
    "        :return:  [batch_size, num_patches, encoder_dim]\n",
    "        \"\"\"\n",
    "        feats = self.swin.forward_features(x)            # [B, C, H, W]\n",
    "        b, c, h, w = feats.shape\n",
    "        feats = feats.flatten(2).transpose(1, 2)         # [B, H*W, C]\n",
    "        # Set encoder_dim once (C)\n",
    "        if self.encoder_dim is None:\n",
    "            self.encoder_dim = feats.shape[-1]\n",
    "        return feats\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cd74a845-885b-4282-9569-6d98a63461ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HybridEncoder(nn.Module):\n",
    "    def __init__(self, cnn_encoder, vit_encoder):\n",
    "        super(HybridEncoder, self).__init__()\n",
    "        self.cnn_encoder = cnn_encoder\n",
    "        self.vit_encoder = vit_encoder\n",
    "        # Combined encoder_dim is the sum of both encoder dimensions.\n",
    "        # (CNN outputs 512 channels; ViT outputs its own embed_dim.)\n",
    "        self.encoder_dim = cnn_encoder.encoder_dim + vit_encoder.encoder_dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Get CNN features: expected shape [B, 49, 512]\n",
    "        cnn_features = self.cnn_encoder(x)\n",
    "\n",
    "        # Get ViT features: expected shape [B, 197, vit_dim] for vit_large_patch16_224\n",
    "        vit_features = self.vit_encoder(x)\n",
    "        # If the ViT output contains a class token, remove it.\n",
    "        if vit_features.shape[1] == 197:\n",
    "            vit_features = vit_features[:, 1:, :]  # Now shape: [B, 196, vit_dim]\n",
    "            B, tokens, D = vit_features.shape\n",
    "            # Reshape tokens into a 14x14 grid: [B, 14, 14, D]\n",
    "            vit_features = vit_features.reshape(B, 14, 14, D)\n",
    "            # Permute to [B, D, 14, 14] for pooling\n",
    "            vit_features = vit_features.permute(0, 3, 1, 2)\n",
    "            # Use adaptive pooling to reduce to a 7x7 grid\n",
    "            vit_features = F.adaptive_avg_pool2d(vit_features, (7, 7))\n",
    "            # Permute back to [B, 7, 7, D] and flatten to [B, 49, D]\n",
    "            vit_features = vit_features.permute(0, 2, 3, 1).reshape(B, -1, D)\n",
    "\n",
    "        # Concatenate the features along the feature dimension (dim=2)\n",
    "        # cnn_features: [B, 49, 512] and vit_features: [B, 49, vit_dim]\n",
    "        hybrid_features = torch.cat([cnn_features, vit_features], dim=2)\n",
    "        return hybrid_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "793fd95b-6f1e-4c77-9bdb-efb501e919a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, encoder_dim, decoder_dim, attention_dim):\n",
    "        super(Attention, self).__init__()\n",
    "        self.encoder_att = nn.Linear(encoder_dim, attention_dim)  # transform encoder output\n",
    "        self.decoder_att = nn.Linear(decoder_dim, attention_dim)  # transform decoder hidden\n",
    "        self.full_att    = nn.Linear(attention_dim, 1)\n",
    "        self.relu        = nn.ReLU()\n",
    "        self.softmax     = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, encoder_out, decoder_hidden):\n",
    "        \"\"\"\n",
    "        encoder_out:    [batch_size, num_patches, encoder_dim]\n",
    "        decoder_hidden: [batch_size, decoder_dim]\n",
    "        \"\"\"\n",
    "        att1 = self.encoder_att(encoder_out)                  # [batch_size, num_patches, attention_dim]\n",
    "        att2 = self.decoder_att(decoder_hidden).unsqueeze(1)  # [batch_size, 1, attention_dim]\n",
    "\n",
    "        # sum -> relu -> full_att -> squeeze -> softmax\n",
    "        att  = self.full_att(self.relu(att1 + att2)).squeeze(2)  # [batch_size, num_patches]\n",
    "        alpha = self.softmax(att)\n",
    "        # Weighted sum of the encoder_out\n",
    "        attention_weighted_encoding = (encoder_out * alpha.unsqueeze(2)).sum(dim=1)  # [batch_size, encoder_dim]\n",
    "        return attention_weighted_encoding, alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b1e35eb8-1577-422f-b680-4fd83a0c79f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, attention_dim, embed_dim, decoder_dim, vocab_size,\n",
    "                 encoder_dim=768, teacher_forcing_ratio=0.5):\n",
    "        super().__init__()\n",
    "\n",
    "        self.tfr       = teacher_forcing_ratio\n",
    "        self.attention = Attention(encoder_dim, decoder_dim, attention_dim)\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.dropout   = nn.Dropout(0.5)\n",
    "\n",
    "        self.rnn1 = nn.GRUCell(embed_dim + encoder_dim, decoder_dim)\n",
    "        self.rnn2 = nn.GRUCell(decoder_dim, decoder_dim)\n",
    "\n",
    "        self.init_h1 = nn.Linear(encoder_dim, decoder_dim)\n",
    "        self.init_h2 = nn.Linear(encoder_dim, decoder_dim)\n",
    "\n",
    "        self.f_beta  = nn.Linear(decoder_dim, encoder_dim)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.fc      = nn.Linear(decoder_dim, vocab_size)\n",
    "\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        nn.init.uniform_(self.embedding.weight, -0.1, 0.1)\n",
    "        nn.init.uniform_(self.fc.weight,       -0.1, 0.1)\n",
    "        nn.init.zeros_(self.fc.bias)\n",
    "\n",
    "    def init_hidden_state(self, encoder_out):\n",
    "        mean_enc = encoder_out.mean(dim=1)\n",
    "        h1 = self.init_h1(mean_enc)\n",
    "        h2 = self.init_h2(mean_enc)\n",
    "        return h1, h2\n",
    "\n",
    "    def forward(self, encoder_out, encoded_captions, caption_lengths):\n",
    "        caption_lengths, sort_ind = caption_lengths.squeeze(1).sort(0, True)\n",
    "        encoder_out      = encoder_out[sort_ind]\n",
    "        encoded_captions = encoded_captions[sort_ind]\n",
    "        embeddings       = self.embedding(encoded_captions)\n",
    "\n",
    "        h1, h2 = self.init_hidden_state(encoder_out)\n",
    "\n",
    "        decode_lens = (caption_lengths - 1).tolist()\n",
    "        T = max(decode_lens)\n",
    "        B = encoder_out.size(0)\n",
    "        V = self.fc.out_features\n",
    "        P = encoder_out.size(1)\n",
    "\n",
    "        predictions = torch.zeros(B, T, V, device=encoder_out.device)\n",
    "        alphas      = torch.zeros(B, T, P, device=encoder_out.device)\n",
    "\n",
    "        prev_tok = encoded_captions[:, 0].clone()\n",
    "\n",
    "        for t in range(T):\n",
    "            Bt = sum(l > t for l in decode_lens)\n",
    "\n",
    "            ctx, alpha = self.attention(encoder_out[:Bt], h1[:Bt])\n",
    "            gate = self.sigmoid(self.f_beta(h1[:Bt])) * ctx\n",
    "\n",
    "            use_tf = torch.rand(1).item() < self.tfr\n",
    "            cur_in = embeddings[:Bt, t] if use_tf else self.embedding(prev_tok[:Bt])\n",
    "            x = torch.cat([cur_in, gate], 1)\n",
    "\n",
    "            h1n = self.rnn1(x, h1[:Bt])\n",
    "            h2n = self.rnn2(h1n, h2[:Bt])\n",
    "\n",
    "            out                 = self.fc(self.dropout(h2n))\n",
    "            predictions[:Bt, t] = out\n",
    "            alphas[:Bt, t]      = alpha\n",
    "\n",
    "            # Get next‐token and update buffer without in-place\n",
    "            next_tokens = out.argmax(1).detach()\n",
    "            prev_new    = prev_tok.clone()\n",
    "            prev_new[:Bt] = next_tokens\n",
    "            prev_tok    = prev_new\n",
    "\n",
    "            # Update hidden states safely (as before)\n",
    "            h1_new = h1.clone(); h1_new[:Bt] = h1n\n",
    "            h2_new = h2.clone(); h2_new[:Bt] = h2n\n",
    "            h1, h2 = h1_new, h2_new\n",
    "        return predictions, encoded_captions, decode_lens, alphas, sort_ind\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c64f622d-9133-4436-b189-ae127e5afadf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageCaptioningTrainer:\n",
    "    def __init__(self, encoder, decoder, \n",
    "                 criterion, encoder_optimizer, decoder_optimizer, \n",
    "                 train_loader, val_loader, test_loader, test_data, max_label_length_test,\n",
    "                 device, char_to_idx, idx_to_char, max_label_length,\n",
    "                 model_name, csv_filename=\"training_results.csv\"):\n",
    "        self.encoder = encoder.to(device)\n",
    "        self.decoder = decoder.to(device)\n",
    "        self.criterion = criterion\n",
    "        self.encoder_optimizer = encoder_optimizer\n",
    "        self.decoder_optimizer = decoder_optimizer\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.test_loader = test_loader\n",
    "        self.test_data = test_data\n",
    "        self.max_label_length_test = max_label_length_test\n",
    "        self.device = device\n",
    "        self.char_to_idx = char_to_idx\n",
    "        self.idx_to_char = idx_to_char\n",
    "        self.max_label_length = max_label_length\n",
    "        self.model_name = model_name\n",
    "        self.csv_filename = csv_filename\n",
    "\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "        self.train_cers = []\n",
    "        self.val_cers = []\n",
    "        self.test_cers = []\n",
    "\n",
    "    def fit(self, num_epochs):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Early stopping parameters\n",
    "        best_val_loss = float('inf')\n",
    "        early_stop_counter = 0\n",
    "        early_stop_patience = 10\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
    "            train_loss, train_cer = self.train_one_epoch()\n",
    "            val_loss, val_cer = self.validate_one_epoch(top_n=5)\n",
    "\n",
    "            print(f\"[{epoch+1}/{num_epochs}] \"\n",
    "                  f\"Train Loss: {train_loss:.4f}, Train CER: {train_cer:.4f} | \"\n",
    "                  f\"Val Loss: {val_loss:.4f}, Val CER: {val_cer:.4f}\")\n",
    "\n",
    "            # Store epoch results\n",
    "            self.train_losses.append(train_loss)\n",
    "            self.val_losses.append(val_loss)\n",
    "            self.train_cers.append(train_cer)\n",
    "            self.val_cers.append(val_cer)\n",
    "            \n",
    "            # Early stopping check: if current val_loss is better, reset counter; otherwise, increment.\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                early_stop_counter = 0\n",
    "            else:\n",
    "                early_stop_counter += 1\n",
    "                print(f\"Validation loss did not improve. Early stop counter: {early_stop_counter}/{early_stop_patience}\")\n",
    "            \n",
    "            if early_stop_counter >= early_stop_patience:\n",
    "                print(\"Early stopping triggered.\")\n",
    "                break\n",
    "        \n",
    "        # Calculate total training time\n",
    "        end_time = time.time() \n",
    "        total_time = end_time - start_time\n",
    "        hours = int(total_time // 3600)\n",
    "        minutes = int((total_time % 3600) // 60)\n",
    "\n",
    "        print(f\"\\nTraining completed in {hours}h {minutes}m.\")\n",
    "\n",
    "        num_epochs_recorded = len(self.train_losses)\n",
    "        epoch_cols = [f\"epoch{i+1}\" for i in range(num_epochs_recorded)]\n",
    "\n",
    "        # Create the new data block to insert\n",
    "        new_rows = pd.DataFrame([\n",
    "            [self.model_name, \"training loss\"] + self.train_losses,\n",
    "            [self.model_name, \"validation loss\"] + self.val_losses,\n",
    "            [self.model_name, \"training cer\"] + self.train_cers,\n",
    "            [self.model_name, \"validation cer\"] + self.val_cers\n",
    "        ], columns=[\"model_name\", \"mode\"] + epoch_cols)\n",
    "        \n",
    "        # Check if CSV already exists\n",
    "        if os.path.exists(self.csv_filename):\n",
    "            df_existing = pd.read_csv(self.csv_filename)\n",
    "            df_existing = df_existing[df_existing[\"model_name\"] != self.model_name]\n",
    "            df_updated = pd.concat([df_existing, new_rows], ignore_index=True)\n",
    "        else:\n",
    "            df_updated = new_rows\n",
    "\n",
    "        df_updated[epoch_cols] = np.floor(df_updated[epoch_cols] * 100) / 100 \n",
    "        \n",
    "        # Save the updated CSV\n",
    "        df_updated.to_csv(self.csv_filename, index=False)\n",
    "        print(f\"\\nResults have been written to: {self.csv_filename}\")\n",
    "\n",
    "        # Save model weights\n",
    "        # torch.save(self.encoder.state_dict(), f\"encoder_{self.model_name}.pth\")\n",
    "        # torch.save(self.decoder.state_dict(), f\"decoder_{self.model_name}.pth\")\n",
    "        # print(f\"Encoder and decoder models saved: encoder_{self.model_name}.pth, decoder_{self.model_name}.pth\")\n",
    "\n",
    "    def train_one_epoch(self):\n",
    "        self.encoder.train()\n",
    "        self.decoder.train()\n",
    "        running_loss           = 0.0\n",
    "        total_edit_distance    = 0\n",
    "        total_ref_length       = 0\n",
    "\n",
    "        for batch_idx, (images, labels, label_lengths) in enumerate(self.train_loader):\n",
    "            images        = images.to(self.device, non_blocking=True)\n",
    "            labels        = labels.to(self.device, non_blocking=True)\n",
    "            label_lengths = label_lengths.to(self.device, non_blocking=True)\n",
    "\n",
    "            self.encoder_optimizer.zero_grad()\n",
    "            self.decoder_optimizer.zero_grad()\n",
    "\n",
    "            encoder_out   = self.encoder(images)\n",
    "            caption_lengths = torch.tensor(\n",
    "                [self.max_label_length] * labels.size(0)\n",
    "            ).unsqueeze(1).to(self.device)\n",
    "\n",
    "            outputs, encoded_captions, decode_lengths, alphas, sort_ind = self.decoder(\n",
    "                encoder_out, labels, caption_lengths\n",
    "            )\n",
    "\n",
    "            # Targets = encoded captions without the <SOS>\n",
    "            targets = encoded_captions[:, 1:]\n",
    "\n",
    "            # Flatten for loss\n",
    "            outputs_flat = outputs.view(-1, self.decoder.fc.out_features)\n",
    "            targets_flat = targets.contiguous().view(-1)\n",
    "\n",
    "            loss = self.criterion(outputs_flat, targets_flat)\n",
    "            loss.backward()\n",
    "\n",
    "            self.decoder_optimizer.step()\n",
    "            self.encoder_optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            # Compute CER for the batch (global style)\n",
    "            batch_size = labels.size(0)\n",
    "            _, preds_flat = torch.max(outputs_flat, dim=1)\n",
    "            preds_seq = preds_flat.view(batch_size, -1)\n",
    "\n",
    "            for i in range(batch_size):\n",
    "                pred_indices   = preds_seq[i].detach().cpu().numpy()\n",
    "                target_indices = targets[i].detach().cpu().numpy()\n",
    "\n",
    "                mask          = (target_indices != self.char_to_idx['<PAD>'])\n",
    "                pred_indices  = pred_indices[mask]\n",
    "                target_indices= target_indices[mask]\n",
    "\n",
    "                pred_chars    = [self.idx_to_char.get(idx, '') for idx in pred_indices]\n",
    "                target_chars  = [self.idx_to_char.get(idx, '') for idx in target_indices]\n",
    "                pred_str      = ''.join(pred_chars)\n",
    "                target_str    = ''.join(target_chars)\n",
    "\n",
    "                edit_dist           = editdistance.eval(pred_str, target_str)\n",
    "                total_edit_distance += edit_dist\n",
    "                total_ref_length    += len(target_str)\n",
    "\n",
    "            # if (batch_idx + 1) % 50 == 0:\n",
    "            #     print(f'Batch {batch_idx + 1}/{len(self.train_loader)} - Loss: {loss.item():.4f}')\n",
    "\n",
    "        avg_loss = running_loss / len(self.train_loader)\n",
    "        avg_cer  = total_edit_distance / total_ref_length if total_ref_length > 0 else 0.0\n",
    "        return avg_loss, avg_cer\n",
    "\n",
    "    def validate_one_epoch(self, top_n=5):\n",
    "        self.encoder.eval()\n",
    "        self.decoder.eval()\n",
    "        running_loss         = 0.0\n",
    "        total_edit_distance  = 0\n",
    "        total_ref_length     = 0\n",
    "\n",
    "        # each sample’s CER\n",
    "        sample_cer_info = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (images, labels, label_lengths) in enumerate(self.val_loader):\n",
    "                images        = images.to(self.device, non_blocking=True)\n",
    "                labels        = labels.to(self.device, non_blocking=True)\n",
    "                label_lengths = label_lengths.to(self.device, non_blocking=True)\n",
    "\n",
    "                encoder_out = self.encoder(images)\n",
    "                caption_lengths = torch.tensor(\n",
    "                    [self.max_label_length] * labels.size(0)\n",
    "                ).unsqueeze(1).to(self.device)\n",
    "\n",
    "                outputs, encoded_captions, decode_lengths, alphas, sort_ind = self.decoder(\n",
    "                    encoder_out, labels, caption_lengths\n",
    "                )\n",
    "                targets = encoded_captions[:, 1:]\n",
    "\n",
    "                outputs_flat = outputs.view(-1, self.decoder.fc.out_features)\n",
    "                targets_flat = targets.contiguous().view(-1)\n",
    "\n",
    "                loss = self.criterion(outputs_flat, targets_flat)\n",
    "                running_loss += loss.item()\n",
    "\n",
    "                batch_size = labels.size(0)\n",
    "                _, preds_flat = torch.max(outputs_flat, dim=1)\n",
    "                preds_seq = preds_flat.view(batch_size, -1)\n",
    "\n",
    "                for i in range(batch_size):\n",
    "                    pred_indices   = preds_seq[i].detach().cpu().numpy()\n",
    "                    target_indices = targets[i].detach().cpu().numpy()\n",
    "\n",
    "                    mask           = (target_indices != self.char_to_idx['<PAD>'])\n",
    "                    pred_indices   = pred_indices[mask]\n",
    "                    target_indices = target_indices[mask]\n",
    "\n",
    "                    pred_chars   = [self.idx_to_char.get(idx, '') for idx in pred_indices]\n",
    "                    target_chars = [self.idx_to_char.get(idx, '') for idx in target_indices]\n",
    "                    pred_str     = ''.join(pred_chars)\n",
    "                    target_str   = ''.join(target_chars)\n",
    "\n",
    "                    edit_dist = editdistance.eval(pred_str, target_str)\n",
    "                    ref_len   = len(target_str)\n",
    "                    cer       = edit_dist / ref_len if ref_len > 0 else 0\n",
    "    \n",
    "                    total_edit_distance += edit_dist\n",
    "                    total_ref_length    += ref_len\n",
    "    \n",
    "                    # Store sample info\n",
    "                    # sample_cer_info.append({\n",
    "                    #     \"pred\": pred_str,\n",
    "                    #     \"gt\": target_str,\n",
    "                    #     \"cer\": cer\n",
    "                    # })\n",
    "\n",
    "                    # Print a few samples from the 1st batch\n",
    "                    # if batch_idx == 0 and i < 3:\n",
    "                    #     print(f\"Sample {i + 1}:\")\n",
    "                    #     print(f\"Predicted: {pred_str}\")\n",
    "                    #     print(f\"Target   : {target_str}\\n\")\n",
    "\n",
    "        avg_loss = running_loss / len(self.val_loader)\n",
    "        avg_cer  = total_edit_distance / total_ref_length if total_ref_length > 0 else 0.0\n",
    "\n",
    "        # Sort by CER descending\n",
    "        sample_cer_info.sort(key=lambda x: x[\"cer\"], reverse=True)\n",
    "        # Take top_n\n",
    "        worst_samples = sample_cer_info[:top_n]\n",
    "    \n",
    "        # print(f\"\\n=== Top {top_n} Worst Samples by CER ===\")\n",
    "        # for idx, sample in enumerate(worst_samples):\n",
    "        #     print(f\"[{idx+1}] CER: {sample['cer']:.3f}\")\n",
    "        #     print(f\"   Predicted: {sample['pred']}\")\n",
    "        #     print(f\"   Ground Truth: {sample['gt']}\\n\")\n",
    "       \n",
    "        return avg_loss, avg_cer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "58f94879-7a3d-41a8-98cf-c64a0dc83d1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unknown characters in test labels: set()\n"
     ]
    }
   ],
   "source": [
    "test_ground_truth_path = os.path.join(DATA_DIR, 'balinese_transliteration_test.txt')\n",
    "test_images_dir        = os.path.join(DATA_DIR, 'balinese_word_test')\n",
    "\n",
    "test_filenames = []\n",
    "test_labels    = []\n",
    "\n",
    "with open(test_ground_truth_path, 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        line = line.strip()\n",
    "        if line:\n",
    "            parts = line.split(';')\n",
    "            if len(parts) == 2:\n",
    "                filename, label = parts\n",
    "                label = label.lower()\n",
    "                test_filenames.append(filename)\n",
    "                test_labels.append(label)\n",
    "            else:\n",
    "                print(f\"Skipping malformed line: {line}\")\n",
    "\n",
    "test_data = pd.DataFrame({\n",
    "    'filename': test_filenames,\n",
    "    'label': test_labels\n",
    "})\n",
    "\n",
    "# Check for unknown chars in test set\n",
    "test_chars = set(''.join(test_data['label']))\n",
    "unknown_chars = test_chars - set(char_to_idx.keys())\n",
    "print(f\"Unknown characters in test labels: {unknown_chars}\")\n",
    "\n",
    "# Encode test labels\n",
    "max_label_length_test = max(len(lbl) for lbl in test_data['label']) + 2\n",
    "def encode_label_test(label, char_to_idx, max_length):\n",
    "    encoded = (\n",
    "        [char_to_idx['<SOS>']] +\n",
    "        [char_to_idx.get(ch, char_to_idx['<UNK>']) for ch in label] +\n",
    "        [char_to_idx['<EOS>']]\n",
    "    )\n",
    "    if len(encoded) > max_length:\n",
    "        encoded = encoded[:max_length]\n",
    "    else:\n",
    "        encoded += [char_to_idx['<PAD>']] * (max_length - len(encoded))\n",
    "    return encoded\n",
    "\n",
    "test_data['encoded_label'] = test_data['label'].apply(lambda x: encode_label_test(x, char_to_idx, max_label_length_test))\n",
    "test_data['label_length']  = test_data['label'].apply(len)\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=(0.5, 0.5, 0.5),\n",
    "        std=(0.5, 0.5, 0.5)\n",
    "    )\n",
    "])\n",
    "\n",
    "test_dataset = BalineseDataset(test_data, test_images_dir, transform=test_transform)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9384c147-036c-46e8-afd0-bf7cc1ad8ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(encoder, decoder, data_loader, device, char_to_idx, idx_to_char, max_seq_length, test_data):\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "\n",
    "    eos_idx = char_to_idx['<EOS>']\n",
    "    results = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (images, labels, label_lengths) in enumerate(data_loader):\n",
    "            images = images.to(device, non_blocking=True)\n",
    "            labels = labels.to(device, non_blocking=True)\n",
    "\n",
    "            batch_size = images.size(0)\n",
    "            encoder_out = encoder(images)  # [B, num_patches, encoder_dim]\n",
    "\n",
    "            h1, h2 = decoder.init_hidden_state(encoder_out)\n",
    "\n",
    "            # Start tokens (all <SOS>)\n",
    "            inputs = torch.full(\n",
    "                (batch_size,),\n",
    "                fill_value=char_to_idx['<SOS>'],\n",
    "                dtype=torch.long,\n",
    "                device=device\n",
    "            )\n",
    "\n",
    "            all_preds = []\n",
    "\n",
    "            for _ in range(max_seq_length):\n",
    "                embeddings = decoder.embedding(inputs)\n",
    "                attn_ctx, _ = decoder.attention(encoder_out, h1)\n",
    "                gate = decoder.sigmoid(decoder.f_beta(h1))\n",
    "                attn_ctx = gate * attn_ctx\n",
    "                x = torch.cat([embeddings, attn_ctx], dim=1)\n",
    "\n",
    "                h1 = decoder.rnn1(x, h1)\n",
    "                h2 = decoder.rnn2(h1, h2)\n",
    "\n",
    "                logits = decoder.fc(decoder.dropout(h2))\n",
    "                preds_idx = logits.argmax(dim=1)\n",
    "\n",
    "                all_preds.append(preds_idx.cpu().numpy())\n",
    "                inputs = preds_idx\n",
    "\n",
    "            all_preds = np.array(all_preds).T\n",
    "\n",
    "            for i in range(batch_size):\n",
    "                pred_indices = all_preds[i]\n",
    "                if eos_idx in pred_indices:\n",
    "                    pred_indices = pred_indices[:np.where(pred_indices == eos_idx)[0][0]]\n",
    "\n",
    "                pred_str = ''.join([idx_to_char.get(idx, '') for idx in pred_indices])\n",
    "\n",
    "                label_indices = labels[i].cpu().numpy()[1:]  # remove <SOS>\n",
    "                if eos_idx in label_indices:\n",
    "                    label_indices = label_indices[:np.where(label_indices == eos_idx)[0][0]]\n",
    "                else:\n",
    "                    label_indices = label_indices[label_indices != char_to_idx['<PAD>']]\n",
    "\n",
    "                label_str = ''.join([idx_to_char.get(idx, '') for idx in label_indices])\n",
    "                image_filename = test_data.iloc[batch_idx * batch_size + i]['filename']\n",
    "\n",
    "                results.append({\n",
    "                    'image_filename': image_filename,\n",
    "                    'predicted_caption': pred_str,\n",
    "                    'ground_truth_caption': label_str\n",
    "                })\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "897c292c-2a83-4f60-a0d4-6156f8ce9011",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_global_cer(results):\n",
    "    total_ed   = 0\n",
    "    total_refs = 0\n",
    "    for r in results:\n",
    "        ref = r['ground_truth_caption']\n",
    "        hyp = r['predicted_caption']\n",
    "        dist = editdistance.eval(ref, hyp)\n",
    "        total_ed   += dist\n",
    "        total_refs += len(ref)\n",
    "    if total_refs == 0:\n",
    "        return 0.0\n",
    "    return total_ed / total_refs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c297dda1-cd38-4fc3-a091-9a3503a71d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_top_worst_samples(results, n=5):\n",
    "    # Calculate CER for each sample\n",
    "    results_with_cer = []\n",
    "    for r in results:\n",
    "        ref = r['ground_truth_caption']\n",
    "        hyp = r['predicted_caption']\n",
    "        dist = editdistance.eval(ref, hyp)\n",
    "        length = len(ref)\n",
    "        cer = dist / length if length > 0 else 0\n",
    "        # Copy the record and add cer\n",
    "        new_r = r.copy()\n",
    "        new_r['cer'] = cer\n",
    "        results_with_cer.append(new_r)\n",
    "\n",
    "    # Sort by CER (descending) and take the top N\n",
    "    results_with_cer.sort(key=lambda x: x['cer'], reverse=True)\n",
    "    worst_samples = results_with_cer[:n]\n",
    "\n",
    "    print(f\"\\n=== Top {n} Worst Samples by CER ===\")\n",
    "    for i, sample in enumerate(worst_samples, start=1):\n",
    "        print(f\"{i}) Image: {sample['image_filename']}\")\n",
    "        print(f\"   CER: {sample['cer']:.4f}\")\n",
    "        print(f\"   Predicted       : {sample['predicted_caption']}\")\n",
    "        print(f\"   Ground Truth    : {sample['ground_truth_caption']}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fb63e5c1-f5e6-4053-a166-1d4ab4dab40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure CSV files exist\n",
    "training_csv = \"training_results.csv\"\n",
    "if not os.path.exists(training_csv) or os.path.getsize(training_csv) == 0:\n",
    "    pd.DataFrame(columns=[\"model_name\", \"mode\", \"epoch1\", \"epoch2\"]).to_csv(training_csv, index=False)\n",
    "\n",
    "csv_file = \"test_cer_results.csv\"\n",
    "if not os.path.exists(csv_file) or os.path.getsize(csv_file) == 0:\n",
    "    pd.DataFrame(columns=[\"model_name\", \"test_cer\"]).to_csv(csv_file, index=False)\n",
    "\n",
    "def log_test_cer(model_name, cer_value):\n",
    "    \"\"\"\n",
    "    Logs or updates the test CER for a given model, rounding values to 4 decimals.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(csv_file)\n",
    "    \n",
    "    # Round the new CER value to 4 decimals\n",
    "    cer_rounded = round(cer_value, 4)\n",
    "    \n",
    "    if model_name in df['model_name'].values:\n",
    "        # Update existing row\n",
    "        df.loc[df['model_name'] == model_name, 'test_cer'] = cer_rounded\n",
    "    else:\n",
    "        # Add new row\n",
    "        new_row = pd.DataFrame({\n",
    "            \"model_name\": [model_name],\n",
    "            \"test_cer\":   [cer_rounded]\n",
    "        })\n",
    "        df = pd.concat([df, new_row], ignore_index=True)\n",
    "    \n",
    "    # Ensure all stored values are rounded to 4 decimals\n",
    "    df['test_cer'] = df['test_cer'].round(4)\n",
    "    \n",
    "    # Save back to CSV\n",
    "    df.to_csv(csv_file, index=False)\n",
    "    print(f\"Logged {model_name}: {cer_rounded:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "02c43587-227f-46a6-aebf-591f6c5c1a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training_pipeline(encoder_class,encoder_kwargs, model_name,vocab_size,encoder_lr, decoder_lr,train_loader,\n",
    "                          val_loader,test_loader,char_to_idx,idx_to_char,max_label_length,max_label_length_test,test_data,\n",
    "                          device, num_epochs=100):\n",
    "    #build encoder & grab its dimension\n",
    "    encoder = encoder_class(**encoder_kwargs).to(device)\n",
    "    # if encoder_dim is None (e.g. Swin), prime it with a dummy batch\n",
    "    if encoder.encoder_dim is None:\n",
    "        with torch.no_grad():\n",
    "            dummy = torch.randn(1, 3, 224, 224, device=device)\n",
    "            _ = encoder(dummy)\n",
    "    enc_dim = encoder.encoder_dim\n",
    "\n",
    "    #build decoder \n",
    "    decoder = DecoderRNN(\n",
    "        attention_dim=256,\n",
    "        embed_dim=256,\n",
    "        decoder_dim=512,\n",
    "        vocab_size=vocab_size,\n",
    "        encoder_dim=enc_dim,\n",
    "        teacher_forcing_ratio=0.5\n",
    "    ).to(device)\n",
    "\n",
    "    # loss, optimizers, trainer\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=char_to_idx['<PAD>'])\n",
    "    encoder_optimizer = optim.Adam(encoder.parameters(), lr=encoder_lr)\n",
    "    decoder_optimizer = optim.Adam(decoder.parameters(), lr=decoder_lr)\n",
    "\n",
    "    # trainer = ImageCaptioningTrainer(\n",
    "    #     encoder=encoder,\n",
    "    #     decoder=decoder,\n",
    "    #     criterion=criterion,\n",
    "    #     encoder_optimizer=encoder_optimizer,\n",
    "    #     decoder_optimizer=decoder_optimizer,\n",
    "    #     train_loader=train_loader,\n",
    "    #     val_loader=val_loader,\n",
    "    #     device=device,\n",
    "    #     char_to_idx=char_to_idx,\n",
    "    #     idx_to_char=idx_to_char,\n",
    "    #     max_label_length=max_label_length,\n",
    "    #     model_name=model_name)\n",
    "    trainer = ImageCaptioningTrainer(\n",
    "        encoder=encoder,\n",
    "        decoder=decoder,\n",
    "        criterion=criterion,\n",
    "        encoder_optimizer=encoder_optimizer,\n",
    "        decoder_optimizer=decoder_optimizer,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        test_loader=test_loader,  \n",
    "        test_data=test_data,      \n",
    "        max_label_length_test=max_label_length_test,  \n",
    "        device=device,\n",
    "        char_to_idx=char_to_idx,\n",
    "        idx_to_char=idx_to_char,\n",
    "        max_label_length=max_label_length,\n",
    "        model_name=model_name\n",
    "    )\n",
    "\n",
    "    trainer.fit(num_epochs)\n",
    "\n",
    "    encoder.eval()\n",
    "    decoder.teacher_forcing_ratio = 0.0\n",
    "\n",
    "    results = inference(\n",
    "        encoder=encoder,\n",
    "        decoder=decoder,\n",
    "        data_loader=test_loader,\n",
    "        device=device,\n",
    "        char_to_idx=char_to_idx,\n",
    "        idx_to_char=idx_to_char,\n",
    "        max_seq_length=max_label_length_test,\n",
    "        test_data=test_data\n",
    "    )\n",
    "\n",
    "    cer = calculate_global_cer(results)\n",
    "    print(f\"{model_name} — Test CER: {cer:.4f}\")\n",
    "    print_top_worst_samples(results, n=5)\n",
    "    log_test_cer(model_name, cer)\n",
    "\n",
    "    del encoder, decoder, trainer\n",
    "    torch.cuda.empty_cache()\n",
    "    print(f\"Memory cleared for {model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1e0a2980-8a58-4b99-9258-aac3be507afe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/apps/miniforge3/24.3.0-0/miniforge3/envs/dsks_2024.06/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/apps/miniforge3/24.3.0-0/miniforge3/envs/dsks_2024.06/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/100\n",
      "[1/100] Train Loss: 2.1580, Train CER: 0.7473 | Val Loss: 1.3239, Val CER: 0.3728\n",
      "\n",
      "Epoch 2/100\n",
      "[2/100] Train Loss: 1.6570, Train CER: 0.5174 | Val Loss: 0.8514, Val CER: 0.2219\n",
      "\n",
      "Epoch 3/100\n",
      "[3/100] Train Loss: 1.3585, Train CER: 0.4152 | Val Loss: 0.5136, Val CER: 0.1354\n",
      "\n",
      "Epoch 4/100\n",
      "[4/100] Train Loss: 1.2155, Train CER: 0.3671 | Val Loss: 0.4654, Val CER: 0.1284\n",
      "\n",
      "Epoch 5/100\n",
      "[5/100] Train Loss: 1.1179, Train CER: 0.3368 | Val Loss: 0.3426, Val CER: 0.0863\n",
      "\n",
      "Epoch 6/100\n",
      "[6/100] Train Loss: 1.0495, Train CER: 0.3184 | Val Loss: 0.2951, Val CER: 0.0798\n",
      "\n",
      "Epoch 7/100\n",
      "[7/100] Train Loss: 0.9660, Train CER: 0.2938 | Val Loss: 0.2668, Val CER: 0.0751\n",
      "\n",
      "Epoch 8/100\n",
      "[8/100] Train Loss: 0.9442, Train CER: 0.2905 | Val Loss: 0.2790, Val CER: 0.0765\n",
      "Validation loss did not improve. Early stop counter: 1/10\n",
      "\n",
      "Epoch 9/100\n",
      "[9/100] Train Loss: 0.9266, Train CER: 0.2806 | Val Loss: 0.2656, Val CER: 0.0773\n",
      "\n",
      "Epoch 10/100\n",
      "[10/100] Train Loss: 0.8727, Train CER: 0.2636 | Val Loss: 0.2940, Val CER: 0.0796\n",
      "Validation loss did not improve. Early stop counter: 1/10\n",
      "\n",
      "Epoch 11/100\n",
      "[11/100] Train Loss: 0.8597, Train CER: 0.2643 | Val Loss: 0.2264, Val CER: 0.0620\n",
      "\n",
      "Epoch 12/100\n",
      "[12/100] Train Loss: 0.8406, Train CER: 0.2538 | Val Loss: 0.2671, Val CER: 0.0694\n",
      "Validation loss did not improve. Early stop counter: 1/10\n",
      "\n",
      "Epoch 13/100\n",
      "[13/100] Train Loss: 0.8249, Train CER: 0.2531 | Val Loss: 0.2574, Val CER: 0.0692\n",
      "Validation loss did not improve. Early stop counter: 2/10\n",
      "\n",
      "Epoch 14/100\n",
      "[14/100] Train Loss: 0.7964, Train CER: 0.2426 | Val Loss: 0.2143, Val CER: 0.0610\n",
      "\n",
      "Epoch 15/100\n",
      "[15/100] Train Loss: 0.7873, Train CER: 0.2436 | Val Loss: 0.2259, Val CER: 0.0570\n",
      "Validation loss did not improve. Early stop counter: 1/10\n",
      "\n",
      "Epoch 16/100\n",
      "[16/100] Train Loss: 0.7781, Train CER: 0.2389 | Val Loss: 0.2109, Val CER: 0.0551\n",
      "\n",
      "Epoch 17/100\n",
      "[17/100] Train Loss: 0.7679, Train CER: 0.2352 | Val Loss: 0.2542, Val CER: 0.0678\n",
      "Validation loss did not improve. Early stop counter: 1/10\n",
      "\n",
      "Epoch 18/100\n",
      "[18/100] Train Loss: 0.7644, Train CER: 0.2369 | Val Loss: 0.2027, Val CER: 0.0566\n",
      "\n",
      "Epoch 19/100\n",
      "[19/100] Train Loss: 0.7304, Train CER: 0.2250 | Val Loss: 0.2141, Val CER: 0.0551\n",
      "Validation loss did not improve. Early stop counter: 1/10\n",
      "\n",
      "Epoch 20/100\n",
      "[20/100] Train Loss: 0.7388, Train CER: 0.2270 | Val Loss: 0.2736, Val CER: 0.0633\n",
      "Validation loss did not improve. Early stop counter: 2/10\n",
      "\n",
      "Epoch 21/100\n",
      "[21/100] Train Loss: 0.7336, Train CER: 0.2251 | Val Loss: 0.2188, Val CER: 0.0520\n",
      "Validation loss did not improve. Early stop counter: 3/10\n",
      "\n",
      "Epoch 22/100\n",
      "[22/100] Train Loss: 0.7125, Train CER: 0.2173 | Val Loss: 0.2272, Val CER: 0.0551\n",
      "Validation loss did not improve. Early stop counter: 4/10\n",
      "\n",
      "Epoch 23/100\n",
      "[23/100] Train Loss: 0.7236, Train CER: 0.2215 | Val Loss: 0.2417, Val CER: 0.0581\n",
      "Validation loss did not improve. Early stop counter: 5/10\n",
      "\n",
      "Epoch 24/100\n",
      "[24/100] Train Loss: 0.7052, Train CER: 0.2185 | Val Loss: 0.2469, Val CER: 0.0571\n",
      "Validation loss did not improve. Early stop counter: 6/10\n",
      "\n",
      "Epoch 25/100\n",
      "[25/100] Train Loss: 0.7360, Train CER: 0.2291 | Val Loss: 0.2184, Val CER: 0.0552\n",
      "Validation loss did not improve. Early stop counter: 7/10\n",
      "\n",
      "Epoch 26/100\n",
      "[26/100] Train Loss: 0.6940, Train CER: 0.2151 | Val Loss: 0.2065, Val CER: 0.0500\n",
      "Validation loss did not improve. Early stop counter: 8/10\n",
      "\n",
      "Epoch 27/100\n",
      "[27/100] Train Loss: 0.7016, Train CER: 0.2148 | Val Loss: 0.2286, Val CER: 0.0515\n",
      "Validation loss did not improve. Early stop counter: 9/10\n",
      "\n",
      "Epoch 28/100\n",
      "[28/100] Train Loss: 0.6750, Train CER: 0.2070 | Val Loss: 0.2005, Val CER: 0.0462\n",
      "\n",
      "Epoch 29/100\n",
      "[29/100] Train Loss: 0.6729, Train CER: 0.2055 | Val Loss: 0.2280, Val CER: 0.0520\n",
      "Validation loss did not improve. Early stop counter: 1/10\n",
      "\n",
      "Epoch 30/100\n",
      "[30/100] Train Loss: 0.6746, Train CER: 0.2070 | Val Loss: 0.2449, Val CER: 0.0566\n",
      "Validation loss did not improve. Early stop counter: 2/10\n",
      "\n",
      "Epoch 31/100\n",
      "[31/100] Train Loss: 0.6656, Train CER: 0.2029 | Val Loss: 0.2325, Val CER: 0.0559\n",
      "Validation loss did not improve. Early stop counter: 3/10\n",
      "\n",
      "Epoch 32/100\n",
      "[32/100] Train Loss: 0.6865, Train CER: 0.2126 | Val Loss: 0.2463, Val CER: 0.0525\n",
      "Validation loss did not improve. Early stop counter: 4/10\n",
      "\n",
      "Epoch 33/100\n",
      "[33/100] Train Loss: 0.6769, Train CER: 0.2115 | Val Loss: 0.2108, Val CER: 0.0525\n",
      "Validation loss did not improve. Early stop counter: 5/10\n",
      "\n",
      "Epoch 34/100\n",
      "[34/100] Train Loss: 0.6733, Train CER: 0.2061 | Val Loss: 0.2074, Val CER: 0.0478\n",
      "Validation loss did not improve. Early stop counter: 6/10\n",
      "\n",
      "Epoch 35/100\n",
      "[35/100] Train Loss: 0.6688, Train CER: 0.2063 | Val Loss: 0.1871, Val CER: 0.0475\n",
      "\n",
      "Epoch 36/100\n",
      "[36/100] Train Loss: 0.6581, Train CER: 0.2020 | Val Loss: 0.2243, Val CER: 0.0517\n",
      "Validation loss did not improve. Early stop counter: 1/10\n",
      "\n",
      "Epoch 37/100\n",
      "[37/100] Train Loss: 0.6690, Train CER: 0.2070 | Val Loss: 0.2592, Val CER: 0.0585\n",
      "Validation loss did not improve. Early stop counter: 2/10\n",
      "\n",
      "Epoch 38/100\n",
      "[38/100] Train Loss: 0.6688, Train CER: 0.2083 | Val Loss: 0.2200, Val CER: 0.0504\n",
      "Validation loss did not improve. Early stop counter: 3/10\n",
      "\n",
      "Epoch 39/100\n",
      "[39/100] Train Loss: 0.6490, Train CER: 0.2005 | Val Loss: 0.2226, Val CER: 0.0501\n",
      "Validation loss did not improve. Early stop counter: 4/10\n",
      "\n",
      "Epoch 40/100\n",
      "[40/100] Train Loss: 0.6524, Train CER: 0.2020 | Val Loss: 0.2550, Val CER: 0.0533\n",
      "Validation loss did not improve. Early stop counter: 5/10\n",
      "\n",
      "Epoch 41/100\n",
      "[41/100] Train Loss: 0.6664, Train CER: 0.2034 | Val Loss: 0.1992, Val CER: 0.0478\n",
      "Validation loss did not improve. Early stop counter: 6/10\n",
      "\n",
      "Epoch 42/100\n",
      "[42/100] Train Loss: 0.6475, Train CER: 0.1948 | Val Loss: 0.2345, Val CER: 0.0506\n",
      "Validation loss did not improve. Early stop counter: 7/10\n",
      "\n",
      "Epoch 43/100\n",
      "[43/100] Train Loss: 0.6645, Train CER: 0.2035 | Val Loss: 0.2820, Val CER: 0.0550\n",
      "Validation loss did not improve. Early stop counter: 8/10\n",
      "\n",
      "Epoch 44/100\n",
      "[44/100] Train Loss: 0.6545, Train CER: 0.2012 | Val Loss: 0.2018, Val CER: 0.0475\n",
      "Validation loss did not improve. Early stop counter: 9/10\n",
      "\n",
      "Epoch 45/100\n",
      "[45/100] Train Loss: 0.6568, Train CER: 0.2007 | Val Loss: 0.2564, Val CER: 0.0563\n",
      "Validation loss did not improve. Early stop counter: 10/10\n",
      "Early stopping triggered.\n",
      "\n",
      "Training completed in 3h 12m.\n",
      "\n",
      "Results have been written to: training_results.csv\n",
      "resnet18_encoder — Test CER: 0.1455\n",
      "\n",
      "=== Top 5 Worst Samples by CER ===\n",
      "1) Image: test3029.png\n",
      "   CER: 10.0000\n",
      "   Predicted       : .arik-agung\n",
      "   Ground Truth    : .\n",
      "\n",
      "2) Image: test1330.png\n",
      "   CER: 9.0000\n",
      "   Predicted       : pangianga\n",
      "   Ground Truth    : .\n",
      "\n",
      "3) Image: test8296.png\n",
      "   CER: 8.0000\n",
      "   Predicted       : kunangsa\n",
      "   Ground Truth    : .\n",
      "\n",
      "4) Image: test2856.png\n",
      "   CER: 7.0000\n",
      "   Predicted       : .etaania\n",
      "   Ground Truth    : .\n",
      "\n",
      "5) Image: test4371.png\n",
      "   CER: 7.0000\n",
      "   Predicted       : karinga\n",
      "   Ground Truth    : .\n",
      "\n",
      "Logged resnet18_encoder: 0.1455\n",
      "Memory cleared for resnet18_encoder\n"
     ]
    }
   ],
   "source": [
    "run_training_pipeline(\n",
    "    encoder_class = ResNet18Encoder,\n",
    "    encoder_kwargs= {},                              \n",
    "    model_name    = \"resnet18_encoder\",\n",
    "    vocab_size    = vocab_size,\n",
    "    encoder_lr    = 1e-4,\n",
    "    decoder_lr    = 4e-4,\n",
    "    train_loader  = train_loader,\n",
    "    val_loader    = val_loader,\n",
    "    test_loader   = test_loader,\n",
    "    char_to_idx   = char_to_idx,\n",
    "    idx_to_char   = idx_to_char,\n",
    "    max_label_length      = max_label_length,\n",
    "    max_label_length_test = max_label_length_test,\n",
    "    test_data      = test_data,\n",
    "    device         = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "32a1bd21-9997-4e3d-9393-58678957806e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/100\n",
      "[1/100] Train Loss: 2.1881, Train CER: 0.7433 | Val Loss: 1.4046, Val CER: 0.3720\n",
      "\n",
      "Epoch 2/100\n",
      "[2/100] Train Loss: 1.6602, Train CER: 0.5217 | Val Loss: 0.7885, Val CER: 0.2183\n",
      "\n",
      "Epoch 3/100\n",
      "[3/100] Train Loss: 1.3436, Train CER: 0.4106 | Val Loss: 0.4807, Val CER: 0.1416\n",
      "\n",
      "Epoch 4/100\n",
      "[4/100] Train Loss: 1.2030, Train CER: 0.3672 | Val Loss: 0.4055, Val CER: 0.1067\n",
      "\n",
      "Epoch 5/100\n",
      "[5/100] Train Loss: 1.1047, Train CER: 0.3352 | Val Loss: 0.3339, Val CER: 0.0798\n",
      "\n",
      "Epoch 6/100\n",
      "[6/100] Train Loss: 1.0593, Train CER: 0.3207 | Val Loss: 0.2937, Val CER: 0.0792\n",
      "\n",
      "Epoch 7/100\n",
      "[7/100] Train Loss: 1.0052, Train CER: 0.3030 | Val Loss: 0.2901, Val CER: 0.0763\n",
      "\n",
      "Epoch 8/100\n",
      "[8/100] Train Loss: 0.9622, Train CER: 0.2895 | Val Loss: 0.2696, Val CER: 0.0762\n",
      "\n",
      "Epoch 9/100\n",
      "[9/100] Train Loss: 0.9286, Train CER: 0.2848 | Val Loss: 0.2512, Val CER: 0.0673\n",
      "\n",
      "Epoch 10/100\n",
      "[10/100] Train Loss: 0.8980, Train CER: 0.2731 | Val Loss: 0.2876, Val CER: 0.0737\n",
      "Validation loss did not improve. Early stop counter: 1/10\n",
      "\n",
      "Epoch 11/100\n",
      "[11/100] Train Loss: 0.8894, Train CER: 0.2722 | Val Loss: 0.3028, Val CER: 0.0823\n",
      "Validation loss did not improve. Early stop counter: 2/10\n",
      "\n",
      "Epoch 12/100\n",
      "[12/100] Train Loss: 0.8785, Train CER: 0.2709 | Val Loss: 0.2091, Val CER: 0.0596\n",
      "\n",
      "Epoch 13/100\n",
      "[13/100] Train Loss: 0.8680, Train CER: 0.2639 | Val Loss: 0.1878, Val CER: 0.0477\n",
      "\n",
      "Epoch 14/100\n",
      "[14/100] Train Loss: 0.8523, Train CER: 0.2576 | Val Loss: 0.2212, Val CER: 0.0593\n",
      "Validation loss did not improve. Early stop counter: 1/10\n",
      "\n",
      "Epoch 15/100\n",
      "[15/100] Train Loss: 0.8459, Train CER: 0.2571 | Val Loss: 0.1999, Val CER: 0.0562\n",
      "Validation loss did not improve. Early stop counter: 2/10\n",
      "\n",
      "Epoch 16/100\n",
      "[16/100] Train Loss: 0.8205, Train CER: 0.2486 | Val Loss: 0.2281, Val CER: 0.0562\n",
      "Validation loss did not improve. Early stop counter: 3/10\n",
      "\n",
      "Epoch 17/100\n",
      "[17/100] Train Loss: 0.8133, Train CER: 0.2491 | Val Loss: 0.2051, Val CER: 0.0573\n",
      "Validation loss did not improve. Early stop counter: 4/10\n",
      "\n",
      "Epoch 18/100\n",
      "[18/100] Train Loss: 0.7658, Train CER: 0.2309 | Val Loss: 0.1728, Val CER: 0.0492\n",
      "\n",
      "Epoch 19/100\n",
      "[19/100] Train Loss: 0.7940, Train CER: 0.2378 | Val Loss: 0.2498, Val CER: 0.0623\n",
      "Validation loss did not improve. Early stop counter: 1/10\n",
      "\n",
      "Epoch 20/100\n",
      "[20/100] Train Loss: 0.7915, Train CER: 0.2422 | Val Loss: 0.2095, Val CER: 0.0556\n",
      "Validation loss did not improve. Early stop counter: 2/10\n",
      "\n",
      "Epoch 21/100\n",
      "[21/100] Train Loss: 0.7851, Train CER: 0.2365 | Val Loss: 0.1825, Val CER: 0.0469\n",
      "Validation loss did not improve. Early stop counter: 3/10\n",
      "\n",
      "Epoch 22/100\n",
      "[22/100] Train Loss: 0.7307, Train CER: 0.2240 | Val Loss: 0.2690, Val CER: 0.0640\n",
      "Validation loss did not improve. Early stop counter: 4/10\n",
      "\n",
      "Epoch 23/100\n",
      "[23/100] Train Loss: 0.7729, Train CER: 0.2406 | Val Loss: 0.1951, Val CER: 0.0614\n",
      "Validation loss did not improve. Early stop counter: 5/10\n",
      "\n",
      "Epoch 24/100\n",
      "[24/100] Train Loss: 0.7367, Train CER: 0.2254 | Val Loss: 0.1898, Val CER: 0.0542\n",
      "Validation loss did not improve. Early stop counter: 6/10\n",
      "\n",
      "Epoch 25/100\n",
      "[25/100] Train Loss: 0.7356, Train CER: 0.2224 | Val Loss: 0.1939, Val CER: 0.0538\n",
      "Validation loss did not improve. Early stop counter: 7/10\n",
      "\n",
      "Epoch 26/100\n",
      "[26/100] Train Loss: 0.7350, Train CER: 0.2217 | Val Loss: 0.2091, Val CER: 0.0514\n",
      "Validation loss did not improve. Early stop counter: 8/10\n",
      "\n",
      "Epoch 27/100\n",
      "[27/100] Train Loss: 0.7185, Train CER: 0.2174 | Val Loss: 0.1961, Val CER: 0.0478\n",
      "Validation loss did not improve. Early stop counter: 9/10\n",
      "\n",
      "Epoch 28/100\n",
      "[28/100] Train Loss: 0.7083, Train CER: 0.2144 | Val Loss: 0.1983, Val CER: 0.0473\n",
      "Validation loss did not improve. Early stop counter: 10/10\n",
      "Early stopping triggered.\n",
      "\n",
      "Training completed in 2h 5m.\n",
      "\n",
      "Results have been written to: training_results.csv\n",
      "vit_base_patch16_224 — Test CER: 0.1338\n",
      "\n",
      "=== Top 5 Worst Samples by CER ===\n",
      "1) Image: test580.png\n",
      "   CER: 8.0000\n",
      "   Predicted       : siangnia\n",
      "   Ground Truth    : .\n",
      "\n",
      "2) Image: test1330.png\n",
      "   CER: 8.0000\n",
      "   Predicted       : sangsana\n",
      "   Ground Truth    : .\n",
      "\n",
      "3) Image: test1335.png\n",
      "   CER: 5.0000\n",
      "   Predicted       : panti\n",
      "   Ground Truth    : .\n",
      "\n",
      "4) Image: test2690.png\n",
      "   CER: 5.0000\n",
      "   Predicted       : adapa\n",
      "   Ground Truth    : b\n",
      "\n",
      "5) Image: test3828.png\n",
      "   CER: 5.0000\n",
      "   Predicted       : panti\n",
      "   Ground Truth    : .\n",
      "\n",
      "Logged vit_base_patch16_224: 0.1338\n",
      "Memory cleared for vit_base_patch16_224\n"
     ]
    }
   ],
   "source": [
    "run_training_pipeline(\n",
    "    encoder_class = ViTEncoder,\n",
    "    encoder_kwargs= {\"model_name\":\"vit_base_patch16_224\",\"pretrained\":True},\n",
    "    model_name    = \"vit_base_patch16_224\",\n",
    "    vocab_size    = vocab_size,\n",
    "    encoder_lr    = 1e-4,\n",
    "    decoder_lr    = 4e-4,\n",
    "    train_loader  = train_loader,\n",
    "    val_loader    = val_loader,\n",
    "    test_loader   = test_loader,\n",
    "    char_to_idx   = char_to_idx,\n",
    "    idx_to_char   = idx_to_char,\n",
    "    max_label_length      = max_label_length,\n",
    "    max_label_length_test = max_label_length_test,\n",
    "    test_data      = test_data,\n",
    "    device         = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "28906722-8ed0-4ee9-bd02-8ab57712e445",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/100\n",
      "[1/100] Train Loss: 2.1350, Train CER: 0.7332 | Val Loss: 1.1122, Val CER: 0.3106\n",
      "\n",
      "Epoch 2/100\n",
      "[2/100] Train Loss: 1.5096, Train CER: 0.4735 | Val Loss: 0.6125, Val CER: 0.1844\n",
      "\n",
      "Epoch 3/100\n",
      "[3/100] Train Loss: 1.2189, Train CER: 0.3731 | Val Loss: 0.4282, Val CER: 0.1121\n",
      "\n",
      "Epoch 4/100\n",
      "[4/100] Train Loss: 1.0953, Train CER: 0.3340 | Val Loss: 0.3449, Val CER: 0.0983\n",
      "\n",
      "Epoch 5/100\n",
      "[5/100] Train Loss: 1.0551, Train CER: 0.3173 | Val Loss: 0.3105, Val CER: 0.0821\n",
      "\n",
      "Epoch 6/100\n",
      "[6/100] Train Loss: 0.9565, Train CER: 0.2892 | Val Loss: 0.2690, Val CER: 0.0781\n",
      "\n",
      "Epoch 7/100\n",
      "[7/100] Train Loss: 0.9688, Train CER: 0.2969 | Val Loss: 0.2808, Val CER: 0.0682\n",
      "Validation loss did not improve. Early stop counter: 1/10\n",
      "\n",
      "Epoch 8/100\n",
      "[8/100] Train Loss: 0.9273, Train CER: 0.2821 | Val Loss: 0.2277, Val CER: 0.0608\n",
      "\n",
      "Epoch 9/100\n",
      "[9/100] Train Loss: 0.8878, Train CER: 0.2703 | Val Loss: 0.2307, Val CER: 0.0625\n",
      "Validation loss did not improve. Early stop counter: 1/10\n",
      "\n",
      "Epoch 10/100\n",
      "[10/100] Train Loss: 0.8641, Train CER: 0.2654 | Val Loss: 0.2115, Val CER: 0.0584\n",
      "\n",
      "Epoch 11/100\n",
      "[11/100] Train Loss: 0.8513, Train CER: 0.2586 | Val Loss: 0.2381, Val CER: 0.0685\n",
      "Validation loss did not improve. Early stop counter: 1/10\n",
      "\n",
      "Epoch 12/100\n",
      "[12/100] Train Loss: 0.8421, Train CER: 0.2521 | Val Loss: 0.1790, Val CER: 0.0515\n",
      "\n",
      "Epoch 13/100\n",
      "[13/100] Train Loss: 0.7929, Train CER: 0.2415 | Val Loss: 0.2244, Val CER: 0.0545\n",
      "Validation loss did not improve. Early stop counter: 1/10\n",
      "\n",
      "Epoch 14/100\n",
      "[14/100] Train Loss: 0.8264, Train CER: 0.2534 | Val Loss: 0.2271, Val CER: 0.0628\n",
      "Validation loss did not improve. Early stop counter: 2/10\n",
      "\n",
      "Epoch 15/100\n",
      "[15/100] Train Loss: 0.8115, Train CER: 0.2496 | Val Loss: 0.1781, Val CER: 0.0478\n",
      "\n",
      "Epoch 16/100\n",
      "[16/100] Train Loss: 0.7922, Train CER: 0.2426 | Val Loss: 0.1789, Val CER: 0.0463\n",
      "Validation loss did not improve. Early stop counter: 1/10\n",
      "\n",
      "Epoch 17/100\n",
      "[17/100] Train Loss: 0.7695, Train CER: 0.2349 | Val Loss: 0.2130, Val CER: 0.0532\n",
      "Validation loss did not improve. Early stop counter: 2/10\n",
      "\n",
      "Epoch 18/100\n",
      "[18/100] Train Loss: 0.7702, Train CER: 0.2342 | Val Loss: 0.1867, Val CER: 0.0497\n",
      "Validation loss did not improve. Early stop counter: 3/10\n",
      "\n",
      "Epoch 19/100\n",
      "[19/100] Train Loss: 0.7417, Train CER: 0.2293 | Val Loss: 0.2075, Val CER: 0.0538\n",
      "Validation loss did not improve. Early stop counter: 4/10\n",
      "\n",
      "Epoch 20/100\n",
      "[20/100] Train Loss: 0.7546, Train CER: 0.2316 | Val Loss: 0.2036, Val CER: 0.0479\n",
      "Validation loss did not improve. Early stop counter: 5/10\n",
      "\n",
      "Epoch 21/100\n",
      "[21/100] Train Loss: 0.7745, Train CER: 0.2342 | Val Loss: 0.1906, Val CER: 0.0468\n",
      "Validation loss did not improve. Early stop counter: 6/10\n",
      "\n",
      "Epoch 22/100\n",
      "[22/100] Train Loss: 0.7415, Train CER: 0.2278 | Val Loss: 0.1894, Val CER: 0.0453\n",
      "Validation loss did not improve. Early stop counter: 7/10\n",
      "\n",
      "Epoch 23/100\n",
      "[23/100] Train Loss: 0.7257, Train CER: 0.2220 | Val Loss: 0.2040, Val CER: 0.0477\n",
      "Validation loss did not improve. Early stop counter: 8/10\n",
      "\n",
      "Epoch 24/100\n",
      "[24/100] Train Loss: 0.7070, Train CER: 0.2165 | Val Loss: 0.1799, Val CER: 0.0477\n",
      "Validation loss did not improve. Early stop counter: 9/10\n",
      "\n",
      "Epoch 25/100\n",
      "[25/100] Train Loss: 0.7435, Train CER: 0.2277 | Val Loss: 0.1844, Val CER: 0.0440\n",
      "Validation loss did not improve. Early stop counter: 10/10\n",
      "Early stopping triggered.\n",
      "\n",
      "Training completed in 2h 36m.\n",
      "\n",
      "Results have been written to: training_results.csv\n",
      "vit_large_patch16_224 — Test CER: 0.1215\n",
      "\n",
      "=== Top 5 Worst Samples by CER ===\n",
      "1) Image: test580.png\n",
      "   CER: 8.0000\n",
      "   Predicted       : buatania\n",
      "   Ground Truth    : .\n",
      "\n",
      "2) Image: test1330.png\n",
      "   CER: 8.0000\n",
      "   Predicted       : pangkala\n",
      "   Ground Truth    : .\n",
      "\n",
      "3) Image: test5564.png\n",
      "   CER: 8.0000\n",
      "   Predicted       : adeg-adeg\n",
      "   Ground Truth    : e\n",
      "\n",
      "4) Image: test8296.png\n",
      "   CER: 6.0000\n",
      "   Predicted       : balang\n",
      "   Ground Truth    : .\n",
      "\n",
      "5) Image: test1335.png\n",
      "   CER: 5.0000\n",
      "   Predicted       : panti\n",
      "   Ground Truth    : .\n",
      "\n",
      "Logged vit_large_patch16_224: 0.1215\n",
      "Memory cleared for vit_large_patch16_224\n"
     ]
    }
   ],
   "source": [
    "run_training_pipeline(\n",
    "    encoder_class = ViTEncoder,\n",
    "    encoder_kwargs= {\"model_name\": \"vit_large_patch16_224\", \"pretrained\": True},\n",
    "    model_name    = \"vit_large_patch16_224\",\n",
    "    vocab_size    = vocab_size,\n",
    "    encoder_lr    = 1e-4,\n",
    "    decoder_lr    = 4e-4,\n",
    "    train_loader  = train_loader,\n",
    "    val_loader    = val_loader,\n",
    "    test_loader   = test_loader,\n",
    "    char_to_idx   = char_to_idx,\n",
    "    idx_to_char   = idx_to_char,\n",
    "    max_label_length      = max_label_length,\n",
    "    max_label_length_test = max_label_length_test,\n",
    "    test_data      = test_data,\n",
    "    device         = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "48be8ef4-df37-44ac-bfc6-347cb29ab6a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/apps/miniforge3/24.3.0-0/miniforge3/envs/dsks_2024.06/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/apps/miniforge3/24.3.0-0/miniforge3/envs/dsks_2024.06/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/100\n",
      "[1/100] Train Loss: 2.1714, Train CER: 0.7438 | Val Loss: 1.3491, Val CER: 0.3508\n",
      "\n",
      "Epoch 2/100\n",
      "[2/100] Train Loss: 1.6279, Train CER: 0.5142 | Val Loss: 0.7246, Val CER: 0.2302\n",
      "\n",
      "Epoch 3/100\n",
      "[3/100] Train Loss: 1.3342, Train CER: 0.4046 | Val Loss: 0.5167, Val CER: 0.1393\n",
      "\n",
      "Epoch 4/100\n",
      "[4/100] Train Loss: 1.1662, Train CER: 0.3547 | Val Loss: 0.3753, Val CER: 0.1046\n",
      "\n",
      "Epoch 5/100\n",
      "[5/100] Train Loss: 1.0890, Train CER: 0.3281 | Val Loss: 0.3154, Val CER: 0.0912\n",
      "\n",
      "Epoch 6/100\n",
      "[6/100] Train Loss: 1.0415, Train CER: 0.3147 | Val Loss: 0.3370, Val CER: 0.0906\n",
      "Validation loss did not improve. Early stop counter: 1/10\n",
      "\n",
      "Epoch 7/100\n",
      "[7/100] Train Loss: 0.9717, Train CER: 0.2948 | Val Loss: 0.3105, Val CER: 0.0800\n",
      "\n",
      "Epoch 8/100\n",
      "[8/100] Train Loss: 0.9423, Train CER: 0.2864 | Val Loss: 0.3008, Val CER: 0.0793\n",
      "\n",
      "Epoch 9/100\n",
      "[9/100] Train Loss: 0.9198, Train CER: 0.2820 | Val Loss: 0.2537, Val CER: 0.0713\n",
      "\n",
      "Epoch 10/100\n",
      "[10/100] Train Loss: 0.8629, Train CER: 0.2621 | Val Loss: 0.2372, Val CER: 0.0605\n",
      "\n",
      "Epoch 11/100\n",
      "[11/100] Train Loss: 0.8595, Train CER: 0.2621 | Val Loss: 0.2576, Val CER: 0.0648\n",
      "Validation loss did not improve. Early stop counter: 1/10\n",
      "\n",
      "Epoch 12/100\n",
      "[12/100] Train Loss: 0.8449, Train CER: 0.2611 | Val Loss: 0.2801, Val CER: 0.0786\n",
      "Validation loss did not improve. Early stop counter: 2/10\n",
      "\n",
      "Epoch 13/100\n",
      "[13/100] Train Loss: 0.8203, Train CER: 0.2549 | Val Loss: 0.2843, Val CER: 0.0635\n",
      "Validation loss did not improve. Early stop counter: 3/10\n",
      "\n",
      "Epoch 14/100\n",
      "[14/100] Train Loss: 0.8012, Train CER: 0.2500 | Val Loss: 0.2248, Val CER: 0.0581\n",
      "\n",
      "Epoch 15/100\n",
      "[15/100] Train Loss: 0.7821, Train CER: 0.2404 | Val Loss: 0.2485, Val CER: 0.0591\n",
      "Validation loss did not improve. Early stop counter: 1/10\n",
      "\n",
      "Epoch 16/100\n",
      "[16/100] Train Loss: 0.7487, Train CER: 0.2298 | Val Loss: 0.2644, Val CER: 0.0637\n",
      "Validation loss did not improve. Early stop counter: 2/10\n",
      "\n",
      "Epoch 17/100\n",
      "[17/100] Train Loss: 0.7626, Train CER: 0.2343 | Val Loss: 0.2410, Val CER: 0.0592\n",
      "Validation loss did not improve. Early stop counter: 3/10\n",
      "\n",
      "Epoch 18/100\n",
      "[18/100] Train Loss: 0.7345, Train CER: 0.2261 | Val Loss: 0.2216, Val CER: 0.0515\n",
      "\n",
      "Epoch 19/100\n",
      "[19/100] Train Loss: 0.7338, Train CER: 0.2277 | Val Loss: 0.2444, Val CER: 0.0727\n",
      "Validation loss did not improve. Early stop counter: 1/10\n",
      "\n",
      "Epoch 20/100\n",
      "[20/100] Train Loss: 0.7443, Train CER: 0.2287 | Val Loss: 0.2330, Val CER: 0.0656\n",
      "Validation loss did not improve. Early stop counter: 2/10\n",
      "\n",
      "Epoch 21/100\n",
      "[21/100] Train Loss: 0.7274, Train CER: 0.2285 | Val Loss: 0.2052, Val CER: 0.0475\n",
      "\n",
      "Epoch 22/100\n",
      "[22/100] Train Loss: 0.7245, Train CER: 0.2227 | Val Loss: 0.2028, Val CER: 0.0535\n",
      "\n",
      "Epoch 23/100\n",
      "[23/100] Train Loss: 0.7114, Train CER: 0.2198 | Val Loss: 0.2137, Val CER: 0.0556\n",
      "Validation loss did not improve. Early stop counter: 1/10\n",
      "\n",
      "Epoch 24/100\n",
      "[24/100] Train Loss: 0.7017, Train CER: 0.2151 | Val Loss: 0.2223, Val CER: 0.0535\n",
      "Validation loss did not improve. Early stop counter: 2/10\n",
      "\n",
      "Epoch 25/100\n",
      "[25/100] Train Loss: 0.7017, Train CER: 0.2159 | Val Loss: 0.2564, Val CER: 0.0676\n",
      "Validation loss did not improve. Early stop counter: 3/10\n",
      "\n",
      "Epoch 26/100\n",
      "[26/100] Train Loss: 0.6887, Train CER: 0.2164 | Val Loss: 0.2342, Val CER: 0.0592\n",
      "Validation loss did not improve. Early stop counter: 4/10\n",
      "\n",
      "Epoch 27/100\n",
      "[27/100] Train Loss: 0.6973, Train CER: 0.2169 | Val Loss: 0.2261, Val CER: 0.0529\n",
      "Validation loss did not improve. Early stop counter: 5/10\n",
      "\n",
      "Epoch 28/100\n",
      "[28/100] Train Loss: 0.6745, Train CER: 0.2087 | Val Loss: 0.2300, Val CER: 0.0562\n",
      "Validation loss did not improve. Early stop counter: 6/10\n",
      "\n",
      "Epoch 29/100\n",
      "[29/100] Train Loss: 0.6759, Train CER: 0.2073 | Val Loss: 0.2227, Val CER: 0.0485\n",
      "Validation loss did not improve. Early stop counter: 7/10\n",
      "\n",
      "Epoch 30/100\n",
      "[30/100] Train Loss: 0.6684, Train CER: 0.2038 | Val Loss: 0.2287, Val CER: 0.0552\n",
      "Validation loss did not improve. Early stop counter: 8/10\n",
      "\n",
      "Epoch 31/100\n",
      "[31/100] Train Loss: 0.6793, Train CER: 0.2092 | Val Loss: 0.2464, Val CER: 0.0559\n",
      "Validation loss did not improve. Early stop counter: 9/10\n",
      "\n",
      "Epoch 32/100\n",
      "[32/100] Train Loss: 0.6750, Train CER: 0.2092 | Val Loss: 0.2340, Val CER: 0.0540\n",
      "Validation loss did not improve. Early stop counter: 10/10\n",
      "Early stopping triggered.\n",
      "\n",
      "Training completed in 2h 23m.\n",
      "\n",
      "Results have been written to: training_results.csv\n",
      "hybrid_cnn_vit_base_encoder — Test CER: 0.1488\n",
      "\n",
      "=== Top 5 Worst Samples by CER ===\n",
      "1) Image: test262.png\n",
      "   CER: 11.0000\n",
      "   Predicted       : carik-agung\n",
      "   Ground Truth    : .\n",
      "\n",
      "2) Image: test4934.png\n",
      "   CER: 11.0000\n",
      "   Predicted       : carik-agung\n",
      "   Ground Truth    : .\n",
      "\n",
      "3) Image: test1330.png\n",
      "   CER: 8.0000\n",
      "   Predicted       : pangsala\n",
      "   Ground Truth    : .\n",
      "\n",
      "4) Image: test10070.png\n",
      "   CER: 8.0000\n",
      "   Predicted       : nglingsi\n",
      "   Ground Truth    : .\n",
      "\n",
      "5) Image: test8296.png\n",
      "   CER: 7.0000\n",
      "   Predicted       : sangsia\n",
      "   Ground Truth    : .\n",
      "\n",
      "Logged hybrid_cnn_vit_base_encoder: 0.1488\n",
      "Memory cleared for hybrid_cnn_vit_base_encoder\n"
     ]
    }
   ],
   "source": [
    "run_training_pipeline(\n",
    "    encoder_class = HybridEncoder,\n",
    "    encoder_kwargs= {\n",
    "        \"cnn_encoder\": ResNet18Encoder(pretrained=True),\n",
    "        \"vit_encoder\": ViTEncoder(model_name=\"vit_base_patch16_224\", pretrained=True)\n",
    "    },\n",
    "    model_name    = \"hybrid_cnn_vit_base_encoder\",\n",
    "    vocab_size    = vocab_size,\n",
    "    encoder_lr    = 1e-4,\n",
    "    decoder_lr    = 4e-4,\n",
    "    train_loader  = train_loader,\n",
    "    val_loader    = val_loader,\n",
    "    test_loader   = test_loader,\n",
    "    char_to_idx   = char_to_idx,\n",
    "    idx_to_char   = idx_to_char,\n",
    "    max_label_length      = max_label_length,\n",
    "    max_label_length_test = max_label_length_test,\n",
    "    test_data      = test_data,\n",
    "    device         = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "10f7e5ce-f746-4935-962e-876d3f5fcccf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/apps/miniforge3/24.3.0-0/miniforge3/envs/dsks_2024.06/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/apps/miniforge3/24.3.0-0/miniforge3/envs/dsks_2024.06/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/100\n",
      "[1/100] Train Loss: 2.1760, Train CER: 0.7424 | Val Loss: 1.2965, Val CER: 0.3439\n",
      "\n",
      "Epoch 2/100\n",
      "[2/100] Train Loss: 1.5839, Train CER: 0.4975 | Val Loss: 0.6410, Val CER: 0.1765\n",
      "\n",
      "Epoch 3/100\n",
      "[3/100] Train Loss: 1.2821, Train CER: 0.3925 | Val Loss: 0.4545, Val CER: 0.1392\n",
      "\n",
      "Epoch 4/100\n",
      "[4/100] Train Loss: 1.1335, Train CER: 0.3459 | Val Loss: 0.3821, Val CER: 0.0981\n",
      "\n",
      "Epoch 5/100\n",
      "[5/100] Train Loss: 1.0529, Train CER: 0.3188 | Val Loss: 0.3238, Val CER: 0.0855\n",
      "\n",
      "Epoch 6/100\n",
      "[6/100] Train Loss: 1.0069, Train CER: 0.3070 | Val Loss: 0.2804, Val CER: 0.0816\n",
      "\n",
      "Epoch 7/100\n",
      "[7/100] Train Loss: 0.9460, Train CER: 0.2909 | Val Loss: 0.2953, Val CER: 0.0836\n",
      "Validation loss did not improve. Early stop counter: 1/10\n",
      "\n",
      "Epoch 8/100\n",
      "[8/100] Train Loss: 0.9234, Train CER: 0.2820 | Val Loss: 0.2735, Val CER: 0.0707\n",
      "\n",
      "Epoch 9/100\n",
      "[9/100] Train Loss: 0.8773, Train CER: 0.2689 | Val Loss: 0.2366, Val CER: 0.0674\n",
      "\n",
      "Epoch 10/100\n",
      "[10/100] Train Loss: 0.8627, Train CER: 0.2635 | Val Loss: 0.2658, Val CER: 0.0769\n",
      "Validation loss did not improve. Early stop counter: 1/10\n",
      "\n",
      "Epoch 11/100\n",
      "[12/100] Train Loss: 0.8358, Train CER: 0.2548 | Val Loss: 0.2118, Val CER: 0.0538\n",
      "\n",
      "Epoch 13/100\n",
      "[13/100] Train Loss: 0.8089, Train CER: 0.2483 | Val Loss: 0.2150, Val CER: 0.0576\n",
      "Validation loss did not improve. Early stop counter: 1/10\n",
      "\n",
      "Epoch 14/100\n",
      "[14/100] Train Loss: 0.7888, Train CER: 0.2422 | Val Loss: 0.1936, Val CER: 0.0507\n",
      "\n",
      "Epoch 15/100\n",
      "[15/100] Train Loss: 0.7684, Train CER: 0.2370 | Val Loss: 0.1852, Val CER: 0.0493\n",
      "\n",
      "Epoch 16/100\n",
      "[16/100] Train Loss: 0.7823, Train CER: 0.2395 | Val Loss: 0.2167, Val CER: 0.0581\n",
      "Validation loss did not improve. Early stop counter: 1/10\n",
      "\n",
      "Epoch 17/100\n",
      "[17/100] Train Loss: 0.7543, Train CER: 0.2327 | Val Loss: 0.2304, Val CER: 0.0679\n",
      "Validation loss did not improve. Early stop counter: 2/10\n",
      "\n",
      "Epoch 18/100\n",
      "[18/100] Train Loss: 0.7626, Train CER: 0.2335 | Val Loss: 0.2129, Val CER: 0.0498\n",
      "Validation loss did not improve. Early stop counter: 3/10\n",
      "\n",
      "Epoch 19/100\n",
      "[19/100] Train Loss: 0.7500, Train CER: 0.2305 | Val Loss: 0.2066, Val CER: 0.0496\n",
      "Validation loss did not improve. Early stop counter: 4/10\n",
      "\n",
      "Epoch 20/100\n",
      "[20/100] Train Loss: 0.7427, Train CER: 0.2276 | Val Loss: 0.1759, Val CER: 0.0481\n",
      "\n",
      "Epoch 21/100\n",
      "[21/100] Train Loss: 0.7251, Train CER: 0.2198 | Val Loss: 0.1765, Val CER: 0.0441\n",
      "Validation loss did not improve. Early stop counter: 1/10\n",
      "\n",
      "Epoch 22/100\n",
      "[22/100] Train Loss: 0.7186, Train CER: 0.2214 | Val Loss: 0.1849, Val CER: 0.0484\n",
      "Validation loss did not improve. Early stop counter: 2/10\n",
      "\n",
      "Epoch 23/100\n",
      "[23/100] Train Loss: 0.7091, Train CER: 0.2209 | Val Loss: 0.1913, Val CER: 0.0454\n",
      "Validation loss did not improve. Early stop counter: 3/10\n",
      "\n",
      "Epoch 24/100\n",
      "[24/100] Train Loss: 0.7147, Train CER: 0.2202 | Val Loss: 0.2044, Val CER: 0.0555\n",
      "Validation loss did not improve. Early stop counter: 4/10\n",
      "\n",
      "Epoch 25/100\n",
      "[25/100] Train Loss: 0.7125, Train CER: 0.2194 | Val Loss: 0.2058, Val CER: 0.0438\n",
      "Validation loss did not improve. Early stop counter: 5/10\n",
      "\n",
      "Epoch 26/100\n",
      "[26/100] Train Loss: 0.6919, Train CER: 0.2126 | Val Loss: 0.2086, Val CER: 0.0519\n",
      "Validation loss did not improve. Early stop counter: 6/10\n",
      "\n",
      "Epoch 27/100\n",
      "[27/100] Train Loss: 0.7086, Train CER: 0.2200 | Val Loss: 0.1936, Val CER: 0.0475\n",
      "Validation loss did not improve. Early stop counter: 7/10\n",
      "\n",
      "Epoch 28/100\n",
      "[28/100] Train Loss: 0.7136, Train CER: 0.2238 | Val Loss: 0.2006, Val CER: 0.0520\n",
      "Validation loss did not improve. Early stop counter: 8/10\n",
      "\n",
      "Epoch 29/100\n",
      "[29/100] Train Loss: 0.7011, Train CER: 0.2134 | Val Loss: 0.2026, Val CER: 0.0463\n",
      "Validation loss did not improve. Early stop counter: 9/10\n",
      "\n",
      "Epoch 30/100\n",
      "[30/100] Train Loss: 0.6902, Train CER: 0.2085 | Val Loss: 0.2234, Val CER: 0.0516\n",
      "Validation loss did not improve. Early stop counter: 10/10\n",
      "Early stopping triggered.\n",
      "\n",
      "Training completed in 3h 10m.\n",
      "\n",
      "Results have been written to: training_results.csv\n",
      "hybrid_cnn_vit_encoder — Test CER: 0.1249\n",
      "\n",
      "=== Top 5 Worst Samples by CER ===\n",
      "1) Image: test1330.png\n",
      "   CER: 8.0000\n",
      "   Predicted       : sangsana\n",
      "   Ground Truth    : .\n",
      "\n",
      "2) Image: test5688.png\n",
      "   CER: 5.0000\n",
      "   Predicted       : deling\n",
      "   Ground Truth    : i\n",
      "\n",
      "3) Image: test6297.png\n",
      "   CER: 5.0000\n",
      "   Predicted       : panti\n",
      "   Ground Truth    : .\n",
      "\n",
      "4) Image: test9332.png\n",
      "   CER: 5.0000\n",
      "   Predicted       : .aling\n",
      "   Ground Truth    : .\n",
      "\n",
      "5) Image: test9386.png\n",
      "   CER: 5.0000\n",
      "   Predicted       : panti\n",
      "   Ground Truth    : .\n",
      "\n",
      "Logged hybrid_cnn_vit_encoder: 0.1249\n",
      "Memory cleared for hybrid_cnn_vit_encoder\n"
     ]
    }
   ],
   "source": [
    "run_training_pipeline(\n",
    "    encoder_class = HybridEncoder,\n",
    "    encoder_kwargs= {\n",
    "        \"cnn_encoder\": ResNet18Encoder(pretrained=True),\n",
    "        \"vit_encoder\": ViTEncoder(model_name=\"vit_large_patch16_224\", pretrained=True)\n",
    "    },\n",
    "    model_name    = \"hybrid_cnn_vit_encoder\",\n",
    "    vocab_size    = vocab_size,\n",
    "    encoder_lr    = 1e-4,\n",
    "    decoder_lr    = 4e-4,\n",
    "    train_loader  = train_loader,\n",
    "    val_loader    = val_loader,\n",
    "    test_loader   = test_loader,\n",
    "    char_to_idx   = char_to_idx,\n",
    "    idx_to_char   = idx_to_char,\n",
    "    max_label_length      = max_label_length,\n",
    "    max_label_length_test = max_label_length_test,\n",
    "    test_data      = test_data,\n",
    "    device         = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0ca61a07-f184-4f3b-a11c-888c5e5334ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/100\n",
      "[1/100] Train Loss: 2.2820, Train CER: 0.8113 | Val Loss: 1.7817, Val CER: 0.4917\n",
      "\n",
      "Epoch 2/100\n",
      "[2/100] Train Loss: 2.0734, Train CER: 0.6477 | Val Loss: 1.7353, Val CER: 0.4964\n",
      "\n",
      "Epoch 3/100\n",
      "[3/100] Train Loss: 2.0035, Train CER: 0.6129 | Val Loss: 1.6499, Val CER: 0.4385\n",
      "\n",
      "Epoch 4/100\n",
      "[4/100] Train Loss: 1.9394, Train CER: 0.5917 | Val Loss: 1.5072, Val CER: 0.4035\n",
      "\n",
      "Epoch 5/100\n",
      "[5/100] Train Loss: 1.8897, Train CER: 0.5665 | Val Loss: 1.4388, Val CER: 0.3634\n",
      "\n",
      "Epoch 6/100\n",
      "[6/100] Train Loss: 1.8360, Train CER: 0.5424 | Val Loss: 1.3846, Val CER: 0.3269\n",
      "\n",
      "Epoch 7/100\n",
      "[7/100] Train Loss: 1.7585, Train CER: 0.5090 | Val Loss: 1.3227, Val CER: 0.3006\n",
      "\n",
      "Epoch 8/100\n",
      "[8/100] Train Loss: 1.6745, Train CER: 0.4792 | Val Loss: 1.1467, Val CER: 0.2841\n",
      "\n",
      "Epoch 9/100\n",
      "[9/100] Train Loss: 1.6230, Train CER: 0.4619 | Val Loss: 1.0080, Val CER: 0.2359\n",
      "\n",
      "Epoch 10/100\n",
      "[10/100] Train Loss: 1.5744, Train CER: 0.4446 | Val Loss: 0.9687, Val CER: 0.2367\n",
      "\n",
      "Epoch 11/100\n",
      "[11/100] Train Loss: 1.5204, Train CER: 0.4295 | Val Loss: 0.8696, Val CER: 0.2173\n",
      "\n",
      "Epoch 12/100\n",
      "[12/100] Train Loss: 1.4496, Train CER: 0.4072 | Val Loss: 0.7859, Val CER: 0.1780\n",
      "\n",
      "Epoch 13/100\n",
      "[13/100] Train Loss: 1.3994, Train CER: 0.3920 | Val Loss: 0.7265, Val CER: 0.1705\n",
      "\n",
      "Epoch 14/100\n",
      "[14/100] Train Loss: 1.3383, Train CER: 0.3791 | Val Loss: 0.6326, Val CER: 0.1366\n",
      "\n",
      "Epoch 15/100\n",
      "[15/100] Train Loss: 1.2740, Train CER: 0.3566 | Val Loss: 0.6543, Val CER: 0.1523\n",
      "Validation loss did not improve. Early stop counter: 1/10\n",
      "\n",
      "Epoch 16/100\n",
      "[16/100] Train Loss: 1.2304, Train CER: 0.3475 | Val Loss: 0.5203, Val CER: 0.1199\n",
      "\n",
      "Epoch 17/100\n",
      "[17/100] Train Loss: 1.1715, Train CER: 0.3296 | Val Loss: 0.5162, Val CER: 0.1208\n",
      "\n",
      "Epoch 18/100\n",
      "[18/100] Train Loss: 1.1424, Train CER: 0.3222 | Val Loss: 0.3956, Val CER: 0.0897\n",
      "\n",
      "Epoch 19/100\n",
      "[19/100] Train Loss: 1.0819, Train CER: 0.3041 | Val Loss: 0.3848, Val CER: 0.0809\n",
      "\n",
      "Epoch 20/100\n",
      "[20/100] Train Loss: 1.0582, Train CER: 0.3011 | Val Loss: 0.3994, Val CER: 0.0945\n",
      "Validation loss did not improve. Early stop counter: 1/10\n",
      "\n",
      "Epoch 21/100\n",
      "[21/100] Train Loss: 1.0216, Train CER: 0.2895 | Val Loss: 0.3352, Val CER: 0.0815\n",
      "\n",
      "Epoch 22/100\n",
      "[22/100] Train Loss: 0.9952, Train CER: 0.2802 | Val Loss: 0.3303, Val CER: 0.0756\n",
      "\n",
      "Epoch 23/100\n",
      "[23/100] Train Loss: 0.9743, Train CER: 0.2819 | Val Loss: 0.3070, Val CER: 0.0750\n",
      "\n",
      "Epoch 24/100\n",
      "[24/100] Train Loss: 0.9484, Train CER: 0.2728 | Val Loss: 0.3047, Val CER: 0.0721\n",
      "\n",
      "Epoch 25/100\n",
      "[25/100] Train Loss: 0.9294, Train CER: 0.2680 | Val Loss: 0.3256, Val CER: 0.0736\n",
      "Validation loss did not improve. Early stop counter: 1/10\n",
      "\n",
      "Epoch 26/100\n",
      "[26/100] Train Loss: 0.9067, Train CER: 0.2623 | Val Loss: 0.2856, Val CER: 0.0685\n",
      "\n",
      "Epoch 27/100\n",
      "[27/100] Train Loss: 0.8922, Train CER: 0.2576 | Val Loss: 0.2825, Val CER: 0.0648\n",
      "\n",
      "Epoch 28/100\n",
      "[28/100] Train Loss: 0.8769, Train CER: 0.2560 | Val Loss: 0.3014, Val CER: 0.0697\n",
      "Validation loss did not improve. Early stop counter: 1/10\n",
      "\n",
      "Epoch 29/100\n",
      "[29/100] Train Loss: 0.8553, Train CER: 0.2499 | Val Loss: 0.2869, Val CER: 0.0700\n",
      "Validation loss did not improve. Early stop counter: 2/10\n",
      "\n",
      "Epoch 30/100\n",
      "[30/100] Train Loss: 0.8492, Train CER: 0.2481 | Val Loss: 0.2751, Val CER: 0.0731\n",
      "\n",
      "Epoch 31/100\n",
      "[31/100] Train Loss: 0.8353, Train CER: 0.2421 | Val Loss: 0.2611, Val CER: 0.0617\n",
      "\n",
      "Epoch 32/100\n",
      "[32/100] Train Loss: 0.8035, Train CER: 0.2356 | Val Loss: 0.2321, Val CER: 0.0589\n",
      "\n",
      "Epoch 33/100\n",
      "[33/100] Train Loss: 0.8154, Train CER: 0.2380 | Val Loss: 0.2621, Val CER: 0.0604\n",
      "Validation loss did not improve. Early stop counter: 1/10\n",
      "\n",
      "Epoch 34/100\n",
      "[34/100] Train Loss: 0.8042, Train CER: 0.2345 | Val Loss: 0.2874, Val CER: 0.0635\n",
      "Validation loss did not improve. Early stop counter: 2/10\n",
      "\n",
      "Epoch 35/100\n",
      "[35/100] Train Loss: 0.7994, Train CER: 0.2349 | Val Loss: 0.2771, Val CER: 0.0652\n",
      "Validation loss did not improve. Early stop counter: 3/10\n",
      "\n",
      "Epoch 36/100\n",
      "[36/100] Train Loss: 0.8069, Train CER: 0.2393 | Val Loss: 0.2562, Val CER: 0.0613\n",
      "Validation loss did not improve. Early stop counter: 4/10\n",
      "\n",
      "Epoch 37/100\n",
      "[37/100] Train Loss: 0.7655, Train CER: 0.2241 | Val Loss: 0.2609, Val CER: 0.0586\n",
      "Validation loss did not improve. Early stop counter: 5/10\n",
      "\n",
      "Epoch 38/100\n",
      "[38/100] Train Loss: 0.7718, Train CER: 0.2265 | Val Loss: 0.2372, Val CER: 0.0578\n",
      "Validation loss did not improve. Early stop counter: 6/10\n",
      "\n",
      "Epoch 39/100\n",
      "[39/100] Train Loss: 0.7525, Train CER: 0.2194 | Val Loss: 0.2612, Val CER: 0.0590\n",
      "Validation loss did not improve. Early stop counter: 7/10\n",
      "\n",
      "Epoch 40/100\n",
      "[40/100] Train Loss: 0.7566, Train CER: 0.2252 | Val Loss: 0.2405, Val CER: 0.0559\n",
      "Validation loss did not improve. Early stop counter: 8/10\n",
      "\n",
      "Epoch 41/100\n",
      "[41/100] Train Loss: 0.7505, Train CER: 0.2215 | Val Loss: 0.2282, Val CER: 0.0475\n",
      "\n",
      "Epoch 42/100\n",
      "[42/100] Train Loss: 0.7358, Train CER: 0.2188 | Val Loss: 0.2216, Val CER: 0.0544\n",
      "\n",
      "Epoch 43/100\n",
      "[43/100] Train Loss: 0.7419, Train CER: 0.2209 | Val Loss: 0.2584, Val CER: 0.0592\n",
      "Validation loss did not improve. Early stop counter: 1/10\n",
      "\n",
      "Epoch 44/100\n",
      "[44/100] Train Loss: 0.7350, Train CER: 0.2130 | Val Loss: 0.2275, Val CER: 0.0516\n",
      "Validation loss did not improve. Early stop counter: 2/10\n",
      "\n",
      "Epoch 45/100\n",
      "[45/100] Train Loss: 0.7213, Train CER: 0.2150 | Val Loss: 0.2326, Val CER: 0.0528\n",
      "Validation loss did not improve. Early stop counter: 3/10\n",
      "\n",
      "Epoch 46/100\n",
      "[46/100] Train Loss: 0.7184, Train CER: 0.2118 | Val Loss: 0.2401, Val CER: 0.0486\n",
      "Validation loss did not improve. Early stop counter: 4/10\n",
      "\n",
      "Epoch 47/100\n",
      "[47/100] Train Loss: 0.7109, Train CER: 0.2095 | Val Loss: 0.2316, Val CER: 0.0529\n",
      "Validation loss did not improve. Early stop counter: 5/10\n",
      "\n",
      "Epoch 48/100\n",
      "[48/100] Train Loss: 0.7153, Train CER: 0.2129 | Val Loss: 0.2353, Val CER: 0.0571\n",
      "Validation loss did not improve. Early stop counter: 6/10\n",
      "\n",
      "Epoch 49/100\n",
      "[49/100] Train Loss: 0.6969, Train CER: 0.2076 | Val Loss: 0.2248, Val CER: 0.0516\n",
      "Validation loss did not improve. Early stop counter: 7/10\n",
      "\n",
      "Epoch 50/100\n",
      "[50/100] Train Loss: 0.6994, Train CER: 0.2094 | Val Loss: 0.2412, Val CER: 0.0525\n",
      "Validation loss did not improve. Early stop counter: 8/10\n",
      "\n",
      "Epoch 51/100\n",
      "[51/100] Train Loss: 0.6759, Train CER: 0.2019 | Val Loss: 0.2186, Val CER: 0.0501\n",
      "\n",
      "Epoch 52/100\n",
      "[52/100] Train Loss: 0.7087, Train CER: 0.2142 | Val Loss: 0.2077, Val CER: 0.0489\n",
      "\n",
      "Epoch 53/100\n",
      "[53/100] Train Loss: 0.6963, Train CER: 0.2099 | Val Loss: 0.2425, Val CER: 0.0558\n",
      "Validation loss did not improve. Early stop counter: 1/10\n",
      "\n",
      "Epoch 54/100\n",
      "[54/100] Train Loss: 0.6927, Train CER: 0.2065 | Val Loss: 0.2464, Val CER: 0.0556\n",
      "Validation loss did not improve. Early stop counter: 2/10\n",
      "\n",
      "Epoch 55/100\n",
      "[55/100] Train Loss: 0.7019, Train CER: 0.2089 | Val Loss: 0.2438, Val CER: 0.0577\n",
      "Validation loss did not improve. Early stop counter: 3/10\n",
      "\n",
      "Epoch 56/100\n",
      "[56/100] Train Loss: 0.6699, Train CER: 0.1985 | Val Loss: 0.2273, Val CER: 0.0517\n",
      "Validation loss did not improve. Early stop counter: 4/10\n",
      "\n",
      "Epoch 57/100\n",
      "[57/100] Train Loss: 0.6852, Train CER: 0.2058 | Val Loss: 0.2417, Val CER: 0.0530\n",
      "Validation loss did not improve. Early stop counter: 5/10\n",
      "\n",
      "Epoch 58/100\n",
      "[58/100] Train Loss: 0.6758, Train CER: 0.2021 | Val Loss: 0.2288, Val CER: 0.0479\n",
      "Validation loss did not improve. Early stop counter: 6/10\n",
      "\n",
      "Epoch 59/100\n",
      "[59/100] Train Loss: 0.6940, Train CER: 0.2055 | Val Loss: 0.2268, Val CER: 0.0521\n",
      "Validation loss did not improve. Early stop counter: 7/10\n",
      "\n",
      "Epoch 60/100\n",
      "[60/100] Train Loss: 0.6838, Train CER: 0.2071 | Val Loss: 0.2192, Val CER: 0.0521\n",
      "Validation loss did not improve. Early stop counter: 8/10\n",
      "\n",
      "Epoch 61/100\n",
      "[61/100] Train Loss: 0.6532, Train CER: 0.1948 | Val Loss: 0.1872, Val CER: 0.0409\n",
      "\n",
      "Epoch 62/100\n",
      "[62/100] Train Loss: 0.6744, Train CER: 0.2008 | Val Loss: 0.1989, Val CER: 0.0431\n",
      "Validation loss did not improve. Early stop counter: 1/10\n",
      "\n",
      "Epoch 63/100\n",
      "[63/100] Train Loss: 0.6609, Train CER: 0.1984 | Val Loss: 0.2374, Val CER: 0.0566\n",
      "Validation loss did not improve. Early stop counter: 2/10\n",
      "\n",
      "Epoch 64/100\n",
      "[64/100] Train Loss: 0.6704, Train CER: 0.1997 | Val Loss: 0.2379, Val CER: 0.0487\n",
      "Validation loss did not improve. Early stop counter: 3/10\n",
      "\n",
      "Epoch 65/100\n",
      "[65/100] Train Loss: 0.6747, Train CER: 0.2056 | Val Loss: 0.2100, Val CER: 0.0475\n",
      "Validation loss did not improve. Early stop counter: 4/10\n",
      "\n",
      "Epoch 66/100\n",
      "[66/100] Train Loss: 0.6564, Train CER: 0.1977 | Val Loss: 0.2272, Val CER: 0.0467\n",
      "Validation loss did not improve. Early stop counter: 5/10\n",
      "\n",
      "Epoch 67/100\n",
      "[67/100] Train Loss: 0.6619, Train CER: 0.1988 | Val Loss: 0.2365, Val CER: 0.0539\n",
      "Validation loss did not improve. Early stop counter: 6/10\n",
      "\n",
      "Epoch 68/100\n",
      "[68/100] Train Loss: 0.6483, Train CER: 0.1971 | Val Loss: 0.2095, Val CER: 0.0443\n",
      "Validation loss did not improve. Early stop counter: 7/10\n",
      "\n",
      "Epoch 69/100\n",
      "[69/100] Train Loss: 0.6520, Train CER: 0.1956 | Val Loss: 0.1984, Val CER: 0.0418\n",
      "Validation loss did not improve. Early stop counter: 8/10\n",
      "\n",
      "Epoch 70/100\n",
      "[70/100] Train Loss: 0.6537, Train CER: 0.1964 | Val Loss: 0.2271, Val CER: 0.0444\n",
      "Validation loss did not improve. Early stop counter: 9/10\n",
      "\n",
      "Epoch 71/100\n",
      "[71/100] Train Loss: 0.6500, Train CER: 0.1930 | Val Loss: 0.1935, Val CER: 0.0405\n",
      "Validation loss did not improve. Early stop counter: 10/10\n",
      "Early stopping triggered.\n",
      "\n",
      "Training completed in 5h 23m.\n",
      "\n",
      "Results have been written to: training_results.csv\n",
      "swin_small_encoder — Test CER: 0.1672\n",
      "\n",
      "=== Top 5 Worst Samples by CER ===\n",
      "1) Image: test3029.png\n",
      "   CER: 11.0000\n",
      "   Predicted       : carik-agung\n",
      "   Ground Truth    : .\n",
      "\n",
      "2) Image: test4934.png\n",
      "   CER: 11.0000\n",
      "   Predicted       : carik-agung\n",
      "   Ground Truth    : .\n",
      "\n",
      "3) Image: test1330.png\n",
      "   CER: 7.0000\n",
      "   Predicted       : laksana\n",
      "   Ground Truth    : .\n",
      "\n",
      "4) Image: test580.png\n",
      "   CER: 5.0000\n",
      "   Predicted       : liang\n",
      "   Ground Truth    : .\n",
      "\n",
      "5) Image: test2052.png\n",
      "   CER: 4.0000\n",
      "   Predicted       : paha\n",
      "   Ground Truth    : .\n",
      "\n",
      "Logged swin_small_encoder: 0.1672\n",
      "Memory cleared for swin_small_encoder\n"
     ]
    }
   ],
   "source": [
    "run_training_pipeline(\n",
    "    encoder_class=SwinEncoder,\n",
    "    encoder_kwargs={\"model_name\":\"swin_small_patch4_window7_224\",\"pretrained\":True},\n",
    "    model_name=\"swin_small_encoder\",\n",
    "    vocab_size=vocab_size,\n",
    "    encoder_lr=1e-4,\n",
    "    decoder_lr=4e-4,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    test_loader=test_loader,\n",
    "    char_to_idx=char_to_idx,\n",
    "    idx_to_char=idx_to_char,\n",
    "    max_label_length=max_label_length,\n",
    "    max_label_length_test=max_label_length_test,\n",
    "    test_data=test_data,\n",
    "    device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "28c698ba-ff1f-4667-a7db-4bf51eae88ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/100\n",
      "[1/100] Train Loss: 2.3448, Train CER: 0.8873 | Val Loss: 1.8442, Val CER: 0.5763\n",
      "\n",
      "Epoch 2/100\n",
      "[2/100] Train Loss: 2.0952, Train CER: 0.6625 | Val Loss: 1.7643, Val CER: 0.4483\n",
      "\n",
      "Epoch 3/100\n",
      "[3/100] Train Loss: 2.0472, Train CER: 0.6332 | Val Loss: 1.6692, Val CER: 0.4564\n",
      "\n",
      "Epoch 4/100\n",
      "[4/100] Train Loss: 1.9948, Train CER: 0.6237 | Val Loss: 1.6211, Val CER: 0.4464\n",
      "\n",
      "Epoch 5/100\n",
      "[5/100] Train Loss: 1.9273, Train CER: 0.5907 | Val Loss: 1.5725, Val CER: 0.4076\n",
      "\n",
      "Epoch 6/100\n",
      "[6/100] Train Loss: 1.8802, Train CER: 0.5728 | Val Loss: 1.4436, Val CER: 0.3904\n",
      "\n",
      "Epoch 7/100\n",
      "[7/100] Train Loss: 1.8279, Train CER: 0.5473 | Val Loss: 1.3251, Val CER: 0.3363\n",
      "\n",
      "Epoch 8/100\n",
      "[8/100] Train Loss: 1.7489, Train CER: 0.5153 | Val Loss: 1.2047, Val CER: 0.2820\n",
      "\n",
      "Epoch 9/100\n",
      "[9/100] Train Loss: 1.6705, Train CER: 0.4812 | Val Loss: 1.0738, Val CER: 0.2476\n",
      "\n",
      "Epoch 10/100\n",
      "[10/100] Train Loss: 1.5880, Train CER: 0.4502 | Val Loss: 0.9923, Val CER: 0.2233\n",
      "\n",
      "Epoch 11/100\n",
      "[11/100] Train Loss: 1.5053, Train CER: 0.4273 | Val Loss: 0.8361, Val CER: 0.1992\n",
      "\n",
      "Epoch 12/100\n",
      "[12/100] Train Loss: 1.4220, Train CER: 0.4034 | Val Loss: 0.7439, Val CER: 0.1890\n",
      "\n",
      "Epoch 13/100\n",
      "[13/100] Train Loss: 1.3501, Train CER: 0.3836 | Val Loss: 0.6707, Val CER: 0.1576\n",
      "\n",
      "Epoch 14/100\n",
      "[14/100] Train Loss: 1.2601, Train CER: 0.3556 | Val Loss: 0.5592, Val CER: 0.1365\n",
      "\n",
      "Epoch 15/100\n",
      "[15/100] Train Loss: 1.1997, Train CER: 0.3425 | Val Loss: 0.4901, Val CER: 0.1191\n",
      "\n",
      "Epoch 16/100\n",
      "[16/100] Train Loss: 1.1451, Train CER: 0.3272 | Val Loss: 0.4947, Val CER: 0.1134\n",
      "Validation loss did not improve. Early stop counter: 1/10\n",
      "\n",
      "Epoch 17/100\n",
      "[17/100] Train Loss: 1.1270, Train CER: 0.3247 | Val Loss: 0.4005, Val CER: 0.0957\n",
      "\n",
      "Epoch 18/100\n",
      "[18/100] Train Loss: 1.0670, Train CER: 0.3093 | Val Loss: 0.4057, Val CER: 0.0991\n",
      "Validation loss did not improve. Early stop counter: 1/10\n",
      "\n",
      "Epoch 19/100\n",
      "[19/100] Train Loss: 1.0363, Train CER: 0.2990 | Val Loss: 0.3550, Val CER: 0.0885\n",
      "\n",
      "Epoch 20/100\n",
      "[20/100] Train Loss: 1.0251, Train CER: 0.2963 | Val Loss: 0.3484, Val CER: 0.0860\n",
      "\n",
      "Epoch 21/100\n",
      "[21/100] Train Loss: 0.9710, Train CER: 0.2792 | Val Loss: 0.3584, Val CER: 0.0886\n",
      "Validation loss did not improve. Early stop counter: 1/10\n",
      "\n",
      "Epoch 22/100\n",
      "[22/100] Train Loss: 0.9457, Train CER: 0.2752 | Val Loss: 0.3352, Val CER: 0.0807\n",
      "\n",
      "Epoch 23/100\n",
      "[23/100] Train Loss: 0.9404, Train CER: 0.2720 | Val Loss: 0.3601, Val CER: 0.0855\n",
      "Validation loss did not improve. Early stop counter: 1/10\n",
      "\n",
      "Epoch 24/100\n",
      "[24/100] Train Loss: 0.9107, Train CER: 0.2648 | Val Loss: 0.3198, Val CER: 0.0742\n",
      "\n",
      "Epoch 25/100\n",
      "[25/100] Train Loss: 0.8880, Train CER: 0.2594 | Val Loss: 0.2920, Val CER: 0.0711\n",
      "\n",
      "Epoch 26/100\n",
      "[26/100] Train Loss: 0.8786, Train CER: 0.2563 | Val Loss: 0.2764, Val CER: 0.0675\n",
      "\n",
      "Epoch 27/100\n",
      "[27/100] Train Loss: 0.8747, Train CER: 0.2612 | Val Loss: 0.2761, Val CER: 0.0669\n",
      "\n",
      "Epoch 28/100\n",
      "[28/100] Train Loss: 0.8508, Train CER: 0.2529 | Val Loss: 0.2655, Val CER: 0.0674\n",
      "\n",
      "Epoch 29/100\n",
      "[29/100] Train Loss: 0.8493, Train CER: 0.2500 | Val Loss: 0.2556, Val CER: 0.0692\n",
      "\n",
      "Epoch 30/100\n",
      "[30/100] Train Loss: 0.8266, Train CER: 0.2479 | Val Loss: 0.2777, Val CER: 0.0645\n",
      "Validation loss did not improve. Early stop counter: 1/10\n",
      "\n",
      "Epoch 31/100\n",
      "[31/100] Train Loss: 0.8264, Train CER: 0.2422 | Val Loss: 0.2921, Val CER: 0.0688\n",
      "Validation loss did not improve. Early stop counter: 2/10\n",
      "\n",
      "Epoch 32/100\n",
      "[32/100] Train Loss: 0.7983, Train CER: 0.2358 | Val Loss: 0.2443, Val CER: 0.0561\n",
      "\n",
      "Epoch 33/100\n",
      "[33/100] Train Loss: 0.7891, Train CER: 0.2351 | Val Loss: 0.2534, Val CER: 0.0591\n",
      "Validation loss did not improve. Early stop counter: 1/10\n",
      "\n",
      "Epoch 34/100\n",
      "[34/100] Train Loss: 0.7767, Train CER: 0.2309 | Val Loss: 0.3060, Val CER: 0.0702\n",
      "Validation loss did not improve. Early stop counter: 2/10\n",
      "\n",
      "Epoch 35/100\n",
      "[35/100] Train Loss: 0.7811, Train CER: 0.2306 | Val Loss: 0.2543, Val CER: 0.0539\n",
      "Validation loss did not improve. Early stop counter: 3/10\n",
      "\n",
      "Epoch 36/100\n",
      "[36/100] Train Loss: 0.7687, Train CER: 0.2257 | Val Loss: 0.2557, Val CER: 0.0645\n",
      "Validation loss did not improve. Early stop counter: 4/10\n",
      "\n",
      "Epoch 37/100\n",
      "[37/100] Train Loss: 0.7905, Train CER: 0.2303 | Val Loss: 0.2566, Val CER: 0.0598\n",
      "Validation loss did not improve. Early stop counter: 5/10\n",
      "\n",
      "Epoch 38/100\n",
      "[38/100] Train Loss: 0.7510, Train CER: 0.2219 | Val Loss: 0.2475, Val CER: 0.0523\n",
      "Validation loss did not improve. Early stop counter: 6/10\n",
      "\n",
      "Epoch 39/100\n",
      "[39/100] Train Loss: 0.7572, Train CER: 0.2254 | Val Loss: 0.2554, Val CER: 0.0593\n",
      "Validation loss did not improve. Early stop counter: 7/10\n",
      "\n",
      "Epoch 40/100\n",
      "[40/100] Train Loss: 0.7457, Train CER: 0.2199 | Val Loss: 0.2371, Val CER: 0.0517\n",
      "\n",
      "Epoch 41/100\n",
      "[41/100] Train Loss: 0.7308, Train CER: 0.2153 | Val Loss: 0.2611, Val CER: 0.0630\n",
      "Validation loss did not improve. Early stop counter: 1/10\n",
      "\n",
      "Epoch 42/100\n",
      "[42/100] Train Loss: 0.7288, Train CER: 0.2188 | Val Loss: 0.2662, Val CER: 0.0635\n",
      "Validation loss did not improve. Early stop counter: 2/10\n",
      "\n",
      "Epoch 43/100\n",
      "[43/100] Train Loss: 0.7346, Train CER: 0.2180 | Val Loss: 0.2501, Val CER: 0.0600\n",
      "Validation loss did not improve. Early stop counter: 3/10\n",
      "\n",
      "Epoch 44/100\n",
      "[44/100] Train Loss: 0.7214, Train CER: 0.2176 | Val Loss: 0.2637, Val CER: 0.0556\n",
      "Validation loss did not improve. Early stop counter: 4/10\n",
      "\n",
      "Epoch 45/100\n",
      "[45/100] Train Loss: 0.7217, Train CER: 0.2115 | Val Loss: 0.2525, Val CER: 0.0607\n",
      "Validation loss did not improve. Early stop counter: 5/10\n",
      "\n",
      "Epoch 46/100\n",
      "[46/100] Train Loss: 0.7400, Train CER: 0.2240 | Val Loss: 0.2443, Val CER: 0.0609\n",
      "Validation loss did not improve. Early stop counter: 6/10\n",
      "\n",
      "Epoch 47/100\n",
      "[47/100] Train Loss: 0.7201, Train CER: 0.2193 | Val Loss: 0.2461, Val CER: 0.0594\n",
      "Validation loss did not improve. Early stop counter: 7/10\n",
      "\n",
      "Epoch 48/100\n",
      "[48/100] Train Loss: 0.7006, Train CER: 0.2113 | Val Loss: 0.2390, Val CER: 0.0485\n",
      "Validation loss did not improve. Early stop counter: 8/10\n",
      "\n",
      "Epoch 49/100\n",
      "[49/100] Train Loss: 0.7074, Train CER: 0.2128 | Val Loss: 0.2438, Val CER: 0.0535\n",
      "Validation loss did not improve. Early stop counter: 9/10\n",
      "\n",
      "Epoch 50/100\n",
      "[50/100] Train Loss: 0.7092, Train CER: 0.2131 | Val Loss: 0.2560, Val CER: 0.0604\n",
      "Validation loss did not improve. Early stop counter: 10/10\n",
      "Early stopping triggered.\n",
      "\n",
      "Training completed in 3h 55m.\n",
      "\n",
      "Results have been written to: training_results.csv\n",
      "swin_base_encoder — Test CER: 0.1717\n",
      "\n",
      "=== Top 5 Worst Samples by CER ===\n",
      "1) Image: test262.png\n",
      "   CER: 11.0000\n",
      "   Predicted       : nganggaling\n",
      "   Ground Truth    : .\n",
      "\n",
      "2) Image: test1330.png\n",
      "   CER: 9.0000\n",
      "   Predicted       : panggraha\n",
      "   Ground Truth    : .\n",
      "\n",
      "3) Image: test4436.png\n",
      "   CER: 7.0000\n",
      "   Predicted       : lungnia\n",
      "   Ground Truth    : .\n",
      "\n",
      "4) Image: test8296.png\n",
      "   CER: 7.0000\n",
      "   Predicted       : ambelum\n",
      "   Ground Truth    : .\n",
      "\n",
      "5) Image: test9874.png\n",
      "   CER: 7.0000\n",
      "   Predicted       : tenggah\n",
      "   Ground Truth    : .\n",
      "\n",
      "Logged swin_base_encoder: 0.1717\n",
      "Memory cleared for swin_base_encoder\n"
     ]
    }
   ],
   "source": [
    "run_training_pipeline(\n",
    "    encoder_class = SwinEncoder,\n",
    "    encoder_kwargs= {\"model_name\": \"swin_base_patch4_window7_224\", \"pretrained\": True},\n",
    "    model_name    = \"swin_base_encoder\",\n",
    "    vocab_size    = vocab_size,\n",
    "    encoder_lr    = 8e-5,    # lower LR for the larger Swin\n",
    "    decoder_lr    = 3e-4,\n",
    "    train_loader  = train_loader,\n",
    "    val_loader    = val_loader,\n",
    "    test_loader   = test_loader,\n",
    "    char_to_idx   = char_to_idx,\n",
    "    idx_to_char   = idx_to_char,\n",
    "    max_label_length      = max_label_length,\n",
    "    max_label_length_test = max_label_length_test,\n",
    "    test_data      = test_data,\n",
    "    device         = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5618d5cb-091d-4910-aac5-4e7a1b5da558",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/100\n",
      "[1/100] Train Loss: 2.4173, Train CER: 0.9353 | Val Loss: 1.8871, Val CER: 0.5650\n",
      "\n",
      "Epoch 2/100\n",
      "[2/100] Train Loss: 2.0683, Train CER: 0.6593 | Val Loss: 1.7047, Val CER: 0.5276\n",
      "\n",
      "Epoch 3/100\n",
      "[3/100] Train Loss: 1.9700, Train CER: 0.6095 | Val Loss: 1.5722, Val CER: 0.4161\n",
      "\n",
      "Epoch 4/100\n",
      "[4/100] Train Loss: 1.9025, Train CER: 0.5760 | Val Loss: 1.4690, Val CER: 0.3766\n",
      "\n",
      "Epoch 5/100\n",
      "[5/100] Train Loss: 1.8221, Train CER: 0.5487 | Val Loss: 1.3167, Val CER: 0.3071\n",
      "\n",
      "Epoch 6/100\n",
      "[6/100] Train Loss: 1.7420, Train CER: 0.5109 | Val Loss: 1.1723, Val CER: 0.2744\n",
      "\n",
      "Epoch 7/100\n",
      "[7/100] Train Loss: 1.6841, Train CER: 0.4868 | Val Loss: 1.1273, Val CER: 0.2616\n",
      "\n",
      "Epoch 8/100\n",
      "[8/100] Train Loss: 1.6289, Train CER: 0.4707 | Val Loss: 1.0291, Val CER: 0.2609\n",
      "\n",
      "Epoch 9/100\n",
      "[9/100] Train Loss: 1.5827, Train CER: 0.4544 | Val Loss: 0.9616, Val CER: 0.2204\n",
      "\n",
      "Epoch 10/100\n",
      "[10/100] Train Loss: 1.5302, Train CER: 0.4359 | Val Loss: 0.8815, Val CER: 0.1919\n",
      "\n",
      "Epoch 11/100\n",
      "[11/100] Train Loss: 1.4757, Train CER: 0.4184 | Val Loss: 0.8261, Val CER: 0.1860\n",
      "\n",
      "Epoch 12/100\n",
      "[12/100] Train Loss: 1.4223, Train CER: 0.4023 | Val Loss: 0.7743, Val CER: 0.1921\n",
      "\n",
      "Epoch 13/100\n",
      "[13/100] Train Loss: 1.3644, Train CER: 0.3860 | Val Loss: 0.7050, Val CER: 0.1553\n",
      "\n",
      "Epoch 14/100\n",
      "[14/100] Train Loss: 1.3088, Train CER: 0.3665 | Val Loss: 0.6429, Val CER: 0.1426\n",
      "\n",
      "Epoch 15/100\n",
      "[15/100] Train Loss: 1.2816, Train CER: 0.3543 | Val Loss: 0.6468, Val CER: 0.1480\n",
      "Validation loss did not improve. Early stop counter: 1/10\n",
      "\n",
      "Epoch 16/100\n",
      "[16/100] Train Loss: 1.2326, Train CER: 0.3466 | Val Loss: 0.5903, Val CER: 0.1351\n",
      "\n",
      "Epoch 17/100\n",
      "[17/100] Train Loss: 1.2127, Train CER: 0.3380 | Val Loss: 0.5466, Val CER: 0.1191\n",
      "\n",
      "Epoch 18/100\n",
      "[18/100] Train Loss: 1.1810, Train CER: 0.3325 | Val Loss: 0.5137, Val CER: 0.1137\n",
      "\n",
      "Epoch 19/100\n",
      "[19/100] Train Loss: 1.1475, Train CER: 0.3226 | Val Loss: 0.5554, Val CER: 0.1186\n",
      "Validation loss did not improve. Early stop counter: 1/10\n",
      "\n",
      "Epoch 20/100\n",
      "[20/100] Train Loss: 1.1170, Train CER: 0.3162 | Val Loss: 0.4742, Val CER: 0.1115\n",
      "\n",
      "Epoch 21/100\n",
      "[21/100] Train Loss: 1.0970, Train CER: 0.3077 | Val Loss: 0.4613, Val CER: 0.1022\n",
      "\n",
      "Epoch 22/100\n",
      "[22/100] Train Loss: 1.0841, Train CER: 0.3098 | Val Loss: 0.4655, Val CER: 0.1026\n",
      "Validation loss did not improve. Early stop counter: 1/10\n",
      "\n",
      "Epoch 23/100\n",
      "[23/100] Train Loss: 1.0613, Train CER: 0.3033 | Val Loss: 0.4523, Val CER: 0.1028\n",
      "\n",
      "Epoch 24/100\n",
      "[24/100] Train Loss: 1.0125, Train CER: 0.2855 | Val Loss: 0.4202, Val CER: 0.0917\n",
      "\n",
      "Epoch 25/100\n",
      "[25/100] Train Loss: 1.0114, Train CER: 0.2867 | Val Loss: 0.4226, Val CER: 0.0942\n",
      "Validation loss did not improve. Early stop counter: 1/10\n",
      "\n",
      "Epoch 26/100\n",
      "[26/100] Train Loss: 0.9857, Train CER: 0.2811 | Val Loss: 0.4291, Val CER: 0.0931\n",
      "Validation loss did not improve. Early stop counter: 2/10\n",
      "\n",
      "Epoch 27/100\n",
      "[27/100] Train Loss: 0.9656, Train CER: 0.2748 | Val Loss: 0.4186, Val CER: 0.0901\n",
      "\n",
      "Epoch 28/100\n",
      "[28/100] Train Loss: 0.9668, Train CER: 0.2775 | Val Loss: 0.3764, Val CER: 0.0861\n",
      "\n",
      "Epoch 29/100\n",
      "[29/100] Train Loss: 0.9556, Train CER: 0.2715 | Val Loss: 0.4074, Val CER: 0.0877\n",
      "Validation loss did not improve. Early stop counter: 1/10\n",
      "\n",
      "Epoch 30/100\n",
      "[30/100] Train Loss: 0.9210, Train CER: 0.2645 | Val Loss: 0.3946, Val CER: 0.0781\n",
      "Validation loss did not improve. Early stop counter: 2/10\n",
      "\n",
      "Epoch 31/100\n",
      "[31/100] Train Loss: 0.8976, Train CER: 0.2582 | Val Loss: 0.3632, Val CER: 0.0786\n",
      "\n",
      "Epoch 32/100\n",
      "[32/100] Train Loss: 0.8945, Train CER: 0.2611 | Val Loss: 0.3975, Val CER: 0.0821\n",
      "Validation loss did not improve. Early stop counter: 1/10\n",
      "\n",
      "Epoch 33/100\n",
      "[33/100] Train Loss: 0.8858, Train CER: 0.2587 | Val Loss: 0.3640, Val CER: 0.0806\n",
      "Validation loss did not improve. Early stop counter: 2/10\n",
      "\n",
      "Epoch 34/100\n",
      "[34/100] Train Loss: 0.8852, Train CER: 0.2579 | Val Loss: 0.3286, Val CER: 0.0752\n",
      "\n",
      "Epoch 35/100\n",
      "[36/100] Train Loss: 0.8528, Train CER: 0.2475 | Val Loss: 0.3689, Val CER: 0.0760\n",
      "Validation loss did not improve. Early stop counter: 2/10\n",
      "\n",
      "Epoch 37/100\n",
      "[37/100] Train Loss: 0.8452, Train CER: 0.2442 | Val Loss: 0.3652, Val CER: 0.0774\n",
      "Validation loss did not improve. Early stop counter: 3/10\n",
      "\n",
      "Epoch 38/100\n",
      "[38/100] Train Loss: 0.8374, Train CER: 0.2428 | Val Loss: 0.3596, Val CER: 0.0789\n",
      "Validation loss did not improve. Early stop counter: 4/10\n",
      "\n",
      "Epoch 39/100\n",
      "[39/100] Train Loss: 0.8187, Train CER: 0.2416 | Val Loss: 0.3331, Val CER: 0.0720\n",
      "Validation loss did not improve. Early stop counter: 5/10\n",
      "\n",
      "Epoch 40/100\n",
      "[40/100] Train Loss: 0.8245, Train CER: 0.2433 | Val Loss: 0.3186, Val CER: 0.0669\n",
      "\n",
      "Epoch 41/100\n",
      "[41/100] Train Loss: 0.8180, Train CER: 0.2396 | Val Loss: 0.3374, Val CER: 0.0711\n",
      "Validation loss did not improve. Early stop counter: 1/10\n",
      "\n",
      "Epoch 42/100\n",
      "[42/100] Train Loss: 0.8120, Train CER: 0.2397 | Val Loss: 0.3256, Val CER: 0.0643\n",
      "Validation loss did not improve. Early stop counter: 2/10\n",
      "\n",
      "Epoch 43/100\n",
      "[43/100] Train Loss: 0.7952, Train CER: 0.2321 | Val Loss: 0.3394, Val CER: 0.0677\n",
      "Validation loss did not improve. Early stop counter: 3/10\n",
      "\n",
      "Epoch 44/100\n",
      "[44/100] Train Loss: 0.7905, Train CER: 0.2315 | Val Loss: 0.3746, Val CER: 0.0675\n",
      "Validation loss did not improve. Early stop counter: 4/10\n",
      "\n",
      "Epoch 45/100\n",
      "[45/100] Train Loss: 0.7915, Train CER: 0.2349 | Val Loss: 0.3158, Val CER: 0.0650\n",
      "\n",
      "Epoch 46/100\n",
      "[46/100] Train Loss: 0.7827, Train CER: 0.2287 | Val Loss: 0.3228, Val CER: 0.0652\n",
      "Validation loss did not improve. Early stop counter: 1/10\n",
      "\n",
      "Epoch 47/100\n",
      "[47/100] Train Loss: 0.7702, Train CER: 0.2312 | Val Loss: 0.3190, Val CER: 0.0682\n",
      "Validation loss did not improve. Early stop counter: 2/10\n",
      "\n",
      "Epoch 48/100\n",
      "[48/100] Train Loss: 0.7724, Train CER: 0.2312 | Val Loss: 0.3201, Val CER: 0.0689\n",
      "Validation loss did not improve. Early stop counter: 3/10\n",
      "\n",
      "Epoch 49/100\n",
      "[49/100] Train Loss: 0.7589, Train CER: 0.2228 | Val Loss: 0.3417, Val CER: 0.0724\n",
      "Validation loss did not improve. Early stop counter: 4/10\n",
      "\n",
      "Epoch 50/100\n",
      "[50/100] Train Loss: 0.7479, Train CER: 0.2207 | Val Loss: 0.3474, Val CER: 0.0676\n",
      "Validation loss did not improve. Early stop counter: 5/10\n",
      "\n",
      "Epoch 51/100\n",
      "[51/100] Train Loss: 0.7429, Train CER: 0.2169 | Val Loss: 0.3092, Val CER: 0.0647\n",
      "\n",
      "Epoch 52/100\n",
      "[52/100] Train Loss: 0.7378, Train CER: 0.2179 | Val Loss: 0.2960, Val CER: 0.0660\n",
      "\n",
      "Epoch 53/100\n",
      "[53/100] Train Loss: 0.7335, Train CER: 0.2196 | Val Loss: 0.3577, Val CER: 0.0714\n",
      "Validation loss did not improve. Early stop counter: 1/10\n",
      "\n",
      "Epoch 54/100\n",
      "[54/100] Train Loss: 0.7298, Train CER: 0.2196 | Val Loss: 0.3172, Val CER: 0.0650\n",
      "Validation loss did not improve. Early stop counter: 2/10\n",
      "\n",
      "Epoch 55/100\n",
      "[55/100] Train Loss: 0.7282, Train CER: 0.2176 | Val Loss: 0.3373, Val CER: 0.0623\n",
      "Validation loss did not improve. Early stop counter: 3/10\n",
      "\n",
      "Epoch 56/100\n",
      "[56/100] Train Loss: 0.7119, Train CER: 0.2109 | Val Loss: 0.3470, Val CER: 0.0628\n",
      "Validation loss did not improve. Early stop counter: 4/10\n",
      "\n",
      "Epoch 57/100\n",
      "[57/100] Train Loss: 0.7225, Train CER: 0.2177 | Val Loss: 0.3063, Val CER: 0.0637\n",
      "Validation loss did not improve. Early stop counter: 5/10\n",
      "\n",
      "Epoch 58/100\n",
      "[58/100] Train Loss: 0.7156, Train CER: 0.2164 | Val Loss: 0.3396, Val CER: 0.0679\n",
      "Validation loss did not improve. Early stop counter: 6/10\n",
      "\n",
      "Epoch 59/100\n",
      "[59/100] Train Loss: 0.7186, Train CER: 0.2136 | Val Loss: 0.3182, Val CER: 0.0647\n",
      "Validation loss did not improve. Early stop counter: 7/10\n",
      "\n",
      "Epoch 60/100\n",
      "[60/100] Train Loss: 0.7082, Train CER: 0.2124 | Val Loss: 0.2975, Val CER: 0.0586\n",
      "Validation loss did not improve. Early stop counter: 8/10\n",
      "\n",
      "Epoch 61/100\n",
      "[61/100] Train Loss: 0.7131, Train CER: 0.2108 | Val Loss: 0.3422, Val CER: 0.0663\n",
      "Validation loss did not improve. Early stop counter: 9/10\n",
      "\n",
      "Epoch 62/100\n",
      "[62/100] Train Loss: 0.7083, Train CER: 0.2114 | Val Loss: 0.2961, Val CER: 0.0585\n",
      "Validation loss did not improve. Early stop counter: 10/10\n",
      "Early stopping triggered.\n",
      "\n",
      "Training completed in 6h 2m.\n",
      "\n",
      "Results have been written to: training_results.csv\n",
      "swin_large_encoder — Test CER: 0.2118\n",
      "\n",
      "=== Top 5 Worst Samples by CER ===\n",
      "1) Image: test1330.png\n",
      "   CER: 9.0000\n",
      "   Predicted       : sangksana\n",
      "   Ground Truth    : .\n",
      "\n",
      "2) Image: test5293.png\n",
      "   CER: 9.0000\n",
      "   Predicted       : adeg-adeg\n",
      "   Ground Truth    : i\n",
      "\n",
      "3) Image: test1335.png\n",
      "   CER: 5.0000\n",
      "   Predicted       : panti\n",
      "   Ground Truth    : .\n",
      "\n",
      "4) Image: test6297.png\n",
      "   CER: 5.0000\n",
      "   Predicted       : panti\n",
      "   Ground Truth    : .\n",
      "\n",
      "5) Image: test8296.png\n",
      "   CER: 5.0000\n",
      "   Predicted       : lakum\n",
      "   Ground Truth    : .\n",
      "\n",
      "Logged swin_large_encoder: 0.2118\n",
      "Memory cleared for swin_large_encoder\n"
     ]
    }
   ],
   "source": [
    "run_training_pipeline(\n",
    "    encoder_class = SwinEncoder,\n",
    "    encoder_kwargs= {\"model_name\": \"swin_large_patch4_window7_224\", \"pretrained\": True},\n",
    "    model_name    = \"swin_large_encoder\",\n",
    "    vocab_size    = vocab_size,\n",
    "    encoder_lr    = 5e-5,\n",
    "    decoder_lr    = 2e-4,\n",
    "    train_loader  = train_loader,\n",
    "    val_loader    = val_loader,\n",
    "    test_loader   = test_loader,\n",
    "    char_to_idx   = char_to_idx,\n",
    "    idx_to_char   = idx_to_char,\n",
    "    max_label_length      = max_label_length,\n",
    "    max_label_length_test = max_label_length_test,\n",
    "    test_data      = test_data,\n",
    "    device         = device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
