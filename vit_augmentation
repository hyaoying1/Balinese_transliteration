{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10537618,"sourceType":"datasetVersion","datasetId":6520118}],"dockerImageVersionId":30840,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nimport editdistance\nimport time \n\n\nfrom PIL import Image\nfrom collections import Counter\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision.models as models\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\n\nimport timm  \n\nfrom sklearn.model_selection import train_test_split\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T04:36:52.723903Z","iopub.execute_input":"2025-04-01T04:36:52.724211Z","iopub.status.idle":"2025-04-01T04:36:52.728691Z","shell.execute_reply.started":"2025-04-01T04:36:52.724188Z","shell.execute_reply":"2025-04-01T04:36:52.727928Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nbase_dir = os.getcwd()\n\n# Same paths as your original code\nground_truth_path = os.path.join(base_dir, '/kaggle/input/balinese/data/balinese_transliteration_train.txt') \nimages_dir        = os.path.join(base_dir, '/kaggle/input/balinese/data/balinese_word_train')\n\nfilenames = []\nlabels    = []\n\nwith open(ground_truth_path, 'r', encoding='utf-8') as file:\n    for line in file:\n        line = line.strip()\n        if line:  # Ensure the line is not empty\n            parts = line.split(';')\n            if len(parts) == 2:\n                filename, label = parts\n                label = label.lower()\n                filenames.append(filename)\n                labels.append(label)\n            else:\n                print(f\"Skipping malformed line: {line}\")\n\ndata = pd.DataFrame({\n    'filename': filenames,\n    'label': labels\n})\n\nlabel_counts = data['label'].value_counts()\n\nall_text = ''.join(data['label'])\nunique_chars = sorted(list(set(all_text)))\n\n# Create character->index starting from 1\nchar_to_idx = {char: idx + 1 for idx, char in enumerate(unique_chars)}\n# Add special tokens\nchar_to_idx['<PAD>'] = 0\nchar_to_idx['<SOS>'] = len(char_to_idx)\nchar_to_idx['<EOS>'] = len(char_to_idx)\n\n# Reverse mapping\nidx_to_char = {v: k for k, v in char_to_idx.items()}\n\nvocab_size = len(char_to_idx)\nprint(f\"Vocabulary size: {vocab_size}\")\n\ndef encode_label(label, char_to_idx, max_length):\n    \"\"\"\n    Converts a label (string) into a list of indices with <SOS>, <EOS>, and padding.\n    Assumes all characters in the label exist in the vocabulary.\n    \"\"\"\n    encoded = (\n        [char_to_idx['<SOS>']] +\n        [char_to_idx[ch] for ch in label] +\n        [char_to_idx['<EOS>']]\n    )\n    # Pad if needed\n    if len(encoded) < max_length:\n        encoded += [char_to_idx['<PAD>']] * (max_length - len(encoded))\n    else:\n        encoded = encoded[:max_length]\n    return encoded\n\nmax_label_length = max(len(label) for label in data['label']) + 2  # +2 for <SOS> and <EOS>\ndata['encoded_label'] = data['label'].apply(lambda x: encode_label(x, char_to_idx, max_label_length))\ndata['label_length']  = data['label'].apply(len)\n\nrare_labels = label_counts[label_counts < 3].index  # NEW: words that appear <3 times\n\ndef custom_split(df, rare_label_list, test_size=0.1, random_state=42):\n    # Separate rare words from frequent ones\n    rare_df     = df[df['label'].isin(rare_label_list)]\n    non_rare_df = df[~df['label'].isin(rare_label_list)]\n\n    #  train/val split for non-rare\n    train_nr, val_nr = train_test_split(non_rare_df, test_size=test_size, \n                                        random_state=random_state)\n\n    # Combine rare samples entirely into training\n    train_df = pd.concat([train_nr, rare_df], ignore_index=True)\n    # Shuffle after combining\n    train_df = train_df.sample(frac=1, random_state=random_state).reset_index(drop=True)\n\n    val_df = val_nr.reset_index(drop=True)\n    return train_df, val_df\n\n# Call custom_split instead of direct train_test_split\ntrain_data, val_data = custom_split(data, rare_labels, test_size=0.1, random_state=42) \n\nprint(f\"Training size: {len(train_data)}; Validation size: {len(val_data)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T04:36:52.736357Z","iopub.execute_input":"2025-04-01T04:36:52.736590Z","iopub.status.idle":"2025-04-01T04:36:52.816900Z","shell.execute_reply.started":"2025-04-01T04:36:52.736571Z","shell.execute_reply":"2025-04-01T04:36:52.816059Z"}},"outputs":[{"name":"stdout","text":"Vocabulary size: 38\nTraining size: 13972; Validation size: 1050\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"class BalineseDataset(Dataset):\n    def __init__(self, df, images_dir, transform=None):\n        self.data       = df.reset_index(drop=True)\n        self.images_dir = images_dir\n        self.transform  = transform\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        img_name     = self.data.loc[idx, 'filename']\n        label        = self.data.loc[idx, 'encoded_label']\n        label_length = self.data.loc[idx, 'label_length']\n\n        img_path = os.path.join(self.images_dir, img_name)\n        image    = Image.open(img_path).convert('RGB')\n\n        if self.transform:\n            image = self.transform(image)\n\n        label = torch.tensor(label, dtype=torch.long)\n        return image, label, torch.tensor(label_length, dtype=torch.long)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T04:36:52.817790Z","iopub.execute_input":"2025-04-01T04:36:52.818012Z","iopub.status.idle":"2025-04-01T04:36:52.823295Z","shell.execute_reply.started":"2025-04-01T04:36:52.817994Z","shell.execute_reply":"2025-04-01T04:36:52.822450Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"transform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(\n        mean=(0.5, 0.5, 0.5),\n        std=(0.5, 0.5, 0.5)\n    )\n])\n\ntrain_dataset = BalineseDataset(train_data, images_dir, transform=transform)\nval_dataset   = BalineseDataset(val_data,   images_dir, transform=transform)\n\nbatch_size = 32\n\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,  num_workers=2, pin_memory=True)\nval_loader   = DataLoader(val_dataset,   batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T04:36:52.824720Z","iopub.execute_input":"2025-04-01T04:36:52.824993Z","iopub.status.idle":"2025-04-01T04:36:52.844997Z","shell.execute_reply.started":"2025-04-01T04:36:52.824974Z","shell.execute_reply":"2025-04-01T04:36:52.844179Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"class ViTEncoder(nn.Module):\n    \"\"\"\n    A simple ViT encoder that extracts patch embeddings as [batch_size, num_patches, hidden_dim].\n    We'll use timm to load a pretrained ViT. Then we use .forward_features() to get a\n    feature map of shape [B, C, H', W'] for many timm ViT models, which we flatten.\n    \"\"\"\n    def __init__(self, model_name=\"vit_large_patch16_224\", pretrained=True):\n        super(ViTEncoder, self).__init__()\n        self.vit = timm.create_model(model_name, pretrained=pretrained)\n        # Remove or replace the classification head\n        self.vit.head = nn.Identity()\n\n        # timm's ViT typically has an embed_dim attribute\n        self.encoder_dim = self.vit.embed_dim\n\n    def forward(self, x):\n        \"\"\"\n        :param x: [batch_size, 3, 224, 224]\n        :return:  [batch_size, num_patches, encoder_dim]\n        \"\"\"\n        # forward_features usually returns [B, hidden_dim, H', W'] or [B, hidden_dim]\n        feats = self.vit.forward_features(x)  # [B, hidden_dim, 14, 14] for vit_base_patch16_224\n\n        # Flatten the spatial dimensions\n        if feats.dim() == 4:  # [B, C, H, W]\n            b, c, h, w = feats.shape\n            feats = feats.permute(0, 2, 3, 1).reshape(b, -1, c)  # => [B, H*W, C]\n\n        return feats","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T04:36:52.845836Z","iopub.execute_input":"2025-04-01T04:36:52.846103Z","iopub.status.idle":"2025-04-01T04:36:52.860505Z","shell.execute_reply.started":"2025-04-01T04:36:52.846084Z","shell.execute_reply":"2025-04-01T04:36:52.859857Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"class Attention(nn.Module):\n    def __init__(self, encoder_dim, decoder_dim, attention_dim):\n        super(Attention, self).__init__()\n        self.encoder_att = nn.Linear(encoder_dim, attention_dim)  # transform encoder output\n        self.decoder_att = nn.Linear(decoder_dim, attention_dim)  # transform decoder hidden\n        self.full_att    = nn.Linear(attention_dim, 1)\n        self.relu        = nn.ReLU()\n        self.softmax     = nn.Softmax(dim=1)\n\n    def forward(self, encoder_out, decoder_hidden):\n        \"\"\"\n        encoder_out:    [batch_size, num_patches, encoder_dim]\n        decoder_hidden: [batch_size, decoder_dim]\n        \"\"\"\n        att1 = self.encoder_att(encoder_out)                  # [batch_size, num_patches, attention_dim]\n        att2 = self.decoder_att(decoder_hidden).unsqueeze(1)  # [batch_size, 1, attention_dim]\n\n        # sum -> relu -> full_att -> squeeze -> softmax\n        att  = self.full_att(self.relu(att1 + att2)).squeeze(2)  # [batch_size, num_patches]\n        alpha = self.softmax(att)\n        # Weighted sum of the encoder_out\n        attention_weighted_encoding = (encoder_out * alpha.unsqueeze(2)).sum(dim=1)  # [batch_size, encoder_dim]\n        return attention_weighted_encoding, alpha\n\nclass DecoderRNN(nn.Module):\n    def __init__(self, attention_dim, embed_dim, decoder_dim, vocab_size, encoder_dim=768, teacher_forcing_ratio=0.5):\n        super(DecoderRNN, self).__init__()\n\n        self.attention     = Attention(encoder_dim, decoder_dim, attention_dim)\n        self.embedding     = nn.Embedding(vocab_size, embed_dim)\n        self.dropout       = nn.Dropout(p=0.5)\n\n        # [embed_dim + encoder_dim] -> decoder_dim\n        self.lstm1 = nn.LSTMCell(embed_dim + encoder_dim, decoder_dim)\n        # decoder_dim -> decoder_dim\n        self.lstm2 = nn.LSTMCell(decoder_dim, decoder_dim)\n\n        # For initializing the hidden states of both LSTM layers\n        self.init_h1 = nn.Linear(encoder_dim, decoder_dim)\n        self.init_c1 = nn.Linear(encoder_dim, decoder_dim)\n        self.init_h2 = nn.Linear(encoder_dim, decoder_dim)\n        self.init_c2 = nn.Linear(encoder_dim, decoder_dim)\n\n        # Gating\n        self.f_beta  = nn.Linear(decoder_dim, encoder_dim)\n        self.sigmoid = nn.Sigmoid()\n\n        # Final linear layer for output vocab\n        self.fc = nn.Linear(decoder_dim, vocab_size)\n\n        self.teacher_forcing_ratio = teacher_forcing_ratio\n\n        self.init_weights()\n\n    def init_weights(self):\n        self.embedding.weight.data.uniform_(-0.1, 0.1)\n        self.fc.bias.data.fill_(0)\n        self.fc.weight.data.uniform_(-0.1, 0.1)\n\n    def init_hidden_state(self, encoder_out):\n        # encoder_out: [batch_size, num_patches, encoder_dim]\n        mean_encoder_out = encoder_out.mean(dim=1)  # [batch_size, encoder_dim]\n        h1 = self.init_h1(mean_encoder_out)         # [batch_size, decoder_dim]\n        c1 = self.init_c1(mean_encoder_out)         # [batch_size, decoder_dim]\n        h2 = self.init_h2(mean_encoder_out)         # [batch_size, decoder_dim]\n        c2 = self.init_c2(mean_encoder_out)         # [batch_size, decoder_dim]\n        return (h1, c1, h2, c2)\n\n    def forward(self, encoder_out, encoded_captions, caption_lengths):\n        \"\"\"\n        encoder_out:      [batch_size, num_patches, encoder_dim]\n        encoded_captions: [batch_size, max_label_length]\n        caption_lengths:  [batch_size, 1]\n        \"\"\"\n        caption_lengths, sort_ind = caption_lengths.squeeze(1).sort(dim=0, descending=True)\n        encoder_out      = encoder_out[sort_ind]\n        encoded_captions = encoded_captions[sort_ind]\n\n        embeddings = self.embedding(encoded_captions)\n\n        # Initialize hidden states for both LSTM layers\n        h1, c1, h2, c2 = self.init_hidden_state(encoder_out)\n\n        decode_lengths    = (caption_lengths - 1).tolist()\n        max_decode_length = max(decode_lengths)\n\n        batch_size = encoder_out.size(0)\n        vocab_size = self.fc.out_features\n\n        predictions = torch.zeros(batch_size, max_decode_length, vocab_size, device=encoder_out.device)\n        alphas      = torch.zeros(batch_size, max_decode_length, encoder_out.size(1), device=encoder_out.device)\n\n        # We'll feed the first token from the input (<SOS>) or from the previous prediction\n        prev_tokens = encoded_captions[:, 0].clone()\n\n        for t in range(max_decode_length):\n            batch_size_t = sum([l > t for l in decode_lengths])\n\n            attention_weighted_encoding, alpha = self.attention(\n                encoder_out[:batch_size_t],\n                h1[:batch_size_t]  # use the first LSTM layer's hidden state for attention\n            )\n\n            # Apply gating\n            gate = self.sigmoid(self.f_beta(h1[:batch_size_t]))\n            attention_weighted_encoding = gate * attention_weighted_encoding\n\n            # Teacher forcing?\n            use_teacher_forcing = (torch.rand(1).item() < self.teacher_forcing_ratio)\n            if use_teacher_forcing:\n                current_input = embeddings[:batch_size_t, t, :]\n            else:\n                current_input = self.embedding(prev_tokens[:batch_size_t].detach())\n\n            # first lstm layer\n            h1_next, c1_next = self.lstm1(\n                torch.cat([current_input, attention_weighted_encoding], dim=1),\n                (h1[:batch_size_t], c1[:batch_size_t])\n            )\n\n            # second lstm layer\n            h2_next, c2_next = self.lstm2(\n                h1_next, (h2[:batch_size_t], c2[:batch_size_t])\n            )\n\n            # Use the second LSTM layer's output (h2_next) for final prediction\n            preds = self.fc(self.dropout(h2_next))\n            predictions[:batch_size_t, t, :] = preds\n            alphas[:batch_size_t, t, :]      = alpha\n\n            # Update prev_tokens with the best predicted token\n            _, next_tokens = preds.max(dim=1)\n            prev_tokens_ = prev_tokens.clone()\n            prev_tokens_[:batch_size_t] = next_tokens.detach()\n            prev_tokens = prev_tokens_\n\n            # Update hidden states\n            # For samples still in the batch, store the new h1, c1, h2, c2\n            h1_new = torch.zeros_like(h1)\n            c1_new = torch.zeros_like(c1)\n            h2_new = torch.zeros_like(h2)\n            c2_new = torch.zeros_like(c2)\n\n            h1_new[:batch_size_t] = h1_next\n            c1_new[:batch_size_t] = c1_next\n            h2_new[:batch_size_t] = h2_next\n            c2_new[:batch_size_t] = c2_next\n\n            h1, c1, h2, c2 = h1_new, c1_new, h2_new, c2_new\n\n        return predictions, encoded_captions, decode_lengths, alphas, sort_ind","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T04:36:52.866207Z","iopub.execute_input":"2025-04-01T04:36:52.866394Z","iopub.status.idle":"2025-04-01T04:36:52.885971Z","shell.execute_reply.started":"2025-04-01T04:36:52.866378Z","shell.execute_reply":"2025-04-01T04:36:52.885284Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"class ImageCaptioningTrainer:\n    def __init__(self, encoder, decoder, \n                 criterion, encoder_optimizer, decoder_optimizer, \n                 train_loader, val_loader, device, \n                 char_to_idx, idx_to_char, max_label_length,\n                 model_name, csv_filename=\"training_results.csv\"):\n        self.encoder = encoder.to(device)\n        self.decoder             = decoder.to(device)\n        self.criterion           = criterion\n        self.encoder_optimizer   = encoder_optimizer\n        self.decoder_optimizer   = decoder_optimizer\n        self.train_loader        = train_loader\n        self.val_loader          = val_loader\n        self.device              = device\n        self.char_to_idx         = char_to_idx\n        self.idx_to_char         = idx_to_char\n        self.max_label_length    = max_label_length\n        self.model_name = model_name\n        self.csv_filename = csv_filename\n\n        self.train_losses = []\n        self.val_losses   = []\n\n        self.train_cers   = []\n        self.val_cers     = []\n\n    def fit(self, num_epochs):\n        start_time = time.time()\n\n        for epoch in range(num_epochs):\n            print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n            train_loss, train_cer = self.train_one_epoch()\n            val_loss,   val_cer   = self.validate_one_epoch(top_n=5)\n\n            print(f\"[{epoch+1}/{num_epochs}] \"\n                  f\"Train Loss: {train_loss:.4f}, Train CER: {train_cer:.4f} | \"\n                  f\"Val Loss: {val_loss:.4f}, Val CER: {val_cer:.4f}\")\n\n            # Store epoch results\n            self.train_losses.append(train_loss)\n            self.val_losses.append(val_loss)\n            self.train_cers.append(train_cer)\n            self.val_cers.append(val_cer)\n            \n        \n        # Calculate total training time\n        end_time = time.time() \n        total_time = end_time - start_time\n        hours = int(total_time // 3600)\n        minutes = int((total_time % 3600) // 60)\n\n        print(f\"\\nTraining completed in {hours}h {minutes}m.\")\n\n        num_epochs = len(self.train_losses)\n        epoch_cols = [f\"epoch{i+1}\" for i in range(num_epochs)]\n\n        # Create the new data block to insert\n        new_rows = pd.DataFrame([\n            [self.model_name, \"training loss\"] + self.train_losses,\n            [self.model_name, \"validation loss\"] + self.val_losses,\n            [self.model_name, \"training cer\"] + self.train_cers,\n            [self.model_name, \"validation cer\"] + self.val_cers\n        ], columns=[\"model_name\", \"mode\"] + epoch_cols)\n        \n        # Check if CSV already exists\n        if os.path.exists(self.csv_filename):\n            df_existing = pd.read_csv(self.csv_filename)\n            df_existing = df_existing[df_existing[\"model_name\"] != self.model_name]\n            df_updated = pd.concat([df_existing, new_rows], ignore_index=True)\n        else:\n            df_updated = new_rows\n        \n        # Save the updated CSV\n        df_updated.to_csv(self.csv_filename, index=False)\n        print(f\"\\nResults have been written to: {self.csv_filename}\")\n\n\n        # Save model weights\n        # torch.save(self.encoder.state_dict(), f\"encoder_{self.model_name}.pth\")\n        # torch.save(self.decoder.state_dict(), f\"decoder_{self.model_name}.pth\")\n        # print(f\"Encoder and decoder models saved: encoder_{self.model_name}.pth, decoder_{self.model_name}.pth\")\n        \n\n    def train_one_epoch(self):\n        self.encoder.train()\n        self.decoder.train()\n        running_loss           = 0.0\n        total_edit_distance    = 0\n        total_ref_length       = 0\n\n        for batch_idx, (images, labels, label_lengths) in enumerate(self.train_loader):\n            images        = images.to(self.device, non_blocking=True)\n            labels        = labels.to(self.device, non_blocking=True)\n            label_lengths = label_lengths.to(self.device, non_blocking=True)\n\n            self.encoder_optimizer.zero_grad()\n            self.decoder_optimizer.zero_grad()\n\n            encoder_out   = self.encoder(images)\n            caption_lengths = torch.tensor(\n                [self.max_label_length] * labels.size(0)\n            ).unsqueeze(1).to(self.device)\n\n            outputs, encoded_captions, decode_lengths, alphas, sort_ind = self.decoder(\n                encoder_out, labels, caption_lengths\n            )\n\n            # Targets = encoded captions without the <SOS>\n            targets = encoded_captions[:, 1:]\n\n            # Flatten for loss\n            outputs_flat = outputs.view(-1, self.decoder.fc.out_features)\n            targets_flat = targets.contiguous().view(-1)\n\n            loss = self.criterion(outputs_flat, targets_flat)\n            loss.backward()\n\n            self.decoder_optimizer.step()\n            self.encoder_optimizer.step()\n\n            running_loss += loss.item()\n\n            # Compute CER for the batch (global style)\n            batch_size = labels.size(0)\n            _, preds_flat = torch.max(outputs_flat, dim=1)\n            preds_seq = preds_flat.view(batch_size, -1)\n\n            for i in range(batch_size):\n                pred_indices   = preds_seq[i].detach().cpu().numpy()\n                target_indices = targets[i].detach().cpu().numpy()\n\n                mask          = (target_indices != self.char_to_idx['<PAD>'])\n                pred_indices  = pred_indices[mask]\n                target_indices= target_indices[mask]\n\n                pred_chars    = [self.idx_to_char.get(idx, '') for idx in pred_indices]\n                target_chars  = [self.idx_to_char.get(idx, '') for idx in target_indices]\n                pred_str      = ''.join(pred_chars)\n                target_str    = ''.join(target_chars)\n\n                edit_dist           = editdistance.eval(pred_str, target_str)\n                total_edit_distance += edit_dist\n                total_ref_length    += len(target_str)\n\n            # if (batch_idx + 1) % 50 == 0:\n            #     print(f'Batch {batch_idx + 1}/{len(self.train_loader)} - Loss: {loss.item():.4f}')\n\n        avg_loss = running_loss / len(self.train_loader)\n        avg_cer  = total_edit_distance / total_ref_length if total_ref_length > 0 else 0.0\n        return avg_loss, avg_cer\n\n    def validate_one_epoch(self, top_n=5):\n        self.encoder.eval()\n        self.decoder.eval()\n        running_loss         = 0.0\n        total_edit_distance  = 0\n        total_ref_length     = 0\n\n        # each sample’s CER\n        sample_cer_info = []\n\n        with torch.no_grad():\n            for batch_idx, (images, labels, label_lengths) in enumerate(self.val_loader):\n                images        = images.to(self.device, non_blocking=True)\n                labels        = labels.to(self.device, non_blocking=True)\n                label_lengths = label_lengths.to(self.device, non_blocking=True)\n\n                encoder_out = self.encoder(images)\n                caption_lengths = torch.tensor(\n                    [self.max_label_length] * labels.size(0)\n                ).unsqueeze(1).to(self.device)\n\n                outputs, encoded_captions, decode_lengths, alphas, sort_ind = self.decoder(\n                    encoder_out, labels, caption_lengths\n                )\n                targets = encoded_captions[:, 1:]\n\n                outputs_flat = outputs.view(-1, self.decoder.fc.out_features)\n                targets_flat = targets.contiguous().view(-1)\n\n                loss = self.criterion(outputs_flat, targets_flat)\n                running_loss += loss.item()\n\n                batch_size = labels.size(0)\n                _, preds_flat = torch.max(outputs_flat, dim=1)\n                preds_seq = preds_flat.view(batch_size, -1)\n\n                for i in range(batch_size):\n                    pred_indices   = preds_seq[i].detach().cpu().numpy()\n                    target_indices = targets[i].detach().cpu().numpy()\n\n                    mask           = (target_indices != self.char_to_idx['<PAD>'])\n                    pred_indices   = pred_indices[mask]\n                    target_indices = target_indices[mask]\n\n                    pred_chars   = [self.idx_to_char.get(idx, '') for idx in pred_indices]\n                    target_chars = [self.idx_to_char.get(idx, '') for idx in target_indices]\n                    pred_str     = ''.join(pred_chars)\n                    target_str   = ''.join(target_chars)\n\n                    edit_dist = editdistance.eval(pred_str, target_str)\n                    ref_len   = len(target_str)\n                    cer       = edit_dist / ref_len if ref_len > 0 else 0\n    \n                    total_edit_distance += edit_dist\n                    total_ref_length    += ref_len\n    \n                    # Store sample info\n                    # sample_cer_info.append({\n                    #     \"pred\": pred_str,\n                    #     \"gt\": target_str,\n                    #     \"cer\": cer\n                    # })\n\n                    # Print a few samples from the 1st batch\n                    # if batch_idx == 0 and i < 3:\n                    #     print(f\"Sample {i + 1}:\")\n                    #     print(f\"Predicted: {pred_str}\")\n                    #     print(f\"Target   : {target_str}\\n\")\n\n        avg_loss = running_loss / len(self.val_loader)\n        avg_cer  = total_edit_distance / total_ref_length if total_ref_length > 0 else 0.0\n\n        # Sort by CER descending\n        sample_cer_info.sort(key=lambda x: x[\"cer\"], reverse=True)\n        # Take top_n\n        worst_samples = sample_cer_info[:top_n]\n    \n        # print(f\"\\n=== Top {top_n} Worst Samples by CER ===\")\n        # for idx, sample in enumerate(worst_samples):\n        #     print(f\"[{idx+1}] CER: {sample['cer']:.3f}\")\n        #     print(f\"   Predicted: {sample['pred']}\")\n        #     print(f\"   Ground Truth: {sample['gt']}\\n\")\n       \n        return avg_loss, avg_cer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T04:36:52.887062Z","iopub.execute_input":"2025-04-01T04:36:52.887326Z","iopub.status.idle":"2025-04-01T04:36:52.908731Z","shell.execute_reply.started":"2025-04-01T04:36:52.887299Z","shell.execute_reply":"2025-04-01T04:36:52.908118Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"test_ground_truth_path = os.path.join(base_dir, '/kaggle/input/balinese/data/balinese_transliteration_test.txt')\ntest_images_dir        = os.path.join(base_dir, '/kaggle/input/balinese/data/balinese_word_test')\n\ntest_filenames = []\ntest_labels    = []\n\nwith open(test_ground_truth_path, 'r', encoding='utf-8') as file:\n    for line in file:\n        line = line.strip()\n        if line:\n            parts = line.split(';')\n            if len(parts) == 2:\n                filename, label = parts\n                label = label.lower()\n                test_filenames.append(filename)\n                test_labels.append(label)\n            else:\n                print(f\"Skipping malformed line: {line}\")\n\ntest_data = pd.DataFrame({\n    'filename': test_filenames,\n    'label': test_labels\n})\n\n# Check for unknown chars in test set\ntest_chars = set(''.join(test_data['label']))\nunknown_chars = test_chars - set(char_to_idx.keys())\nprint(f\"Unknown characters in test labels: {unknown_chars}\")\n\n# Encode test labels\nmax_label_length_test = max(len(lbl) for lbl in test_data['label']) + 2\ndef encode_label(label, char_to_idx, max_length):\n    \"\"\"\n    Converts a label (string) into a list of indices with <SOS>, <EOS>, and padding.\n    Assumes that all characters in the label exist in the vocabulary.\n    \"\"\"\n    encoded = (\n        [char_to_idx['<SOS>']] +\n        [char_to_idx[ch] for ch in label] +\n        [char_to_idx['<EOS>']]\n    )\n    # Pad if needed\n    if len(encoded) < max_length:\n        encoded += [char_to_idx['<PAD>']] * (max_length - len(encoded))\n    else:\n        encoded = encoded[:max_length]\n    return encoded\n\ntest_data['encoded_label'] = test_data['label'].apply(lambda x: encode_label_test(x, char_to_idx, max_label_length_test))\ntest_data['label_length']  = test_data['label'].apply(len)\n\ntest_transform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(\n        mean=(0.5, 0.5, 0.5),\n        std=(0.5, 0.5, 0.5)\n    )\n])\n\ntest_dataset = BalineseDataset(test_data, test_images_dir, transform=test_transform)\ntest_loader  = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T04:36:52.910370Z","iopub.execute_input":"2025-04-01T04:36:52.910572Z","iopub.status.idle":"2025-04-01T04:36:52.988924Z","shell.execute_reply.started":"2025-04-01T04:36:52.910555Z","shell.execute_reply":"2025-04-01T04:36:52.987827Z"}},"outputs":[{"name":"stdout","text":"Unknown characters in test labels: set()\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m<ipython-input-23-577c337b3e14>\u001b[0m in \u001b[0;36m<cell line: 49>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mencoded\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m \u001b[0mtest_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'encoded_label'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mencode_label_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchar_to_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_label_length_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label_length'\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[1;32m   4922\u001b[0m             \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4923\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4924\u001b[0;31m         ).apply()\n\u001b[0m\u001b[1;32m   4925\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4926\u001b[0m     def _reindex_indexer(\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1425\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1426\u001b[0m         \u001b[0;31m# self.func is Callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1427\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1428\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1429\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1505\u001b[0m         \u001b[0;31m#  Categorical (GH51645).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1506\u001b[0m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"ignore\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCategoricalDtype\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1507\u001b[0;31m         mapped = obj._map_values(\n\u001b[0m\u001b[1;32m   1508\u001b[0m             \u001b[0mmapper\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcurried\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_action\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1509\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/base.py\u001b[0m in \u001b[0;36m_map_values\u001b[0;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m    919\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_action\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mna_action\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    920\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 921\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0malgorithms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_action\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mna_action\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    922\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mfinal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/algorithms.py\u001b[0m in \u001b[0;36mmap_array\u001b[0;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1742\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mna_action\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1743\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1744\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m         return lib.map_infer_mask(\n","\u001b[0;32mlib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n","\u001b[0;32m<ipython-input-23-577c337b3e14>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mencoded\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m \u001b[0mtest_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'encoded_label'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mencode_label_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchar_to_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_label_length_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label_length'\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-7-70af901c28a6>\u001b[0m in \u001b[0;36mencode_label_test\u001b[0;34m(label, char_to_idx, max_length)\u001b[0m\n\u001b[1;32m     33\u001b[0m     encoded = (\n\u001b[1;32m     34\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mchar_to_idx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'<SOS>'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0;34m[\u001b[0m\u001b[0mchar_to_idx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchar_to_idx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'<UNK>'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mchar_to_idx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'<EOS>'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     )\n","\u001b[0;32m<ipython-input-7-70af901c28a6>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     33\u001b[0m     encoded = (\n\u001b[1;32m     34\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mchar_to_idx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'<SOS>'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0;34m[\u001b[0m\u001b[0mchar_to_idx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchar_to_idx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'<UNK>'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mchar_to_idx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'<EOS>'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     )\n","\u001b[0;31mKeyError\u001b[0m: '<UNK>'"],"ename":"KeyError","evalue":"'<UNK>'","output_type":"error"}],"execution_count":23},{"cell_type":"code","source":"def inference(encoder, decoder, data_loader, device, char_to_idx, idx_to_char, max_seq_length, test_data):\n    encoder.eval()\n    decoder.eval()\n\n    eos_idx = char_to_idx['<EOS>']\n    results = []\n\n    with torch.no_grad():\n        for batch_idx, (images, labels, label_lengths) in enumerate(data_loader):\n            images = images.to(device, non_blocking=True)\n            labels = labels.to(device, non_blocking=True)\n\n            batch_size  = images.size(0)\n            encoder_out = encoder(images)  # [B, num_patches, encoder_dim]\n\n            # Init LSTM state\n            h1, c1, h2, c2 = decoder.init_hidden_state(encoder_out)\n\n            # Start tokens (all <SOS>)\n            inputs = torch.full(\n                (batch_size,),\n                fill_value=char_to_idx['<SOS>'],\n                dtype=torch.long,\n                device=device\n            )\n\n            all_preds = []\n\n            for _ in range(max_seq_length):\n                # Embedding\n                embeddings = decoder.embedding(inputs)\n\n                # Attention\n                attention_weighted_encoding, _ = decoder.attention(encoder_out, h1)\n\n                # Gating\n                gate = decoder.sigmoid(decoder.f_beta(h1))\n                attention_weighted_encoding = gate * attention_weighted_encoding\n\n                # Pass through LSTM layers\n                h1, c1 = decoder.lstm1(\n                    torch.cat([embeddings, attention_weighted_encoding], dim=1),\n                    (h1, c1)\n                )\n                h2, c2 = decoder.lstm2(h1, (h2, c2))\n\n                # Get predicted token\n                preds = decoder.fc(decoder.dropout(h2))  # [batch_size, vocab_size]\n                _, preds_idx = preds.max(dim=1)\n\n                # Feed next token\n                all_preds.append(preds_idx.cpu().numpy())\n                inputs = preds_idx\n\n            # Reformat predictions to [batch_size, max_seq_length]\n            all_preds = np.array(all_preds).T\n\n            for i in range(batch_size):\n                pred_indices = all_preds[i]\n\n                # Stop at <EOS> if present\n                if eos_idx in pred_indices:\n                    first_eos = np.where(pred_indices == eos_idx)[0][0]\n                    pred_indices = pred_indices[:first_eos]\n\n                # Convert token indices -> string\n                pred_chars = [idx_to_char.get(idx, '') for idx in pred_indices]\n                pred_str   = ''.join(pred_chars)\n\n                # Process ground truth\n                label_indices = labels[i].cpu().numpy()\n                # remove <SOS>\n                label_indices = label_indices[1:]\n\n                if eos_idx in label_indices:\n                    eos_pos = np.where(label_indices == eos_idx)[0][0]\n                    label_indices = label_indices[:eos_pos]\n                else:\n                    # remove <PAD> if present\n                    label_indices = label_indices[label_indices != char_to_idx['<PAD>']]\n\n                label_chars = [idx_to_char.get(idx, '') for idx in label_indices]\n                label_str   = ''.join(label_chars)\n\n                global_idx    = batch_idx * batch_size + i\n                image_filename= test_data.iloc[global_idx]['filename']\n\n                results.append({\n                    'image_filename': image_filename,\n                    'predicted_caption': pred_str,\n                    'ground_truth_caption': label_str\n                })\n\n    return results","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T04:36:52.989566Z","iopub.status.idle":"2025-04-01T04:36:52.989802Z","shell.execute_reply":"2025-04-01T04:36:52.989705Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def calculate_global_cer(results):\n    total_ed   = 0\n    total_refs = 0\n    for r in results:\n        ref = r['ground_truth_caption']\n        hyp = r['predicted_caption']\n        dist = editdistance.eval(ref, hyp)\n        total_ed   += dist\n        total_refs += len(ref)\n    if total_refs == 0:\n        return 0.0\n    return total_ed / total_refs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T04:36:52.990302Z","iopub.status.idle":"2025-04-01T04:36:52.990624Z","shell.execute_reply":"2025-04-01T04:36:52.990486Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def print_top_worst_samples(results, n=5):\n    # Calculate CER for each sample\n    results_with_cer = []\n    for r in results:\n        ref = r['ground_truth_caption']\n        hyp = r['predicted_caption']\n        dist = editdistance.eval(ref, hyp)\n        length = len(ref)\n        cer = dist / length if length > 0 else 0\n        # Copy the record and add cer\n        new_r = r.copy()\n        new_r['cer'] = cer\n        results_with_cer.append(new_r)\n\n    # Sort by CER (descending) and take the top N\n    results_with_cer.sort(key=lambda x: x['cer'], reverse=True)\n    worst_samples = results_with_cer[:n]\n\n    print(f\"\\n=== Top {n} Worst Samples by CER ===\")\n    for i, sample in enumerate(worst_samples, start=1):\n        print(f\"{i}) Image: {sample['image_filename']}\")\n        print(f\"   CER: {sample['cer']:.4f}\")\n        print(f\"   Predicted       : {sample['predicted_caption']}\")\n        print(f\"   Ground Truth    : {sample['ground_truth_caption']}\")\n        print()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T04:36:52.991587Z","iopub.status.idle":"2025-04-01T04:36:52.991982Z","shell.execute_reply":"2025-04-01T04:36:52.991798Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"training_csv = \"training_results.csv\"\nif not os.path.exists(training_csv) or os.path.getsize(training_csv) == 0:\n    pd.DataFrame(columns=[\"model_name\", \"mode\", \"epoch1\", \"epoch2\"]).to_csv(training_csv, index=False)\n\ncsv_file = \"test_cer_results.csv\"\nif not os.path.exists(csv_file) or os.path.getsize(csv_file) == 0:\n    pd.DataFrame(columns=[\"model_name\", \"test_cer\"]).to_csv(csv_file, index=False)\n\ndef log_test_cer(model_name, cer_value):\n    df = pd.read_csv(csv_file)\n    # Check if model_name exists\n    if model_name in df['model_name'].values:\n        # Update existing row\n        df.loc[df['model_name'] == model_name, 'test_cer'] = cer_value\n    else:\n        # Add new row - use concat instead of append\n        new_row = pd.DataFrame({\"model_name\": [model_name], \"test_cer\": [cer_value]})\n        df = pd.concat([df, new_row], ignore_index=True)\n    \n    df.to_csv(csv_file, index=False)\n    print(f\"Logged {model_name}: {cer_value:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T04:36:52.992789Z","iopub.status.idle":"2025-04-01T04:36:52.993172Z","shell.execute_reply":"2025-04-01T04:36:52.993014Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"encoder_vit_large = ViTEncoder(model_name=\"vit_large_patch16_224\", pretrained=True)\ndecoder_vit_large = DecoderRNN(\n    attention_dim=256,\n    embed_dim=256,\n    decoder_dim=512,\n    vocab_size=vocab_size,\n    encoder_dim=encoder_vit_large.encoder_dim,\n    teacher_forcing_ratio=0.5\n)\n\nencoder_vit_large = encoder_vit_large.to(device)\ndecoder_vit_large = decoder_vit_large.to(device)\n\nencoder_optimizer_large = optim.Adam(encoder_vit_large.parameters(), lr=1e-4)\ndecoder_optimizer_large = optim.Adam(decoder_vit_large.parameters(), lr=4e-4)\n\ntrainer_large = ImageCaptioningTrainer(\n    encoder=encoder_vit_large,\n    decoder=decoder_vit_large,\n    criterion=criterion,                    \n    encoder_optimizer=encoder_optimizer_large,\n    decoder_optimizer=decoder_optimizer_large,\n    train_loader=train_loader,              \n    val_loader=val_loader,\n    device=device,\n    char_to_idx=char_to_idx,\n    idx_to_char=idx_to_char,\n    max_label_length=max_label_length,\n    model_name=\"vit_large_patch16_224\"     \n)\n\nnum_epochs = 2\ntrainer_large.fit(num_epochs)\n\nencoder_vit_large.eval()\ndecoder_vit_large.teacher_forcing_ratio = 0.0\n\nresults_vit_large = inference(\n    encoder=encoder_vit_large,\n    decoder=decoder_vit_large,\n    data_loader=test_loader,\n    device=device,\n    char_to_idx=char_to_idx,\n    idx_to_char=idx_to_char,\n    max_seq_length=max_label_length_test,\n    test_data=test_data\n)\n\ncer_vit_large = calculate_global_cer(results_vit_large)\nprint(f\"ViT Large — Test CER: {cer_vit_large:.4f}\")\n\nprint_top_worst_samples(results_vit_large, n=5)\n\n# CSV logging \nlog_test_cer(\"vit_large_patch16_224\", cer_vit_large)\n\n# Manually delete references to free GPU memory\ndel encoder_vit_large\ndel decoder_vit_large\ndel trainer_large\n\n# Empty the PyTorch CUDA cache\ntorch.cuda.empty_cache()\nprint(\"Memory cleared\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T04:36:52.993842Z","iopub.status.idle":"2025-04-01T04:36:52.994225Z","shell.execute_reply":"2025-04-01T04:36:52.994066Z"}},"outputs":[],"execution_count":null}]}