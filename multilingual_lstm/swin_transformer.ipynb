{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52ee18c4-7d20-4008-a971-06f1bd0f39e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import editdistance\n",
    "import time \n",
    "\n",
    "from PIL import Image\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import timm  \n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96f12cf6-64ef-4626-9f39-9812dca91ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0db252f6-2b6c-432a-9035-0ae2156a123c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base project path\n",
    "BASE_DIR = \"/projects/qb36/lontar_project\"\n",
    "DATA_DIR = os.path.join(BASE_DIR, \"data\")\n",
    "\n",
    "# Define file paths\n",
    "# balinese_ground_truth_path = os.path.join(DATA_DIR, 'balinese_transliteration_train.txt')\n",
    "# images_dir = os.path.join(DATA_DIR, 'balinese_word_train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40817bb8-ce60-48d3-927c-04a2629c08b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image_labels(data_dir, ground_truth_file):\n",
    "    filenames, labels = [], []\n",
    "    with open(ground_truth_file, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            parts = line.strip().split(';')\n",
    "            if len(parts) == 2:\n",
    "                filename, label = parts\n",
    "                filenames.append(os.path.join(data_dir, filename))\n",
    "                labels.append(label.lower())\n",
    "    return filenames, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff154e0d-da41-4cef-9953-be3ce53e4958",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the list of languages (Multilingual setup)\n",
    "languages = ['balinese', 'khmer', 'sundanese']\n",
    "\n",
    "# Load data for all languages\n",
    "filenames, labels = [], []\n",
    "for lang in languages:\n",
    "    img_dir = os.path.join(DATA_DIR, f'{lang}_word_train')\n",
    "    gt_path = os.path.join(DATA_DIR, f'{lang}_transliteration_train.txt')\n",
    "    img_files, lbls = load_image_labels(img_dir, gt_path)\n",
    "    filenames.extend(img_files)\n",
    "    labels.extend(lbls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "175121cf-9739-4410-b71a-6be0cc26d7e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 41\n",
      "Training size: 29502; Validation size: 3279\n"
     ]
    }
   ],
   "source": [
    "# Building the vocabulary\n",
    "all_text = ''.join(labels)\n",
    "unique_chars = sorted(set(all_text))\n",
    "\n",
    "# Special tokens\n",
    "char_to_idx = {char: idx for idx, char in enumerate(['<PAD>', '<UNK>', '<SOS>', '<EOS>'] + unique_chars)}\n",
    "idx_to_char = {idx: char for char, idx in char_to_idx.items()}\n",
    "vocab_size = len(char_to_idx)\n",
    "print(f'Vocabulary size: {vocab_size}')\n",
    "\n",
    "# Encoding labels\n",
    "def encode_label(label, char_to_idx, max_length):\n",
    "    encoded = [char_to_idx['<SOS>']] + [char_to_idx.get(ch, char_to_idx['<UNK>']) for ch in label] + [char_to_idx['<EOS>']]\n",
    "    encoded += [char_to_idx['<PAD>']] * (max_length - len(encoded))\n",
    "    return encoded[:max_length]\n",
    "\n",
    "max_label_length = max(len(label) for label in labels) + 2\n",
    "encoded_labels = [encode_label(label, char_to_idx, max_label_length) for label in labels]\n",
    "\n",
    "# Data preparation\n",
    "data = pd.DataFrame({\n",
    "    'filename': filenames,\n",
    "    'label': labels,\n",
    "    'encoded_label': encoded_labels,\n",
    "    'label_length': [len(lbl) for lbl in encoded_labels]\n",
    "})\n",
    "\n",
    "# Train-validation split\n",
    "def custom_split(df, test_size=0.1, random_state=42):\n",
    "    return train_test_split(df, test_size=test_size, random_state=random_state)\n",
    "\n",
    "train_data, val_data = custom_split(data)\n",
    "print(f'Training size: {len(train_data)}; Validation size: {len(val_data)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d307eca6-2574-4032-80d7-844b8e58184d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultilingualDataset(Dataset):\n",
    "    def __init__(self, df, transform=None):\n",
    "        self.data = df.reset_index(drop=True)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.data.loc[idx, 'filename']\n",
    "        label = self.data.loc[idx, 'encoded_label']\n",
    "        label_length = self.data.loc[idx, 'label_length']\n",
    "\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        label = torch.tensor(label, dtype=torch.long)\n",
    "        return image, label, torch.tensor(label_length, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bbb03afe-e10e-45eb-b348-1351e71f38d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaders created successfully.\n"
     ]
    }
   ],
   "source": [
    "# Transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "batch_size = 64\n",
    "train_dataset = MultilingualDataset(train_data, transform=transform)\n",
    "val_dataset = MultilingualDataset(val_data, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "print(\"Data loaders created successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "591ff7ca-4284-4662-bf71-5962fe8793f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwinEncoder(nn.Module):\n",
    "    def __init__(self, model_name=\"swin_small_patch4_window7_224\", pretrained=True):\n",
    "        \"\"\"\n",
    "        A simple Swin Transformer encoder that extracts patch embeddings\n",
    "        as [batch_size, num_patches, hidden_dim]. We'll use timm to load \n",
    "        a pretrained Swin model, remove its classification head, then flatten.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.swin = timm.create_model(model_name, pretrained=pretrained)\n",
    "        self.swin.head = nn.Identity()\n",
    "\n",
    "        # We'll assign encoder_dim dynamically after forward\n",
    "        self.encoder_dim = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        :param x: [batch_size, 3, 224, 224]\n",
    "        :return:  [batch_size, num_patches, encoder_dim]\n",
    "        \"\"\"\n",
    "        feats = self.swin.forward_features(x)            # [B, C, H, W]\n",
    "        b, c, h, w = feats.shape\n",
    "        feats = feats.flatten(2).transpose(1, 2)         # [B, H*W, C]\n",
    "        # Set encoder_dim once (C)\n",
    "        if self.encoder_dim is None:\n",
    "            self.encoder_dim = feats.shape[-1]\n",
    "        return feats\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "793fd95b-6f1e-4c77-9bdb-efb501e919a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, encoder_dim, decoder_dim, attention_dim):\n",
    "        super(Attention, self).__init__()\n",
    "        self.encoder_att = nn.Linear(encoder_dim, attention_dim)  # transform encoder output\n",
    "        self.decoder_att = nn.Linear(decoder_dim, attention_dim)  # transform decoder hidden\n",
    "        self.full_att    = nn.Linear(attention_dim, 1)\n",
    "        self.relu        = nn.ReLU()\n",
    "        self.softmax     = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, encoder_out, decoder_hidden):\n",
    "        \"\"\"\n",
    "        encoder_out:    [batch_size, num_patches, encoder_dim]\n",
    "        decoder_hidden: [batch_size, decoder_dim]\n",
    "        \"\"\"\n",
    "        att1 = self.encoder_att(encoder_out)                  # [batch_size, num_patches, attention_dim]\n",
    "        att2 = self.decoder_att(decoder_hidden).unsqueeze(1)  # [batch_size, 1, attention_dim]\n",
    "\n",
    "        # sum -> relu -> full_att -> squeeze -> softmax\n",
    "        att  = self.full_att(self.relu(att1 + att2)).squeeze(2)  # [batch_size, num_patches]\n",
    "        alpha = self.softmax(att)\n",
    "        # Weighted sum of the encoder_out\n",
    "        attention_weighted_encoding = (encoder_out * alpha.unsqueeze(2)).sum(dim=1)  # [batch_size, encoder_dim]\n",
    "        return attention_weighted_encoding, alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b1e35eb8-1577-422f-b680-4fd83a0c79f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, attention_dim, embed_dim, decoder_dim, vocab_size, encoder_dim=768, teacher_forcing_ratio=0.5):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "\n",
    "        self.attention     = Attention(encoder_dim, decoder_dim, attention_dim)\n",
    "        self.embedding     = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.dropout       = nn.Dropout(p=0.5)\n",
    "\n",
    "        # [embed_dim + encoder_dim] -> decoder_dim\n",
    "        self.lstm1 = nn.LSTMCell(embed_dim + encoder_dim, decoder_dim)\n",
    "        # decoder_dim -> decoder_dim\n",
    "        self.lstm2 = nn.LSTMCell(decoder_dim, decoder_dim)\n",
    "\n",
    "        # For initializing the hidden states of both LSTM layers\n",
    "        self.init_h1 = nn.Linear(encoder_dim, decoder_dim)\n",
    "        self.init_c1 = nn.Linear(encoder_dim, decoder_dim)\n",
    "        self.init_h2 = nn.Linear(encoder_dim, decoder_dim)\n",
    "        self.init_c2 = nn.Linear(encoder_dim, decoder_dim)\n",
    "\n",
    "        # Gating\n",
    "        self.f_beta  = nn.Linear(decoder_dim, encoder_dim)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "        # Final linear layer for output vocab\n",
    "        self.fc = nn.Linear(decoder_dim, vocab_size)\n",
    "\n",
    "        self.teacher_forcing_ratio = teacher_forcing_ratio\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        self.embedding.weight.data.uniform_(-0.1, 0.1)\n",
    "        self.fc.bias.data.fill_(0)\n",
    "        self.fc.weight.data.uniform_(-0.1, 0.1)\n",
    "\n",
    "    def init_hidden_state(self, encoder_out):\n",
    "        # encoder_out: [batch_size, num_patches, encoder_dim]\n",
    "        mean_encoder_out = encoder_out.mean(dim=1)  # [batch_size, encoder_dim]\n",
    "        h1 = self.init_h1(mean_encoder_out)         # [batch_size, decoder_dim]\n",
    "        c1 = self.init_c1(mean_encoder_out)         # [batch_size, decoder_dim]\n",
    "        h2 = self.init_h2(mean_encoder_out)         # [batch_size, decoder_dim]\n",
    "        c2 = self.init_c2(mean_encoder_out)         # [batch_size, decoder_dim]\n",
    "        return (h1, c1, h2, c2)\n",
    "\n",
    "\n",
    "    def forward(self, encoder_out, encoded_captions, caption_lengths):\n",
    "        \"\"\"\n",
    "        encoder_out:      [batch_size, num_patches, encoder_dim]\n",
    "        encoded_captions: [batch_size, max_label_length]\n",
    "        caption_lengths:  [batch_size, 1]\n",
    "        \"\"\"\n",
    "        caption_lengths, sort_ind = caption_lengths.squeeze(1).sort(dim=0, descending=True)\n",
    "        encoder_out      = encoder_out[sort_ind]\n",
    "        encoded_captions = encoded_captions[sort_ind]\n",
    "\n",
    "        embeddings = self.embedding(encoded_captions)\n",
    "\n",
    "        # Initialize hidden states for both LSTM layers\n",
    "        h1, c1, h2, c2 = self.init_hidden_state(encoder_out)\n",
    "\n",
    "        decode_lengths    = (caption_lengths - 1).tolist()\n",
    "        max_decode_length = max(decode_lengths)\n",
    "\n",
    "        batch_size = encoder_out.size(0)\n",
    "        vocab_size = self.fc.out_features\n",
    "\n",
    "        predictions = torch.zeros(batch_size, max_decode_length, vocab_size, device=encoder_out.device)\n",
    "        alphas      = torch.zeros(batch_size, max_decode_length, encoder_out.size(1), device=encoder_out.device)\n",
    "\n",
    "        # We'll feed the first token from the input (<SOS>) or from the previous prediction\n",
    "        prev_tokens = encoded_captions[:, 0].clone()\n",
    "\n",
    "        for t in range(max_decode_length):\n",
    "            batch_size_t = sum([l > t for l in decode_lengths])\n",
    "\n",
    "            attention_weighted_encoding, alpha = self.attention(\n",
    "                encoder_out[:batch_size_t],\n",
    "                h1[:batch_size_t]  # use the first LSTM layer's hidden state for attention\n",
    "            )\n",
    "\n",
    "            # Apply gating\n",
    "            gate = self.sigmoid(self.f_beta(h1[:batch_size_t]))\n",
    "            attention_weighted_encoding = gate * attention_weighted_encoding\n",
    "\n",
    "            # Teacher forcing?\n",
    "            use_teacher_forcing = (torch.rand(1).item() < self.teacher_forcing_ratio)\n",
    "            if use_teacher_forcing:\n",
    "                current_input = embeddings[:batch_size_t, t, :]\n",
    "            else:\n",
    "                current_input = self.embedding(prev_tokens[:batch_size_t].detach())\n",
    "\n",
    "            # first lstm layer\n",
    "            h1_next, c1_next = self.lstm1(\n",
    "                torch.cat([current_input, attention_weighted_encoding], dim=1),\n",
    "                (h1[:batch_size_t], c1[:batch_size_t])\n",
    "            )\n",
    "\n",
    "            # second lstm layer\n",
    "            h2_next, c2_next = self.lstm2(\n",
    "                h1_next, (h2[:batch_size_t], c2[:batch_size_t])\n",
    "            )\n",
    "\n",
    "            # Use the second LSTM layer's output (h2_next) for final prediction\n",
    "            preds = self.fc(self.dropout(h2_next))\n",
    "            predictions[:batch_size_t, t, :] = preds\n",
    "            alphas[:batch_size_t, t, :]      = alpha\n",
    "\n",
    "            # Update prev_tokens with the best predicted token\n",
    "            _, next_tokens = preds.max(dim=1)\n",
    "            prev_tokens_ = prev_tokens.clone()\n",
    "            prev_tokens_[:batch_size_t] = next_tokens.detach()\n",
    "            prev_tokens = prev_tokens_\n",
    "\n",
    "            # Update hidden states\n",
    "            # For samples still in the batch, store the new h1, c1, h2, c2\n",
    "            h1_new = torch.zeros_like(h1)\n",
    "            c1_new = torch.zeros_like(c1)\n",
    "            h2_new = torch.zeros_like(h2)\n",
    "            c2_new = torch.zeros_like(c2)\n",
    "\n",
    "            h1_new[:batch_size_t] = h1_next\n",
    "            c1_new[:batch_size_t] = c1_next\n",
    "            h2_new[:batch_size_t] = h2_next\n",
    "            c2_new[:batch_size_t] = c2_next\n",
    "\n",
    "            h1, c1, h2, c2 = h1_new, c1_new, h2_new, c2_new\n",
    "\n",
    "        return predictions, encoded_captions, decode_lengths, alphas, sort_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c64f622d-9133-4436-b189-ae127e5afadf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageCaptioningTrainer:\n",
    "    def __init__(self, encoder, decoder, \n",
    "                 criterion, encoder_optimizer, decoder_optimizer, \n",
    "                 train_loader, val_loader, test_loader, test_data, max_label_length_test,\n",
    "                 device, char_to_idx, idx_to_char, max_label_length,\n",
    "                 model_name, csv_filename=\"training_results.csv\"):\n",
    "        self.encoder = encoder.to(device)\n",
    "        self.decoder = decoder.to(device)\n",
    "        self.criterion = criterion\n",
    "        self.encoder_optimizer = encoder_optimizer\n",
    "        self.decoder_optimizer = decoder_optimizer\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.test_loader = test_loader\n",
    "        self.test_data = test_data\n",
    "        self.max_label_length_test = max_label_length_test\n",
    "        self.device = device\n",
    "        self.char_to_idx = char_to_idx\n",
    "        self.idx_to_char = idx_to_char\n",
    "        self.max_label_length = max_label_length\n",
    "        self.model_name = model_name\n",
    "        self.csv_filename = csv_filename\n",
    "\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "        self.train_cers = []\n",
    "        self.val_cers = []\n",
    "        self.test_cers = []\n",
    "\n",
    "    \n",
    "    def fit(self, num_epochs):\n",
    "        start_time = time.time()\n",
    "        best_test_cer = float('inf')\n",
    "        early_stop_counter = 0\n",
    "        early_stop_patience = 5\n",
    "    \n",
    "        for epoch in range(num_epochs):\n",
    "            print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
    "            train_loss, train_cer = self.train_one_epoch()\n",
    "            val_loss, val_cer = self.validate_one_epoch()\n",
    "    \n",
    "            results = inference(\n",
    "                encoder=self.encoder,\n",
    "                decoder=self.decoder,\n",
    "                data_loader=self.test_loader,\n",
    "                device=self.device,\n",
    "                char_to_idx=self.char_to_idx,\n",
    "                idx_to_char=self.idx_to_char,\n",
    "                max_seq_length=self.max_label_length_test,\n",
    "                test_data=self.test_data\n",
    "            )\n",
    "            test_cer = calculate_global_cer(results)\n",
    "    \n",
    "            print(f\"[{epoch+1}/{num_epochs}] \"\n",
    "                  f\"Train Loss: {train_loss:.4f}, Train CER: {train_cer:.4f} | \"\n",
    "                  f\"Val Loss: {val_loss:.4f}, Val CER: {val_cer:.4f} | \"\n",
    "                  f\"Test CER: {test_cer:.4f}\")\n",
    "    \n",
    "            self.train_losses.append(train_loss)\n",
    "            self.val_losses.append(val_loss)\n",
    "            self.train_cers.append(train_cer)\n",
    "            self.val_cers.append(val_cer)\n",
    "            self.test_cers.append(test_cer)\n",
    "    \n",
    "            if test_cer < best_test_cer:\n",
    "                best_test_cer = test_cer\n",
    "                early_stop_counter = 0\n",
    "            else:\n",
    "                early_stop_counter += 1\n",
    "                print(f\"Test CER did not improve. Early stop counter: {early_stop_counter}/{early_stop_patience}\")\n",
    "            \n",
    "            if early_stop_counter >= early_stop_patience:\n",
    "                print(\"Early stopping triggered.\")\n",
    "                break\n",
    "    \n",
    "        total_time = time.time() - start_time\n",
    "        hours = int(total_time // 3600)\n",
    "        minutes = int((total_time % 3600) // 60)\n",
    "        print(f\"\\nTraining completed in {hours}h {minutes}m.\")\n",
    "    \n",
    "        # Save CSV\n",
    "        num_epochs_recorded = len(self.train_losses)\n",
    "        epoch_cols = [f\"epoch{i+1}\" for i in range(num_epochs_recorded)]\n",
    "    \n",
    "        new_rows = pd.DataFrame([\n",
    "            [self.model_name, \"training loss\"] + self.train_losses,\n",
    "            [self.model_name, \"validation loss\"] + self.val_losses,\n",
    "            [self.model_name, \"training cer\"] + self.train_cers,\n",
    "            [self.model_name, \"validation cer\"] + self.val_cers,\n",
    "            [self.model_name, \"test cer\"] + self.test_cers\n",
    "        ], columns=[\"model_name\", \"mode\"] + epoch_cols)\n",
    "    \n",
    "        if os.path.exists(self.csv_filename):\n",
    "            df_existing = pd.read_csv(self.csv_filename)\n",
    "            df_existing = df_existing[df_existing[\"model_name\"] != self.model_name]\n",
    "            df_updated = pd.concat([df_existing, new_rows], ignore_index=True)\n",
    "        else:\n",
    "            df_updated = new_rows\n",
    "    \n",
    "        df_updated[epoch_cols] = np.floor(df_updated[epoch_cols] * 100) / 100 \n",
    "        df_updated.to_csv(self.csv_filename, index=False)\n",
    "        print(f\"\\nResults have been written to: {self.csv_filename}\")\n",
    "\n",
    "    def train_one_epoch(self):\n",
    "        self.encoder.train()\n",
    "        self.decoder.train()\n",
    "        running_loss           = 0.0\n",
    "        total_edit_distance    = 0\n",
    "        total_ref_length       = 0\n",
    "\n",
    "        for batch_idx, (images, labels, label_lengths) in enumerate(self.train_loader):\n",
    "            images        = images.to(self.device, non_blocking=True)\n",
    "            labels        = labels.to(self.device, non_blocking=True)\n",
    "            label_lengths = label_lengths.to(self.device, non_blocking=True)\n",
    "\n",
    "            self.encoder_optimizer.zero_grad()\n",
    "            self.decoder_optimizer.zero_grad()\n",
    "\n",
    "            encoder_out   = self.encoder(images)\n",
    "            caption_lengths = torch.tensor(\n",
    "                [self.max_label_length] * labels.size(0)\n",
    "            ).unsqueeze(1).to(self.device)\n",
    "\n",
    "            outputs, encoded_captions, decode_lengths, alphas, sort_ind = self.decoder(\n",
    "                encoder_out, labels, caption_lengths\n",
    "            )\n",
    "\n",
    "            # Targets = encoded captions without the <SOS>\n",
    "            targets = encoded_captions[:, 1:]\n",
    "\n",
    "            # Flatten for loss\n",
    "            outputs_flat = outputs.view(-1, self.decoder.fc.out_features)\n",
    "            targets_flat = targets.contiguous().view(-1)\n",
    "\n",
    "            loss = self.criterion(outputs_flat, targets_flat)\n",
    "            loss.backward()\n",
    "\n",
    "            self.decoder_optimizer.step()\n",
    "            self.encoder_optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            # Compute CER for the batch (global style)\n",
    "            batch_size = labels.size(0)\n",
    "            _, preds_flat = torch.max(outputs_flat, dim=1)\n",
    "            preds_seq = preds_flat.view(batch_size, -1)\n",
    "\n",
    "            for i in range(batch_size):\n",
    "                pred_indices   = preds_seq[i].detach().cpu().numpy()\n",
    "                target_indices = targets[i].detach().cpu().numpy()\n",
    "\n",
    "                mask          = (target_indices != self.char_to_idx['<PAD>'])\n",
    "                pred_indices  = pred_indices[mask]\n",
    "                target_indices= target_indices[mask]\n",
    "\n",
    "                pred_chars    = [self.idx_to_char.get(idx, '') for idx in pred_indices]\n",
    "                target_chars  = [self.idx_to_char.get(idx, '') for idx in target_indices]\n",
    "                pred_str      = ''.join(pred_chars)\n",
    "                target_str    = ''.join(target_chars)\n",
    "\n",
    "                edit_dist           = editdistance.eval(pred_str, target_str)\n",
    "                total_edit_distance += edit_dist\n",
    "                total_ref_length    += len(target_str)\n",
    "\n",
    "            # if (batch_idx + 1) % 50 == 0:\n",
    "            #     print(f'Batch {batch_idx + 1}/{len(self.train_loader)} - Loss: {loss.item():.4f}')\n",
    "\n",
    "        avg_loss = running_loss / len(self.train_loader)\n",
    "        avg_cer  = total_edit_distance / total_ref_length if total_ref_length > 0 else 0.0\n",
    "        return avg_loss, avg_cer\n",
    "\n",
    "    def validate_one_epoch(self, top_n=5):\n",
    "        self.encoder.eval()\n",
    "        self.decoder.eval()\n",
    "        running_loss         = 0.0\n",
    "        total_edit_distance  = 0\n",
    "        total_ref_length     = 0\n",
    "\n",
    "        # each sample’s CER\n",
    "        sample_cer_info = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (images, labels, label_lengths) in enumerate(self.val_loader):\n",
    "                images        = images.to(self.device, non_blocking=True)\n",
    "                labels        = labels.to(self.device, non_blocking=True)\n",
    "                label_lengths = label_lengths.to(self.device, non_blocking=True)\n",
    "\n",
    "                encoder_out = self.encoder(images)\n",
    "                caption_lengths = torch.tensor(\n",
    "                    [self.max_label_length] * labels.size(0)\n",
    "                ).unsqueeze(1).to(self.device)\n",
    "\n",
    "                outputs, encoded_captions, decode_lengths, alphas, sort_ind = self.decoder(\n",
    "                    encoder_out, labels, caption_lengths\n",
    "                )\n",
    "                targets = encoded_captions[:, 1:]\n",
    "\n",
    "                outputs_flat = outputs.view(-1, self.decoder.fc.out_features)\n",
    "                targets_flat = targets.contiguous().view(-1)\n",
    "\n",
    "                loss = self.criterion(outputs_flat, targets_flat)\n",
    "                running_loss += loss.item()\n",
    "\n",
    "                batch_size = labels.size(0)\n",
    "                _, preds_flat = torch.max(outputs_flat, dim=1)\n",
    "                preds_seq = preds_flat.view(batch_size, -1)\n",
    "\n",
    "                for i in range(batch_size):\n",
    "                    pred_indices   = preds_seq[i].detach().cpu().numpy()\n",
    "                    target_indices = targets[i].detach().cpu().numpy()\n",
    "\n",
    "                    mask           = (target_indices != self.char_to_idx['<PAD>'])\n",
    "                    pred_indices   = pred_indices[mask]\n",
    "                    target_indices = target_indices[mask]\n",
    "\n",
    "                    pred_chars   = [self.idx_to_char.get(idx, '') for idx in pred_indices]\n",
    "                    target_chars = [self.idx_to_char.get(idx, '') for idx in target_indices]\n",
    "                    pred_str     = ''.join(pred_chars)\n",
    "                    target_str   = ''.join(target_chars)\n",
    "\n",
    "                    edit_dist = editdistance.eval(pred_str, target_str)\n",
    "                    ref_len   = len(target_str)\n",
    "                    cer       = edit_dist / ref_len if ref_len > 0 else 0\n",
    "    \n",
    "                    total_edit_distance += edit_dist\n",
    "                    total_ref_length    += ref_len\n",
    "    \n",
    "                    # Store sample info\n",
    "                    # sample_cer_info.append({\n",
    "                    #     \"pred\": pred_str,\n",
    "                    #     \"gt\": target_str,\n",
    "                    #     \"cer\": cer\n",
    "                    # })\n",
    "\n",
    "                    # Print a few samples from the 1st batch\n",
    "                    # if batch_idx == 0 and i < 3:\n",
    "                    #     print(f\"Sample {i + 1}:\")\n",
    "                    #     print(f\"Predicted: {pred_str}\")\n",
    "                    #     print(f\"Target   : {target_str}\\n\")\n",
    "\n",
    "        avg_loss = running_loss / len(self.val_loader)\n",
    "        avg_cer  = total_edit_distance / total_ref_length if total_ref_length > 0 else 0.0\n",
    "\n",
    "        # Sort by CER descending\n",
    "        sample_cer_info.sort(key=lambda x: x[\"cer\"], reverse=True)\n",
    "        # Take top_n\n",
    "        worst_samples = sample_cer_info[:top_n]\n",
    "    \n",
    "        # print(f\"\\n=== Top {top_n} Worst Samples by CER ===\")\n",
    "        # for idx, sample in enumerate(worst_samples):\n",
    "        #     print(f\"[{idx+1}] CER: {sample['cer']:.3f}\")\n",
    "        #     print(f\"   Predicted: {sample['pred']}\")\n",
    "        #     print(f\"   Ground Truth: {sample['gt']}\\n\")\n",
    "       \n",
    "        return avg_loss, avg_cer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "58f94879-7a3d-41a8-98cf-c64a0dc83d1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unknown characters in test labels: set()\n"
     ]
    }
   ],
   "source": [
    "test_ground_truth_path = os.path.join(DATA_DIR, 'balinese_transliteration_test.txt')\n",
    "test_images_dir        = os.path.join(DATA_DIR, 'balinese_word_test')\n",
    "\n",
    "test_filenames = []\n",
    "test_labels    = []\n",
    "\n",
    "with open(test_ground_truth_path, 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        line = line.strip()\n",
    "        if line:\n",
    "            parts = line.split(';')\n",
    "            if len(parts) == 2:\n",
    "                filename, label = parts\n",
    "                label = label.lower()\n",
    "                test_filenames.append(filename)\n",
    "                test_labels.append(label)\n",
    "            else:\n",
    "                print(f\"Skipping malformed line: {line}\")\n",
    "\n",
    "test_data = pd.DataFrame({\n",
    "    'filename': test_filenames,\n",
    "    'label': test_labels\n",
    "})\n",
    "\n",
    "# Check for unknown chars in test set\n",
    "test_chars = set(''.join(test_data['label']))\n",
    "unknown_chars = test_chars - set(char_to_idx.keys())\n",
    "print(f\"Unknown characters in test labels: {unknown_chars}\")\n",
    "\n",
    "# Encode test labels\n",
    "max_label_length_test = max(len(lbl) for lbl in test_data['label']) + 2\n",
    "def encode_label_test(label, char_to_idx, max_length):\n",
    "    encoded = (\n",
    "        [char_to_idx['<SOS>']] +\n",
    "        [char_to_idx.get(ch, char_to_idx['<UNK>']) for ch in label] +\n",
    "        [char_to_idx['<EOS>']]\n",
    "    )\n",
    "    if len(encoded) > max_length:\n",
    "        encoded = encoded[:max_length]\n",
    "    else:\n",
    "        encoded += [char_to_idx['<PAD>']] * (max_length - len(encoded))\n",
    "    return encoded\n",
    "\n",
    "test_data['encoded_label'] = test_data['label'].apply(lambda x: encode_label_test(x, char_to_idx, max_label_length_test))\n",
    "test_data['label_length']  = test_data['label'].apply(len)\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=(0.5, 0.5, 0.5),\n",
    "        std=(0.5, 0.5, 0.5)\n",
    "    )\n",
    "])\n",
    "\n",
    "test_data['filename'] = test_data['filename'].apply(lambda x: os.path.join(test_images_dir, x))\n",
    "\n",
    "# Use MultilingualDataset without the extra image directory\n",
    "test_dataset = MultilingualDataset(test_data, transform=test_transform)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9384c147-036c-46e8-afd0-bf7cc1ad8ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(encoder, decoder, data_loader, device, char_to_idx, idx_to_char, max_seq_length, test_data):\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "\n",
    "    eos_idx = char_to_idx['<EOS>']\n",
    "    results = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (images, labels, label_lengths) in enumerate(data_loader):\n",
    "            images = images.to(device, non_blocking=True)\n",
    "            labels = labels.to(device, non_blocking=True)\n",
    "\n",
    "            batch_size  = images.size(0)\n",
    "            encoder_out = encoder(images)  # [B, num_patches, encoder_dim]\n",
    "\n",
    "            # Init LSTM state\n",
    "            h1, c1, h2, c2 = decoder.init_hidden_state(encoder_out)\n",
    "\n",
    "            # Start tokens (all <SOS>)\n",
    "            inputs = torch.full(\n",
    "                (batch_size,),\n",
    "                fill_value=char_to_idx['<SOS>'],\n",
    "                dtype=torch.long,\n",
    "                device=device\n",
    "            )\n",
    "\n",
    "            all_preds = []\n",
    "\n",
    "            for _ in range(max_seq_length):\n",
    "                # Embedding\n",
    "                embeddings = decoder.embedding(inputs)\n",
    "\n",
    "                # Attention\n",
    "                attention_weighted_encoding, _ = decoder.attention(encoder_out, h1)\n",
    "\n",
    "                # Gating\n",
    "                gate = decoder.sigmoid(decoder.f_beta(h1))\n",
    "                attention_weighted_encoding = gate * attention_weighted_encoding\n",
    "\n",
    "                # Pass through LSTM layers\n",
    "                h1, c1 = decoder.lstm1(\n",
    "                    torch.cat([embeddings, attention_weighted_encoding], dim=1),\n",
    "                    (h1, c1)\n",
    "                )\n",
    "                h2, c2 = decoder.lstm2(h1, (h2, c2))\n",
    "\n",
    "                # Get predicted token\n",
    "                preds = decoder.fc(decoder.dropout(h2))  # [batch_size, vocab_size]\n",
    "                _, preds_idx = preds.max(dim=1)\n",
    "\n",
    "                # Feed next token\n",
    "                all_preds.append(preds_idx.cpu().numpy())\n",
    "                inputs = preds_idx\n",
    "\n",
    "            # Reformat predictions to [batch_size, max_seq_length]\n",
    "            all_preds = np.array(all_preds).T\n",
    "\n",
    "            for i in range(batch_size):\n",
    "                pred_indices = all_preds[i]\n",
    "\n",
    "                # Stop at <EOS> if present\n",
    "                if eos_idx in pred_indices:\n",
    "                    first_eos = np.where(pred_indices == eos_idx)[0][0]\n",
    "                    pred_indices = pred_indices[:first_eos]\n",
    "\n",
    "                # Convert token indices -> string\n",
    "                pred_chars = [idx_to_char.get(idx, '') for idx in pred_indices]\n",
    "                pred_str   = ''.join(pred_chars)\n",
    "\n",
    "                # Process ground truth\n",
    "                label_indices = labels[i].cpu().numpy()\n",
    "                # remove <SOS>\n",
    "                label_indices = label_indices[1:]\n",
    "\n",
    "                if eos_idx in label_indices:\n",
    "                    eos_pos = np.where(label_indices == eos_idx)[0][0]\n",
    "                    label_indices = label_indices[:eos_pos]\n",
    "                else:\n",
    "                    # remove <PAD> if present\n",
    "                    label_indices = label_indices[label_indices != char_to_idx['<PAD>']]\n",
    "\n",
    "                label_chars = [idx_to_char.get(idx, '') for idx in label_indices]\n",
    "                label_str   = ''.join(label_chars)\n",
    "\n",
    "                global_idx    = batch_idx * batch_size + i\n",
    "                image_filename= test_data.iloc[global_idx]['filename']\n",
    "\n",
    "                results.append({\n",
    "                    'image_filename': image_filename,\n",
    "                    'predicted_caption': pred_str,\n",
    "                    'ground_truth_caption': label_str\n",
    "                })\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "897c292c-2a83-4f60-a0d4-6156f8ce9011",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_global_cer(results):\n",
    "    total_ed   = 0\n",
    "    total_refs = 0\n",
    "    for r in results:\n",
    "        ref = r['ground_truth_caption']\n",
    "        hyp = r['predicted_caption']\n",
    "        dist = editdistance.eval(ref, hyp)\n",
    "        total_ed   += dist\n",
    "        total_refs += len(ref)\n",
    "    if total_refs == 0:\n",
    "        return 0.0\n",
    "    return total_ed / total_refs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c297dda1-cd38-4fc3-a091-9a3503a71d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_top_worst_samples(results, n=10):\n",
    "    # Calculate CER for each sample\n",
    "    results_with_cer = []\n",
    "    for r in results:\n",
    "        ref = r['ground_truth_caption']\n",
    "        hyp = r['predicted_caption']\n",
    "        dist = editdistance.eval(ref, hyp)\n",
    "        length = len(ref)\n",
    "        cer = dist / length if length > 0 else 0\n",
    "        # Copy the record and add cer\n",
    "        new_r = r.copy()\n",
    "        new_r['cer'] = cer\n",
    "        results_with_cer.append(new_r)\n",
    "\n",
    "    # Sort by CER (descending) and take the top N\n",
    "    results_with_cer.sort(key=lambda x: x['cer'], reverse=True)\n",
    "    worst_samples = results_with_cer[:n]\n",
    "\n",
    "    print(f\"\\n=== Top {n} Worst Samples by CER ===\")\n",
    "    for i, sample in enumerate(worst_samples, start=1):\n",
    "        print(f\"{i}) Image: {sample['image_filename']}\")\n",
    "        print(f\"   CER: {sample['cer']:.4f}\")\n",
    "        print(f\"   Predicted       : {sample['predicted_caption']}\")\n",
    "        print(f\"   Ground Truth    : {sample['ground_truth_caption']}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fb63e5c1-f5e6-4053-a166-1d4ab4dab40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure CSV files exist\n",
    "training_csv = \"training_results.csv\"\n",
    "if not os.path.exists(training_csv) or os.path.getsize(training_csv) == 0:\n",
    "    pd.DataFrame(columns=[\"model_name\", \"mode\", \"epoch1\", \"epoch2\"]).to_csv(training_csv, index=False)\n",
    "\n",
    "csv_file = \"test_cer_results.csv\"\n",
    "if not os.path.exists(csv_file) or os.path.getsize(csv_file) == 0:\n",
    "    pd.DataFrame(columns=[\"model_name\", \"test_cer\"]).to_csv(csv_file, index=False)\n",
    "\n",
    "def log_test_cer(model_name, cer_value):\n",
    "    \"\"\"\n",
    "    Logs or updates the test CER for a given model, rounding values to 4 decimals.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(csv_file)\n",
    "    \n",
    "    # Round the new CER value to 4 decimals\n",
    "    cer_rounded = round(cer_value, 4)\n",
    "    \n",
    "    if model_name in df['model_name'].values:\n",
    "        # Update existing row\n",
    "        df.loc[df['model_name'] == model_name, 'test_cer'] = cer_rounded\n",
    "    else:\n",
    "        # Add new row\n",
    "        new_row = pd.DataFrame({\n",
    "            \"model_name\": [model_name],\n",
    "            \"test_cer\":   [cer_rounded]\n",
    "        })\n",
    "        df = pd.concat([df, new_row], ignore_index=True)\n",
    "    \n",
    "    # Ensure all stored values are rounded to 4 decimals\n",
    "    df['test_cer'] = df['test_cer'].round(4)\n",
    "    \n",
    "    # Save back to CSV\n",
    "    df.to_csv(csv_file, index=False)\n",
    "    print(f\"Logged {model_name}: {cer_rounded:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "02c43587-227f-46a6-aebf-591f6c5c1a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training_pipeline(encoder_class,encoder_kwargs, model_name,vocab_size,encoder_lr, decoder_lr,train_loader,\n",
    "                          val_loader,test_loader,char_to_idx,idx_to_char,max_label_length,max_label_length_test,test_data,\n",
    "                          device, num_epochs=100):\n",
    "    #build encoder & grab its dimension\n",
    "    encoder = encoder_class(**encoder_kwargs).to(device)\n",
    "    # if encoder_dim is None (e.g. Swin), prime it with a dummy batch\n",
    "    if encoder.encoder_dim is None:\n",
    "        with torch.no_grad():\n",
    "            dummy = torch.randn(1, 3, 224, 224, device=device)\n",
    "            _ = encoder(dummy)\n",
    "    enc_dim = encoder.encoder_dim\n",
    "\n",
    "    #build decoder \n",
    "    decoder = DecoderRNN(\n",
    "        attention_dim=256,\n",
    "        embed_dim=256,\n",
    "        decoder_dim=512,\n",
    "        vocab_size=vocab_size,\n",
    "        encoder_dim=enc_dim,\n",
    "        teacher_forcing_ratio=0.5\n",
    "    ).to(device)\n",
    "\n",
    "    # loss, optimizers, trainer\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=char_to_idx['<PAD>'])\n",
    "    encoder_optimizer = optim.Adam(encoder.parameters(), lr=encoder_lr)\n",
    "    decoder_optimizer = optim.Adam(decoder.parameters(), lr=decoder_lr)\n",
    "\n",
    "    # trainer = ImageCaptioningTrainer(\n",
    "    #     encoder=encoder,\n",
    "    #     decoder=decoder,\n",
    "    #     criterion=criterion,\n",
    "    #     encoder_optimizer=encoder_optimizer,\n",
    "    #     decoder_optimizer=decoder_optimizer,\n",
    "    #     train_loader=train_loader,\n",
    "    #     val_loader=val_loader,\n",
    "    #     device=device,\n",
    "    #     char_to_idx=char_to_idx,\n",
    "    #     idx_to_char=idx_to_char,\n",
    "    #     max_label_length=max_label_length,\n",
    "    #     model_name=model_name)\n",
    "    trainer = ImageCaptioningTrainer(\n",
    "        encoder=encoder,\n",
    "        decoder=decoder,\n",
    "        criterion=criterion,\n",
    "        encoder_optimizer=encoder_optimizer,\n",
    "        decoder_optimizer=decoder_optimizer,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        test_loader=test_loader,  \n",
    "        test_data=test_data,      \n",
    "        max_label_length_test=max_label_length_test,  \n",
    "        device=device,\n",
    "        char_to_idx=char_to_idx,\n",
    "        idx_to_char=idx_to_char,\n",
    "        max_label_length=max_label_length,\n",
    "        model_name=model_name\n",
    "    )\n",
    "\n",
    "    trainer.fit(num_epochs)\n",
    "\n",
    "    encoder.eval()\n",
    "    decoder.teacher_forcing_ratio = 0.0\n",
    "\n",
    "    results = inference(\n",
    "        encoder=encoder,\n",
    "        decoder=decoder,\n",
    "        data_loader=test_loader,\n",
    "        device=device,\n",
    "        char_to_idx=char_to_idx,\n",
    "        idx_to_char=idx_to_char,\n",
    "        max_seq_length=max_label_length_test,\n",
    "        test_data=test_data\n",
    "    )\n",
    "\n",
    "    cer = calculate_global_cer(results)\n",
    "    print(f\"{model_name} — Test CER: {cer:.4f}\")\n",
    "    print_top_worst_samples(results, n=5)\n",
    "    log_test_cer(model_name, cer)\n",
    "\n",
    "    del encoder, decoder, trainer\n",
    "    torch.cuda.empty_cache()\n",
    "    print(f\"Memory cleared for {model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0ca61a07-f184-4f3b-a11c-888c5e5334ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/100\n",
      "[1/100] Train Loss: 2.2638, Train CER: 0.8490 | Val Loss: 1.9259, Val CER: 0.7584 | Test CER: 0.7050\n",
      "\n",
      "Epoch 2/100\n",
      "[2/100] Train Loss: 1.8359, Train CER: 0.6072 | Val Loss: 1.7182, Val CER: 0.6132 | Test CER: 0.6827\n",
      "\n",
      "Epoch 3/100\n",
      "[3/100] Train Loss: 1.6053, Train CER: 0.5166 | Val Loss: 1.4968, Val CER: 0.4495 | Test CER: 0.6062\n",
      "\n",
      "Epoch 4/100\n",
      "[4/100] Train Loss: 1.4108, Train CER: 0.4318 | Val Loss: 1.3139, Val CER: 0.3896 | Test CER: 0.5522\n",
      "\n",
      "Epoch 5/100\n",
      "[5/100] Train Loss: 1.2336, Train CER: 0.3640 | Val Loss: 1.1659, Val CER: 0.3485 | Test CER: 0.5219\n",
      "\n",
      "Epoch 6/100\n",
      "[6/100] Train Loss: 1.0507, Train CER: 0.3019 | Val Loss: 1.0026, Val CER: 0.2822 | Test CER: 0.4848\n",
      "\n",
      "Epoch 7/100\n",
      "[7/100] Train Loss: 0.8894, Train CER: 0.2499 | Val Loss: 0.8991, Val CER: 0.2620 | Test CER: 0.4548\n",
      "\n",
      "Epoch 8/100\n",
      "[8/100] Train Loss: 0.7673, Train CER: 0.2122 | Val Loss: 0.7950, Val CER: 0.2209 | Test CER: 0.4428\n",
      "\n",
      "Epoch 9/100\n",
      "[9/100] Train Loss: 0.6517, Train CER: 0.1789 | Val Loss: 0.7663, Val CER: 0.2273 | Test CER: 0.4090\n",
      "\n",
      "Epoch 10/100\n",
      "[10/100] Train Loss: 0.5595, Train CER: 0.1515 | Val Loss: 0.6915, Val CER: 0.1898 | Test CER: 0.3856\n",
      "\n",
      "Epoch 11/100\n",
      "[11/100] Train Loss: 0.4760, Train CER: 0.1280 | Val Loss: 0.6604, Val CER: 0.1869 | Test CER: 0.3735\n",
      "\n",
      "Epoch 12/100\n",
      "[12/100] Train Loss: 0.4262, Train CER: 0.1147 | Val Loss: 0.6572, Val CER: 0.1804 | Test CER: 0.3727\n",
      "\n",
      "Epoch 13/100\n",
      "[13/100] Train Loss: 0.3711, Train CER: 0.0996 | Val Loss: 0.6619, Val CER: 0.1889 | Test CER: 0.3443\n",
      "\n",
      "Epoch 14/100\n",
      "[14/100] Train Loss: 0.3455, Train CER: 0.0933 | Val Loss: 0.6162, Val CER: 0.1607 | Test CER: 0.3400\n",
      "\n",
      "Epoch 15/100\n",
      "[15/100] Train Loss: 0.2979, Train CER: 0.0794 | Val Loss: 0.5745, Val CER: 0.1547 | Test CER: 0.3352\n",
      "\n",
      "Epoch 16/100\n",
      "[16/100] Train Loss: 0.2611, Train CER: 0.0693 | Val Loss: 0.5676, Val CER: 0.1476 | Test CER: 0.3204\n",
      "\n",
      "Epoch 17/100\n",
      "[17/100] Train Loss: 0.2389, Train CER: 0.0638 | Val Loss: 0.6006, Val CER: 0.1532 | Test CER: 0.3180\n",
      "\n",
      "Epoch 18/100\n",
      "[18/100] Train Loss: 0.2229, Train CER: 0.0589 | Val Loss: 0.6175, Val CER: 0.1520 | Test CER: 0.3193\n",
      "Test CER did not improve. Early stop counter: 1/5\n",
      "\n",
      "Epoch 19/100\n",
      "[19/100] Train Loss: 0.2040, Train CER: 0.0545 | Val Loss: 0.6111, Val CER: 0.1525 | Test CER: 0.3068\n",
      "\n",
      "Epoch 20/100\n",
      "[20/100] Train Loss: 0.1963, Train CER: 0.0531 | Val Loss: 0.6088, Val CER: 0.1439 | Test CER: 0.3088\n",
      "Test CER did not improve. Early stop counter: 1/5\n",
      "\n",
      "Epoch 21/100\n",
      "[21/100] Train Loss: 0.1689, Train CER: 0.0452 | Val Loss: 0.6220, Val CER: 0.1418 | Test CER: 0.3025\n",
      "\n",
      "Epoch 22/100\n",
      "[22/100] Train Loss: 0.1645, Train CER: 0.0435 | Val Loss: 0.5988, Val CER: 0.1358 | Test CER: 0.2989\n",
      "\n",
      "Epoch 23/100\n",
      "[23/100] Train Loss: 0.1533, Train CER: 0.0403 | Val Loss: 0.5941, Val CER: 0.1337 | Test CER: 0.2986\n",
      "\n",
      "Epoch 24/100\n",
      "[24/100] Train Loss: 0.1397, Train CER: 0.0382 | Val Loss: 0.6024, Val CER: 0.1431 | Test CER: 0.2930\n",
      "\n",
      "Epoch 25/100\n",
      "[25/100] Train Loss: 0.1260, Train CER: 0.0341 | Val Loss: 0.5999, Val CER: 0.1335 | Test CER: 0.2961\n",
      "Test CER did not improve. Early stop counter: 1/5\n",
      "\n",
      "Epoch 26/100\n",
      "[26/100] Train Loss: 0.1260, Train CER: 0.0336 | Val Loss: 0.5823, Val CER: 0.1291 | Test CER: 0.2947\n",
      "Test CER did not improve. Early stop counter: 2/5\n",
      "\n",
      "Epoch 27/100\n",
      "[27/100] Train Loss: 0.1250, Train CER: 0.0329 | Val Loss: 0.5434, Val CER: 0.1206 | Test CER: 0.2848\n",
      "\n",
      "Epoch 28/100\n",
      "[28/100] Train Loss: 0.1174, Train CER: 0.0313 | Val Loss: 0.5927, Val CER: 0.1344 | Test CER: 0.2820\n",
      "\n",
      "Epoch 29/100\n",
      "[29/100] Train Loss: 0.1070, Train CER: 0.0286 | Val Loss: 0.5910, Val CER: 0.1317 | Test CER: 0.2811\n",
      "\n",
      "Epoch 30/100\n",
      "[30/100] Train Loss: 0.1041, Train CER: 0.0275 | Val Loss: 0.5760, Val CER: 0.1246 | Test CER: 0.2818\n",
      "Test CER did not improve. Early stop counter: 1/5\n",
      "\n",
      "Epoch 31/100\n",
      "[31/100] Train Loss: 0.0981, Train CER: 0.0266 | Val Loss: 0.5957, Val CER: 0.1265 | Test CER: 0.2746\n",
      "\n",
      "Epoch 32/100\n",
      "[32/100] Train Loss: 0.0872, Train CER: 0.0228 | Val Loss: 0.5939, Val CER: 0.1251 | Test CER: 0.2767\n",
      "Test CER did not improve. Early stop counter: 1/5\n",
      "\n",
      "Epoch 33/100\n",
      "[33/100] Train Loss: 0.0879, Train CER: 0.0238 | Val Loss: 0.5916, Val CER: 0.1226 | Test CER: 0.2715\n",
      "\n",
      "Epoch 34/100\n",
      "[34/100] Train Loss: 0.0820, Train CER: 0.0219 | Val Loss: 0.5897, Val CER: 0.1253 | Test CER: 0.2780\n",
      "Test CER did not improve. Early stop counter: 1/5\n",
      "\n",
      "Epoch 35/100\n",
      "[35/100] Train Loss: 0.0792, Train CER: 0.0213 | Val Loss: 0.6033, Val CER: 0.1242 | Test CER: 0.2725\n",
      "Test CER did not improve. Early stop counter: 2/5\n",
      "\n",
      "Epoch 36/100\n",
      "[36/100] Train Loss: 0.0742, Train CER: 0.0195 | Val Loss: 0.5970, Val CER: 0.1255 | Test CER: 0.2743\n",
      "Test CER did not improve. Early stop counter: 3/5\n",
      "\n",
      "Epoch 37/100\n",
      "[37/100] Train Loss: 0.0739, Train CER: 0.0201 | Val Loss: 0.6113, Val CER: 0.1265 | Test CER: 0.2670\n",
      "\n",
      "Epoch 38/100\n",
      "[38/100] Train Loss: 0.0727, Train CER: 0.0201 | Val Loss: 0.6415, Val CER: 0.1312 | Test CER: 0.2823\n",
      "Test CER did not improve. Early stop counter: 1/5\n",
      "\n",
      "Epoch 39/100\n",
      "[39/100] Train Loss: 0.0695, Train CER: 0.0183 | Val Loss: 0.6295, Val CER: 0.1238 | Test CER: 0.2721\n",
      "Test CER did not improve. Early stop counter: 2/5\n",
      "\n",
      "Epoch 40/100\n",
      "[40/100] Train Loss: 0.0637, Train CER: 0.0175 | Val Loss: 0.6162, Val CER: 0.1231 | Test CER: 0.2624\n",
      "\n",
      "Epoch 41/100\n",
      "[41/100] Train Loss: 0.0620, Train CER: 0.0160 | Val Loss: 0.6476, Val CER: 0.1283 | Test CER: 0.2804\n",
      "Test CER did not improve. Early stop counter: 1/5\n",
      "\n",
      "Epoch 42/100\n",
      "[42/100] Train Loss: 0.0714, Train CER: 0.0190 | Val Loss: 0.6456, Val CER: 0.1247 | Test CER: 0.2654\n",
      "Test CER did not improve. Early stop counter: 2/5\n",
      "\n",
      "Epoch 43/100\n",
      "[43/100] Train Loss: 0.0597, Train CER: 0.0161 | Val Loss: 0.6285, Val CER: 0.1227 | Test CER: 0.2649\n",
      "Test CER did not improve. Early stop counter: 3/5\n",
      "\n",
      "Epoch 44/100\n",
      "[44/100] Train Loss: 0.0533, Train CER: 0.0143 | Val Loss: 0.6434, Val CER: 0.1225 | Test CER: 0.2698\n",
      "Test CER did not improve. Early stop counter: 4/5\n",
      "\n",
      "Epoch 45/100\n",
      "[45/100] Train Loss: 0.0492, Train CER: 0.0131 | Val Loss: 0.6454, Val CER: 0.1261 | Test CER: 0.2585\n",
      "\n",
      "Epoch 46/100\n",
      "[46/100] Train Loss: 0.0573, Train CER: 0.0152 | Val Loss: 0.6842, Val CER: 0.1337 | Test CER: 0.2639\n",
      "Test CER did not improve. Early stop counter: 1/5\n",
      "\n",
      "Epoch 47/100\n",
      "[47/100] Train Loss: 0.0499, Train CER: 0.0130 | Val Loss: 0.6138, Val CER: 0.1231 | Test CER: 0.2618\n",
      "Test CER did not improve. Early stop counter: 2/5\n",
      "\n",
      "Epoch 48/100\n",
      "[48/100] Train Loss: 0.0508, Train CER: 0.0139 | Val Loss: 0.6347, Val CER: 0.1244 | Test CER: 0.2652\n",
      "Test CER did not improve. Early stop counter: 3/5\n",
      "\n",
      "Epoch 49/100\n",
      "[49/100] Train Loss: 0.0538, Train CER: 0.0141 | Val Loss: 0.6229, Val CER: 0.1225 | Test CER: 0.2741\n",
      "Test CER did not improve. Early stop counter: 4/5\n",
      "\n",
      "Epoch 50/100\n",
      "[50/100] Train Loss: 0.0578, Train CER: 0.0151 | Val Loss: 0.6130, Val CER: 0.1217 | Test CER: 0.2583\n",
      "\n",
      "Epoch 51/100\n",
      "[51/100] Train Loss: 0.0482, Train CER: 0.0116 | Val Loss: 0.6176, Val CER: 0.1189 | Test CER: 0.2546\n",
      "\n",
      "Epoch 52/100\n",
      "[52/100] Train Loss: 0.0429, Train CER: 0.0113 | Val Loss: 0.6128, Val CER: 0.1180 | Test CER: 0.2543\n",
      "\n",
      "Epoch 53/100\n",
      "[53/100] Train Loss: 0.0453, Train CER: 0.0118 | Val Loss: 0.5942, Val CER: 0.1107 | Test CER: 0.2585\n",
      "Test CER did not improve. Early stop counter: 1/5\n",
      "\n",
      "Epoch 54/100\n",
      "[54/100] Train Loss: 0.0504, Train CER: 0.0132 | Val Loss: 0.6157, Val CER: 0.1136 | Test CER: 0.2579\n",
      "Test CER did not improve. Early stop counter: 2/5\n",
      "\n",
      "Epoch 55/100\n",
      "[55/100] Train Loss: 0.0383, Train CER: 0.0102 | Val Loss: 0.6800, Val CER: 0.1171 | Test CER: 0.2532\n",
      "\n",
      "Epoch 56/100\n",
      "[56/100] Train Loss: 0.0419, Train CER: 0.0107 | Val Loss: 0.6225, Val CER: 0.1151 | Test CER: 0.2563\n",
      "Test CER did not improve. Early stop counter: 1/5\n",
      "\n",
      "Epoch 57/100\n",
      "[57/100] Train Loss: 0.0433, Train CER: 0.0112 | Val Loss: 0.6193, Val CER: 0.1140 | Test CER: 0.2539\n",
      "Test CER did not improve. Early stop counter: 2/5\n",
      "\n",
      "Epoch 58/100\n",
      "[58/100] Train Loss: 0.0430, Train CER: 0.0109 | Val Loss: 0.6659, Val CER: 0.1214 | Test CER: 0.2493\n",
      "\n",
      "Epoch 59/100\n",
      "[59/100] Train Loss: 0.0413, Train CER: 0.0113 | Val Loss: 0.5964, Val CER: 0.1098 | Test CER: 0.2601\n",
      "Test CER did not improve. Early stop counter: 1/5\n",
      "\n",
      "Epoch 60/100\n",
      "[60/100] Train Loss: 0.0326, Train CER: 0.0089 | Val Loss: 0.6163, Val CER: 0.1164 | Test CER: 0.2426\n",
      "\n",
      "Epoch 61/100\n",
      "[61/100] Train Loss: 0.0397, Train CER: 0.0099 | Val Loss: 0.6054, Val CER: 0.1155 | Test CER: 0.2532\n",
      "Test CER did not improve. Early stop counter: 1/5\n",
      "\n",
      "Epoch 62/100\n",
      "[62/100] Train Loss: 0.0370, Train CER: 0.0095 | Val Loss: 0.5998, Val CER: 0.1139 | Test CER: 0.2627\n",
      "Test CER did not improve. Early stop counter: 2/5\n",
      "\n",
      "Epoch 63/100\n",
      "[63/100] Train Loss: 0.0367, Train CER: 0.0098 | Val Loss: 0.6454, Val CER: 0.1158 | Test CER: 0.2529\n",
      "Test CER did not improve. Early stop counter: 3/5\n",
      "\n",
      "Epoch 64/100\n",
      "[64/100] Train Loss: 0.0418, Train CER: 0.0110 | Val Loss: 0.6278, Val CER: 0.1157 | Test CER: 0.2454\n",
      "Test CER did not improve. Early stop counter: 4/5\n",
      "\n",
      "Epoch 65/100\n",
      "[65/100] Train Loss: 0.0355, Train CER: 0.0088 | Val Loss: 0.5719, Val CER: 0.1062 | Test CER: 0.2513\n",
      "Test CER did not improve. Early stop counter: 5/5\n",
      "Early stopping triggered.\n",
      "\n",
      "Training completed in 9h 45m.\n",
      "\n",
      "Results have been written to: training_results.csv\n",
      "swin_small_encoder — Test CER: 0.2513\n",
      "\n",
      "=== Top 5 Worst Samples by CER ===\n",
      "1) Image: /projects/qb36/lontar_project/data/balinese_word_test/test262.png\n",
      "   CER: 11.0000\n",
      "   Predicted       : bnrok-agung\n",
      "   Ground Truth    : .\n",
      "\n",
      "2) Image: /projects/qb36/lontar_project/data/balinese_word_test/test3029.png\n",
      "   CER: 11.0000\n",
      "   Predicted       : tarik-agung\n",
      "   Ground Truth    : .\n",
      "\n",
      "3) Image: /projects/qb36/lontar_project/data/balinese_word_test/test4934.png\n",
      "   CER: 11.0000\n",
      "   Predicted       : bnrik-agung\n",
      "   Ground Truth    : .\n",
      "\n",
      "4) Image: /projects/qb36/lontar_project/data/balinese_word_test/test9874.png\n",
      "   CER: 11.0000\n",
      "   Predicted       : bnron--gung\n",
      "   Ground Truth    : .\n",
      "\n",
      "5) Image: /projects/qb36/lontar_project/data/balinese_word_test/test1330.png\n",
      "   CER: 10.0000\n",
      "   Predicted       : angledaaya\n",
      "   Ground Truth    : .\n",
      "\n",
      "Logged swin_small_encoder: 0.2513\n",
      "Memory cleared for swin_small_encoder\n"
     ]
    }
   ],
   "source": [
    "run_training_pipeline(\n",
    "    encoder_class=SwinEncoder,\n",
    "    encoder_kwargs={\"model_name\":\"swin_small_patch4_window7_224\",\"pretrained\":True},\n",
    "    model_name=\"swin_small_encoder\",\n",
    "    vocab_size=vocab_size,\n",
    "    encoder_lr=1e-4,\n",
    "    decoder_lr=4e-4,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    test_loader=test_loader,\n",
    "    char_to_idx=char_to_idx,\n",
    "    idx_to_char=idx_to_char,\n",
    "    max_label_length=max_label_length,\n",
    "    max_label_length_test=max_label_length_test,\n",
    "    test_data=test_data,\n",
    "    device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "28c698ba-ff1f-4667-a7db-4bf51eae88ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/100\n",
      "[1/100] Train Loss: 2.3235, Train CER: 0.8929 | Val Loss: 1.9760, Val CER: 0.7552 | Test CER: 0.7241\n",
      "\n",
      "Epoch 2/100\n",
      "[2/100] Train Loss: 1.9424, Train CER: 0.6549 | Val Loss: 1.8164, Val CER: 0.5925 | Test CER: 0.6970\n",
      "\n",
      "Epoch 3/100\n",
      "[3/100] Train Loss: 1.8247, Train CER: 0.5985 | Val Loss: 1.8122, Val CER: 0.5942 | Test CER: 0.7761\n",
      "Test CER did not improve. Early stop counter: 1/5\n",
      "\n",
      "Epoch 4/100\n",
      "[4/100] Train Loss: 1.7590, Train CER: 0.5757 | Val Loss: 1.6909, Val CER: 0.5425 | Test CER: 0.6623\n",
      "\n",
      "Epoch 5/100\n",
      "[5/100] Train Loss: 1.6667, Train CER: 0.5271 | Val Loss: 1.6065, Val CER: 0.5324 | Test CER: 0.6122\n",
      "\n",
      "Epoch 6/100\n",
      "[6/100] Train Loss: 1.5677, Train CER: 0.4812 | Val Loss: 1.5207, Val CER: 0.4538 | Test CER: 0.6055\n",
      "\n",
      "Epoch 7/100\n",
      "[7/100] Train Loss: 1.4562, Train CER: 0.4368 | Val Loss: 1.3906, Val CER: 0.4168 | Test CER: 0.5496\n",
      "\n",
      "Epoch 8/100\n",
      "[8/100] Train Loss: 1.3406, Train CER: 0.3905 | Val Loss: 1.2981, Val CER: 0.3832 | Test CER: 0.5293\n",
      "\n",
      "Epoch 9/100\n",
      "[9/100] Train Loss: 1.2120, Train CER: 0.3444 | Val Loss: 1.1444, Val CER: 0.3197 | Test CER: 0.5062\n",
      "\n",
      "Epoch 10/100\n",
      "[10/100] Train Loss: 1.1019, Train CER: 0.3059 | Val Loss: 1.1043, Val CER: 0.3141 | Test CER: 0.4782\n",
      "\n",
      "Epoch 11/100\n",
      "[11/100] Train Loss: 1.0129, Train CER: 0.2766 | Val Loss: 0.9983, Val CER: 0.2748 | Test CER: 0.4667\n",
      "\n",
      "Epoch 12/100\n",
      "[12/100] Train Loss: 0.8984, Train CER: 0.2411 | Val Loss: 0.9429, Val CER: 0.2581 | Test CER: 0.4305\n",
      "\n",
      "Epoch 13/100\n",
      "[13/100] Train Loss: 0.8075, Train CER: 0.2141 | Val Loss: 0.8766, Val CER: 0.2410 | Test CER: 0.4149\n",
      "\n",
      "Epoch 14/100\n",
      "[14/100] Train Loss: 0.7038, Train CER: 0.1838 | Val Loss: 0.7826, Val CER: 0.2112 | Test CER: 0.4013\n",
      "\n",
      "Epoch 15/100\n",
      "[15/100] Train Loss: 0.6358, Train CER: 0.1655 | Val Loss: 0.7875, Val CER: 0.2135 | Test CER: 0.3897\n",
      "\n",
      "Epoch 16/100\n",
      "[16/100] Train Loss: 0.5378, Train CER: 0.1392 | Val Loss: 0.7217, Val CER: 0.1906 | Test CER: 0.3709\n",
      "\n",
      "Epoch 17/100\n",
      "[17/100] Train Loss: 0.4801, Train CER: 0.1234 | Val Loss: 0.6921, Val CER: 0.1875 | Test CER: 0.3594\n",
      "\n",
      "Epoch 18/100\n",
      "[18/100] Train Loss: 0.4309, Train CER: 0.1100 | Val Loss: 0.6783, Val CER: 0.1767 | Test CER: 0.3600\n",
      "Test CER did not improve. Early stop counter: 1/5\n",
      "\n",
      "Epoch 19/100\n",
      "[19/100] Train Loss: 0.3809, Train CER: 0.0976 | Val Loss: 0.6497, Val CER: 0.1647 | Test CER: 0.3430\n",
      "\n",
      "Epoch 20/100\n",
      "[20/100] Train Loss: 0.3274, Train CER: 0.0836 | Val Loss: 0.6456, Val CER: 0.1625 | Test CER: 0.3358\n",
      "\n",
      "Epoch 21/100\n",
      "[21/100] Train Loss: 0.2974, Train CER: 0.0749 | Val Loss: 0.6453, Val CER: 0.1599 | Test CER: 0.3325\n",
      "\n",
      "Epoch 22/100\n",
      "[22/100] Train Loss: 0.2648, Train CER: 0.0678 | Val Loss: 0.6348, Val CER: 0.1564 | Test CER: 0.3268\n",
      "\n",
      "Epoch 23/100\n",
      "[23/100] Train Loss: 0.2427, Train CER: 0.0626 | Val Loss: 0.6600, Val CER: 0.1582 | Test CER: 0.3295\n",
      "Test CER did not improve. Early stop counter: 1/5\n",
      "\n",
      "Epoch 24/100\n",
      "[24/100] Train Loss: 0.2247, Train CER: 0.0580 | Val Loss: 0.6226, Val CER: 0.1494 | Test CER: 0.3154\n",
      "\n",
      "Epoch 25/100\n",
      "[25/100] Train Loss: 0.2094, Train CER: 0.0545 | Val Loss: 0.6378, Val CER: 0.1522 | Test CER: 0.3143\n",
      "\n",
      "Epoch 26/100\n",
      "[26/100] Train Loss: 0.1936, Train CER: 0.0499 | Val Loss: 0.6309, Val CER: 0.1467 | Test CER: 0.3075\n",
      "\n",
      "Epoch 27/100\n",
      "[27/100] Train Loss: 0.1751, Train CER: 0.0459 | Val Loss: 0.5997, Val CER: 0.1368 | Test CER: 0.3058\n",
      "\n",
      "Epoch 28/100\n",
      "[28/100] Train Loss: 0.2139, Train CER: 0.0542 | Val Loss: 0.6466, Val CER: 0.1523 | Test CER: 0.3385\n",
      "Test CER did not improve. Early stop counter: 1/5\n",
      "\n",
      "Epoch 29/100\n",
      "[29/100] Train Loss: 0.1483, Train CER: 0.0385 | Val Loss: 0.5761, Val CER: 0.1333 | Test CER: 0.2957\n",
      "\n",
      "Epoch 30/100\n",
      "[30/100] Train Loss: 0.1354, Train CER: 0.0348 | Val Loss: 0.6062, Val CER: 0.1301 | Test CER: 0.2982\n",
      "Test CER did not improve. Early stop counter: 1/5\n",
      "\n",
      "Epoch 31/100\n",
      "[31/100] Train Loss: 0.1263, Train CER: 0.0331 | Val Loss: 0.5953, Val CER: 0.1358 | Test CER: 0.2943\n",
      "\n",
      "Epoch 32/100\n",
      "[32/100] Train Loss: 0.1648, Train CER: 0.0425 | Val Loss: 0.6004, Val CER: 0.1386 | Test CER: 0.3048\n",
      "Test CER did not improve. Early stop counter: 1/5\n",
      "\n",
      "Epoch 33/100\n",
      "[33/100] Train Loss: 0.1277, Train CER: 0.0334 | Val Loss: 0.5783, Val CER: 0.1264 | Test CER: 0.2879\n",
      "\n",
      "Epoch 34/100\n",
      "[34/100] Train Loss: 0.1114, Train CER: 0.0293 | Val Loss: 0.6239, Val CER: 0.1311 | Test CER: 0.2917\n",
      "Test CER did not improve. Early stop counter: 1/5\n",
      "\n",
      "Epoch 35/100\n",
      "[35/100] Train Loss: 0.1017, Train CER: 0.0268 | Val Loss: 0.5837, Val CER: 0.1237 | Test CER: 0.2857\n",
      "\n",
      "Epoch 36/100\n",
      "[36/100] Train Loss: 0.0995, Train CER: 0.0256 | Val Loss: 0.6062, Val CER: 0.1289 | Test CER: 0.2925\n",
      "Test CER did not improve. Early stop counter: 1/5\n",
      "\n",
      "Epoch 37/100\n",
      "[37/100] Train Loss: 0.0943, Train CER: 0.0252 | Val Loss: 0.5937, Val CER: 0.1250 | Test CER: 0.2854\n",
      "\n",
      "Epoch 38/100\n",
      "[38/100] Train Loss: 0.0932, Train CER: 0.0247 | Val Loss: 0.6295, Val CER: 0.1283 | Test CER: 0.2896\n",
      "Test CER did not improve. Early stop counter: 1/5\n",
      "\n",
      "Epoch 39/100\n",
      "[39/100] Train Loss: 0.0864, Train CER: 0.0226 | Val Loss: 0.6032, Val CER: 0.1224 | Test CER: 0.2876\n",
      "Test CER did not improve. Early stop counter: 2/5\n",
      "\n",
      "Epoch 40/100\n",
      "[40/100] Train Loss: 0.0809, Train CER: 0.0212 | Val Loss: 0.5910, Val CER: 0.1186 | Test CER: 0.2876\n",
      "Test CER did not improve. Early stop counter: 3/5\n",
      "\n",
      "Epoch 41/100\n",
      "[41/100] Train Loss: 0.0868, Train CER: 0.0231 | Val Loss: 0.6557, Val CER: 0.1289 | Test CER: 0.2830\n",
      "\n",
      "Epoch 42/100\n",
      "[42/100] Train Loss: 0.0793, Train CER: 0.0216 | Val Loss: 0.6056, Val CER: 0.1238 | Test CER: 0.2833\n",
      "Test CER did not improve. Early stop counter: 1/5\n",
      "\n",
      "Epoch 43/100\n",
      "[43/100] Train Loss: 0.0757, Train CER: 0.0196 | Val Loss: 0.6435, Val CER: 0.1295 | Test CER: 0.2815\n",
      "\n",
      "Epoch 44/100\n",
      "[44/100] Train Loss: 0.0768, Train CER: 0.0199 | Val Loss: 0.6333, Val CER: 0.1282 | Test CER: 0.2891\n",
      "Test CER did not improve. Early stop counter: 1/5\n",
      "\n",
      "Epoch 45/100\n",
      "[45/100] Train Loss: 0.0731, Train CER: 0.0197 | Val Loss: 0.6192, Val CER: 0.1200 | Test CER: 0.2769\n",
      "\n",
      "Epoch 46/100\n",
      "[46/100] Train Loss: 0.0642, Train CER: 0.0165 | Val Loss: 0.6265, Val CER: 0.1290 | Test CER: 0.2780\n",
      "Test CER did not improve. Early stop counter: 1/5\n",
      "\n",
      "Epoch 47/100\n",
      "[47/100] Train Loss: 0.0740, Train CER: 0.0195 | Val Loss: 0.6286, Val CER: 0.1222 | Test CER: 0.2719\n",
      "\n",
      "Epoch 48/100\n",
      "[48/100] Train Loss: 0.0680, Train CER: 0.0178 | Val Loss: 0.5974, Val CER: 0.1203 | Test CER: 0.2757\n",
      "Test CER did not improve. Early stop counter: 1/5\n",
      "\n",
      "Epoch 49/100\n",
      "[49/100] Train Loss: 0.0638, Train CER: 0.0164 | Val Loss: 0.6437, Val CER: 0.1289 | Test CER: 0.2751\n",
      "Test CER did not improve. Early stop counter: 2/5\n",
      "\n",
      "Epoch 50/100\n",
      "[50/100] Train Loss: 0.0591, Train CER: 0.0155 | Val Loss: 0.6115, Val CER: 0.1161 | Test CER: 0.2723\n",
      "Test CER did not improve. Early stop counter: 3/5\n",
      "\n",
      "Epoch 51/100\n",
      "[51/100] Train Loss: 0.0580, Train CER: 0.0153 | Val Loss: 0.6342, Val CER: 0.1226 | Test CER: 0.2691\n",
      "\n",
      "Epoch 52/100\n",
      "[52/100] Train Loss: 0.0530, Train CER: 0.0142 | Val Loss: 0.6278, Val CER: 0.1201 | Test CER: 0.2724\n",
      "Test CER did not improve. Early stop counter: 1/5\n",
      "\n",
      "Epoch 53/100\n",
      "[53/100] Train Loss: 0.0615, Train CER: 0.0161 | Val Loss: 0.6407, Val CER: 0.1220 | Test CER: 0.2742\n",
      "Test CER did not improve. Early stop counter: 2/5\n",
      "\n",
      "Epoch 54/100\n",
      "[54/100] Train Loss: 0.0518, Train CER: 0.0129 | Val Loss: 0.5944, Val CER: 0.1136 | Test CER: 0.2683\n",
      "\n",
      "Epoch 55/100\n",
      "[55/100] Train Loss: 0.0592, Train CER: 0.0152 | Val Loss: 0.6129, Val CER: 0.1171 | Test CER: 0.2713\n",
      "Test CER did not improve. Early stop counter: 1/5\n",
      "\n",
      "Epoch 56/100\n",
      "[56/100] Train Loss: 0.0545, Train CER: 0.0143 | Val Loss: 0.6035, Val CER: 0.1190 | Test CER: 0.2798\n",
      "Test CER did not improve. Early stop counter: 2/5\n",
      "\n",
      "Epoch 57/100\n",
      "[57/100] Train Loss: 0.0519, Train CER: 0.0133 | Val Loss: 0.6137, Val CER: 0.1157 | Test CER: 0.2740\n",
      "Test CER did not improve. Early stop counter: 3/5\n",
      "\n",
      "Epoch 58/100\n",
      "[58/100] Train Loss: 0.0411, Train CER: 0.0109 | Val Loss: 0.6028, Val CER: 0.1113 | Test CER: 0.2785\n",
      "Test CER did not improve. Early stop counter: 4/5\n",
      "\n",
      "Epoch 59/100\n",
      "[59/100] Train Loss: 0.0518, Train CER: 0.0135 | Val Loss: 0.6211, Val CER: 0.1200 | Test CER: 0.2728\n",
      "Test CER did not improve. Early stop counter: 5/5\n",
      "Early stopping triggered.\n",
      "\n",
      "Training completed in 9h 52m.\n",
      "\n",
      "Results have been written to: training_results.csv\n",
      "swin_base_encoder — Test CER: 0.2728\n",
      "\n",
      "=== Top 5 Worst Samples by CER ===\n",
      "1) Image: /projects/qb36/lontar_project/data/balinese_word_test/test1330.png\n",
      "   CER: 8.0000\n",
      "   Predicted       : anycaraa\n",
      "   Ground Truth    : .\n",
      "\n",
      "2) Image: /projects/qb36/lontar_project/data/balinese_word_test/test4934.png\n",
      "   CER: 8.0000\n",
      "   Predicted       : garingan\n",
      "   Ground Truth    : .\n",
      "\n",
      "3) Image: /projects/qb36/lontar_project/data/balinese_word_test/test9874.png\n",
      "   CER: 8.0000\n",
      "   Predicted       : gangkeng\n",
      "   Ground Truth    : .\n",
      "\n",
      "4) Image: /projects/qb36/lontar_project/data/balinese_word_test/test10070.png\n",
      "   CER: 8.0000\n",
      "   Predicted       : ngalanda\n",
      "   Ground Truth    : .\n",
      "\n",
      "5) Image: /projects/qb36/lontar_project/data/balinese_word_test/test3029.png\n",
      "   CER: 6.0000\n",
      "   Predicted       : 11ngng\n",
      "   Ground Truth    : .\n",
      "\n",
      "Logged swin_base_encoder: 0.2728\n",
      "Memory cleared for swin_base_encoder\n"
     ]
    }
   ],
   "source": [
    "run_training_pipeline(\n",
    "    encoder_class = SwinEncoder,\n",
    "    encoder_kwargs= {\"model_name\": \"swin_base_patch4_window7_224\", \"pretrained\": True},\n",
    "    model_name    = \"swin_base_encoder\",\n",
    "    vocab_size    = vocab_size,\n",
    "    encoder_lr    = 8e-5,    # lower LR for the larger Swin\n",
    "    decoder_lr    = 3e-4,\n",
    "    train_loader  = train_loader,\n",
    "    val_loader    = val_loader,\n",
    "    test_loader   = test_loader,\n",
    "    char_to_idx   = char_to_idx,\n",
    "    idx_to_char   = idx_to_char,\n",
    "    max_label_length      = max_label_length,\n",
    "    max_label_length_test = max_label_length_test,\n",
    "    test_data      = test_data,\n",
    "    device         = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5618d5cb-091d-4910-aac5-4e7a1b5da558",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 672.00 MiB (GPU 0; 44.35 GiB total capacity; 42.59 GiB already allocated; 65.81 MiB free; 43.94 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mrun_training_pipeline\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_class\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mSwinEncoder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mswin_large_patch4_window7_224\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpretrained\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m    \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mswin_large_encoder\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvocab_size\u001b[49m\u001b[43m    \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mvocab_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_lr\u001b[49m\u001b[43m    \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m8e-5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_lr\u001b[49m\u001b[43m    \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4e-4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m    \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m   \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchar_to_idx\u001b[49m\u001b[43m   \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mchar_to_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43midx_to_char\u001b[49m\u001b[43m   \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43midx_to_char\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_label_length\u001b[49m\u001b[43m      \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmax_label_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_label_length_test\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmax_label_length_test\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_data\u001b[49m\u001b[43m      \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtest_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m         \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[19], line 59\u001b[0m, in \u001b[0;36mrun_training_pipeline\u001b[0;34m(encoder_class, encoder_kwargs, model_name, vocab_size, encoder_lr, decoder_lr, train_loader, val_loader, test_loader, char_to_idx, idx_to_char, max_label_length, max_label_length_test, test_data, device, num_epochs)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# trainer = ImageCaptioningTrainer(\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m#     encoder=encoder,\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m#     decoder=decoder,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m#     max_label_length=max_label_length,\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m#     model_name=model_name)\u001b[39;00m\n\u001b[1;32m     41\u001b[0m trainer \u001b[38;5;241m=\u001b[39m ImageCaptioningTrainer(\n\u001b[1;32m     42\u001b[0m     encoder\u001b[38;5;241m=\u001b[39mencoder,\n\u001b[1;32m     43\u001b[0m     decoder\u001b[38;5;241m=\u001b[39mdecoder,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     56\u001b[0m     model_name\u001b[38;5;241m=\u001b[39mmodel_name\n\u001b[1;32m     57\u001b[0m )\n\u001b[0;32m---> 59\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m encoder\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m     62\u001b[0m decoder\u001b[38;5;241m.\u001b[39mteacher_forcing_ratio \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n",
      "Cell \u001b[0;32mIn[13], line 39\u001b[0m, in \u001b[0;36mImageCaptioningTrainer.fit\u001b[0;34m(self, num_epochs)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 39\u001b[0m     train_loss, train_cer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m     val_loss, val_cer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalidate_one_epoch()\n\u001b[1;32m     42\u001b[0m     results \u001b[38;5;241m=\u001b[39m inference(\n\u001b[1;32m     43\u001b[0m         encoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder,\n\u001b[1;32m     44\u001b[0m         decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     50\u001b[0m         test_data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtest_data\n\u001b[1;32m     51\u001b[0m     )\n",
      "Cell \u001b[0;32mIn[13], line 124\u001b[0m, in \u001b[0;36mImageCaptioningTrainer.train_one_epoch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    119\u001b[0m encoder_out   \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(images)\n\u001b[1;32m    120\u001b[0m caption_lengths \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\n\u001b[1;32m    121\u001b[0m     [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_label_length] \u001b[38;5;241m*\u001b[39m labels\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    122\u001b[0m )\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m--> 124\u001b[0m outputs, encoded_captions, decode_lengths, alphas, sort_ind \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_out\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaption_lengths\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;66;03m# Targets = encoded captions without the <SOS>\u001b[39;00m\n\u001b[1;32m    129\u001b[0m targets \u001b[38;5;241m=\u001b[39m encoded_captions[:, \u001b[38;5;241m1\u001b[39m:]\n",
      "File \u001b[0;32m/apps/miniforge3/24.3.0-0/miniforge3/envs/dsks_2024.06/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[12], line 76\u001b[0m, in \u001b[0;36mDecoderRNN.forward\u001b[0;34m(self, encoder_out, encoded_captions, caption_lengths)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_decode_length):\n\u001b[1;32m     74\u001b[0m     batch_size_t \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m([l \u001b[38;5;241m>\u001b[39m t \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m decode_lengths])\n\u001b[0;32m---> 76\u001b[0m     attention_weighted_encoding, alpha \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_out\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43mbatch_size_t\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[43m        \u001b[49m\u001b[43mh1\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43mbatch_size_t\u001b[49m\u001b[43m]\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# use the first LSTM layer's hidden state for attention\u001b[39;49;00m\n\u001b[1;32m     79\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     81\u001b[0m     \u001b[38;5;66;03m# Apply gating\u001b[39;00m\n\u001b[1;32m     82\u001b[0m     gate \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msigmoid(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf_beta(h1[:batch_size_t]))\n",
      "File \u001b[0;32m/apps/miniforge3/24.3.0-0/miniforge3/envs/dsks_2024.06/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[11], line 19\u001b[0m, in \u001b[0;36mAttention.forward\u001b[0;34m(self, encoder_out, decoder_hidden)\u001b[0m\n\u001b[1;32m     16\u001b[0m att2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder_att(decoder_hidden)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# [batch_size, 1, attention_dim]\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# sum -> relu -> full_att -> squeeze -> softmax\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m att  \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfull_att(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[43matt1\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43matt2\u001b[49m\u001b[43m)\u001b[49m)\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m2\u001b[39m)  \u001b[38;5;66;03m# [batch_size, num_patches]\u001b[39;00m\n\u001b[1;32m     20\u001b[0m alpha \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msoftmax(att)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Weighted sum of the encoder_out\u001b[39;00m\n",
      "File \u001b[0;32m/apps/miniforge3/24.3.0-0/miniforge3/envs/dsks_2024.06/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/apps/miniforge3/24.3.0-0/miniforge3/envs/dsks_2024.06/lib/python3.9/site-packages/torch/nn/modules/activation.py:103\u001b[0m, in \u001b[0;36mReLU.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 103\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/apps/miniforge3/24.3.0-0/miniforge3/envs/dsks_2024.06/lib/python3.9/site-packages/torch/nn/functional.py:1457\u001b[0m, in \u001b[0;36mrelu\u001b[0;34m(input, inplace)\u001b[0m\n\u001b[1;32m   1455\u001b[0m     result \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu_(\u001b[38;5;28minput\u001b[39m)\n\u001b[1;32m   1456\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1457\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1458\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 672.00 MiB (GPU 0; 44.35 GiB total capacity; 42.59 GiB already allocated; 65.81 MiB free; 43.94 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "run_training_pipeline(\n",
    "    encoder_class = SwinEncoder,\n",
    "    encoder_kwargs= {\"model_name\": \"swin_large_patch4_window7_224\", \"pretrained\": True},\n",
    "    model_name    = \"swin_large_encoder\",\n",
    "    vocab_size    = vocab_size,\n",
    "    encoder_lr    = 8e-5,\n",
    "    decoder_lr    = 4e-4,\n",
    "    train_loader  = train_loader,\n",
    "    val_loader    = val_loader,\n",
    "    test_loader   = test_loader,\n",
    "    char_to_idx   = char_to_idx,\n",
    "    idx_to_char   = idx_to_char,\n",
    "    max_label_length      = max_label_length,\n",
    "    max_label_length_test = max_label_length_test,\n",
    "    test_data      = test_data,\n",
    "    device         = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9e80751d-1f3d-4463-b296-246d1f99e2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.ipc_collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7884c060-7201-4050-b7a0-3689f5938d90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total memory       : 45416.12 MB\n",
      "Allocated memory   : 43609.55 MB\n",
      "Cached memory      : 44996.00 MB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Total memory available on the current GPU (in bytes)\n",
    "total_memory = torch.cuda.get_device_properties(0).total_memory\n",
    "\n",
    "# Memory currently allocated by tensors (in bytes)\n",
    "allocated_memory = torch.cuda.memory_allocated(0)\n",
    "\n",
    "# Memory cached by the allocator (in bytes)\n",
    "cached_memory = torch.cuda.memory_reserved(0)\n",
    "\n",
    "# Print all in MB\n",
    "print(f\"Total memory       : {total_memory / 1024 ** 2:.2f} MB\")\n",
    "print(f\"Allocated memory   : {allocated_memory / 1024 ** 2:.2f} MB\")\n",
    "print(f\"Cached memory      : {cached_memory / 1024 ** 2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a2d301d0-085f-4c1f-a1c4-15280a5783de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max allocated memory : 44282.02 MB\n",
      "Max reserved memory  : 45058.00 MB\n"
     ]
    }
   ],
   "source": [
    "print(f\"Max allocated memory : {torch.cuda.max_memory_allocated(0) / 1024 ** 2:.2f} MB\")\n",
    "print(f\"Max reserved memory  : {torch.cuda.max_memory_reserved(0) / 1024 ** 2:.2f} MB\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
