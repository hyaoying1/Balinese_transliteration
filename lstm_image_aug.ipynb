{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ee18c4-7d20-4008-a971-06f1bd0f39e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import editdistance\n",
    "import time \n",
    "\n",
    "from PIL import Image\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import timm  \n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f12cf6-64ef-4626-9f39-9812dca91ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db252f6-2b6c-432a-9035-0ae2156a123c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base project path\n",
    "BASE_DIR = \"/projects/qb36/lontar_project\"\n",
    "DATA_DIR = os.path.join(BASE_DIR, \"data\")\n",
    "\n",
    "# Define file paths\n",
    "ground_truth_path = os.path.join(DATA_DIR, 'balinese_transliteration_train.txt')\n",
    "images_dir = os.path.join(DATA_DIR, 'balinese_word_train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324fc09b-fa47-4a5f-9ec9-9bb6abd60a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_dir = os.getcwd()\n",
    "\n",
    "# # Same paths as your original code\n",
    "# ground_truth_path = os.path.join(base_dir, 'balinese_transliteration_train.txt') \n",
    "# images_dir        = os.path.join(base_dir, 'balinese_word_train')\n",
    "\n",
    "\n",
    "filenames = []\n",
    "labels    = []\n",
    "\n",
    "with open(ground_truth_path, 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        line = line.strip()\n",
    "        if line:  # Ensure the line is not empty\n",
    "            parts = line.split(';')\n",
    "            if len(parts) == 2:\n",
    "                filename, label = parts\n",
    "                label = label.lower()\n",
    "                filenames.append(filename)\n",
    "                labels.append(label)\n",
    "            else:\n",
    "                print(f\"Skipping malformed line: {line}\")\n",
    "\n",
    "data = pd.DataFrame({\n",
    "    'filename': filenames,\n",
    "    'label': labels\n",
    "})\n",
    "\n",
    "label_counts = data['label'].value_counts()\n",
    "\n",
    "all_text = ''.join(data['label'])\n",
    "unique_chars = sorted(list(set(all_text)))\n",
    "\n",
    "# Create character->index starting from 1\n",
    "char_to_idx = {char: idx + 1 for idx, char in enumerate(unique_chars)}\n",
    "# Add special tokens\n",
    "char_to_idx['<PAD>'] = 0\n",
    "char_to_idx['<UNK>'] = len(char_to_idx)\n",
    "char_to_idx['<SOS>'] = len(char_to_idx)\n",
    "char_to_idx['<EOS>'] = len(char_to_idx)\n",
    "\n",
    "# Reverse mapping\n",
    "idx_to_char = {v: k for k, v in char_to_idx.items()}\n",
    "\n",
    "vocab_size = len(char_to_idx)\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "\n",
    "def encode_label(label, char_to_idx, max_length):\n",
    "    \"\"\"\n",
    "    Converts a label (string) into a list of indices with <SOS>, <EOS>, padding, etc.\n",
    "    \"\"\"\n",
    "    encoded = (\n",
    "        [char_to_idx['<SOS>']] +\n",
    "        [char_to_idx.get(ch, char_to_idx['<UNK>']) for ch in label] +\n",
    "        [char_to_idx['<EOS>']]\n",
    "    )\n",
    "    # Pad if needed\n",
    "    if len(encoded) < max_length:\n",
    "        encoded += [char_to_idx['<PAD>']] * (max_length - len(encoded))\n",
    "    else:\n",
    "        encoded = encoded[:max_length]\n",
    "    return encoded\n",
    "\n",
    "max_label_length = max(len(label) for label in data['label']) + 2  # +2 for <SOS> and <EOS>\n",
    "data['encoded_label'] = data['label'].apply(lambda x: encode_label(x, char_to_idx, max_label_length))\n",
    "data['label_length']  = data['label'].apply(len)\n",
    "\n",
    "rare_labels = label_counts[label_counts < 3].index  # NEW: words that appear <3 times\n",
    "\n",
    "def custom_split(df, rare_label_list, test_size=0.1, random_state=42):\n",
    "    # Separate rare words from frequent ones\n",
    "    rare_df     = df[df['label'].isin(rare_label_list)]\n",
    "    non_rare_df = df[~df['label'].isin(rare_label_list)]\n",
    "\n",
    "    #  train/val split for non-rare\n",
    "    train_nr, val_nr = train_test_split(non_rare_df, test_size=test_size, \n",
    "                                        random_state=random_state)\n",
    "\n",
    "    # Combine rare samples entirely into training\n",
    "    train_df = pd.concat([train_nr, rare_df], ignore_index=True)\n",
    "    # Shuffle after combining\n",
    "    train_df = train_df.sample(frac=1, random_state=random_state).reset_index(drop=True)\n",
    "\n",
    "    val_df = val_nr.reset_index(drop=True)\n",
    "    return train_df, val_df\n",
    "\n",
    "# Call custom_split instead of direct train_test_split\n",
    "train_data, val_data = custom_split(data, rare_labels, test_size=0.1, random_state=42) \n",
    "\n",
    "print(f\"Training size: {len(train_data)}; Validation size: {len(val_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d307eca6-2574-4032-80d7-844b8e58184d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BalineseDataset(Dataset):\n",
    "    def __init__(self, df, images_dir, transform=None):\n",
    "        self.data       = df.reset_index(drop=True)\n",
    "        self.images_dir = images_dir\n",
    "        self.transform  = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name     = self.data.loc[idx, 'filename']\n",
    "        label        = self.data.loc[idx, 'encoded_label']\n",
    "        label_length = self.data.loc[idx, 'label_length']\n",
    "\n",
    "        img_path = os.path.join(self.images_dir, img_name)\n",
    "        image    = Image.open(img_path).convert('RGB')\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        label = torch.tensor(label, dtype=torch.long)\n",
    "        return image, label, torch.tensor(label_length, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb03afe-e10e-45eb-b348-1351e71f38d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomRotation(degrees=5),\n",
    "    transforms.RandomAffine(degrees=0, translate=(0.05, 0.05)),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.RandomErasing(p=0.5, scale=(0.01, 0.05), ratio=(0.3, 3.3)),\n",
    "    transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)),\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)),\n",
    "])\n",
    "\n",
    "train_dataset = BalineseDataset(train_data, images_dir, transform=train_transform)\n",
    "val_dataset   = BalineseDataset(val_data,   images_dir, transform=val_transform)\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,  num_workers=2, pin_memory=True)\n",
    "val_loader   = DataLoader(val_dataset,   batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2653d9-3262-42fd-8a83-f4b71abae491",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet18Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Encoder that uses a pretrained ResNet18 to extract features of shape \n",
    "    [B, H*W, C], which the DecoderRNN can then attend over.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, pretrained=True):\n",
    "        super(ResNet18Encoder, self).__init__()\n",
    "        resnet = models.resnet18(pretrained=pretrained)\n",
    "\n",
    "        # Remove the classification (fc) layer\n",
    "        modules = list(resnet.children())[:-2]  # remove the avgpool & fc\n",
    "        self.cnn = nn.Sequential(*modules)\n",
    "\n",
    "        # last convolutional block outputs 512 channels\n",
    "        self.encoder_dim = 512\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Input shape:  x -> [batch_size, 3, 224, 224]\n",
    "        Output shape: -> [batch_size, num_patches, encoder_dim]\n",
    "                       where num_patches = H' * W' from the final feature map\n",
    "        \"\"\"\n",
    "        # pass through ResNet (up to layer4)\n",
    "        features = self.cnn(x)  # [B, 512, H', W']\n",
    "\n",
    "        # Flatten the spatial dims\n",
    "        # shape => [B, 512, H', W'] -> [B, H'*W', 512]\n",
    "        b, c, h, w = features.shape\n",
    "        features = features.permute(0, 2, 3, 1)   # [B, H', W', C]\n",
    "        features = features.reshape(b, -1, c)     # [B, H'*W', C]\n",
    "\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d1c0aac-ffb1-46dd-b122-741af90faf0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViTEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple ViT encoder that extracts patch embeddings as [batch_size, num_patches, hidden_dim].\n",
    "    We'll use timm to load a pretrained ViT. Then we use .forward_features() to get a\n",
    "    feature map of shape [B, C, H', W'] for many timm ViT models, which we flatten.\n",
    "    \"\"\"\n",
    "    def __init__(self, model_name, pretrained=True):\n",
    "        super(ViTEncoder, self).__init__()\n",
    "        self.vit = timm.create_model(model_name, pretrained=pretrained)\n",
    "        # Remove or replace the classification head\n",
    "        self.vit.head = nn.Identity()\n",
    "\n",
    "        # timm's ViT typically has an embed_dim attribute\n",
    "        self.encoder_dim = self.vit.embed_dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        :param x: [batch_size, 3, 224, 224]\n",
    "        :return:  [batch_size, num_patches, encoder_dim]\n",
    "        \"\"\"\n",
    "        # forward_features usually returns [B, hidden_dim, H', W'] or [B, hidden_dim]\n",
    "        feats = self.vit.forward_features(x)  # [B, hidden_dim, 14, 14] for vit_base_patch16_224\n",
    "\n",
    "        # Flatten the spatial dimensions\n",
    "        if feats.dim() == 4:  # [B, C, H, W]\n",
    "            b, c, h, w = feats.shape\n",
    "            feats = feats.permute(0, 2, 3, 1).reshape(b, -1, c)  # => [B, H*W, C]\n",
    "\n",
    "        return feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591ff7ca-4284-4662-bf71-5962fe8793f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwinEncoder(nn.Module):\n",
    "    def __init__(self, model_name=\"swin_small_patch4_window7_224\", pretrained=True):\n",
    "        \"\"\"\n",
    "        A simple Swin Transformer encoder that extracts patch embeddings\n",
    "        as [batch_size, num_patches, hidden_dim]. We'll use timm to load \n",
    "        a pretrained Swin model, remove its classification head, then flatten.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.swin = timm.create_model(model_name, pretrained=pretrained)\n",
    "        self.swin.head = nn.Identity()\n",
    "\n",
    "        # We'll assign encoder_dim dynamically after forward\n",
    "        self.encoder_dim = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        :param x: [batch_size, 3, 224, 224]\n",
    "        :return:  [batch_size, num_patches, encoder_dim]\n",
    "        \"\"\"\n",
    "        feats = self.swin.forward_features(x)            # [B, C, H, W]\n",
    "        b, c, h, w = feats.shape\n",
    "        feats = feats.flatten(2).transpose(1, 2)         # [B, H*W, C]\n",
    "        # Set encoder_dim once (C)\n",
    "        if self.encoder_dim is None:\n",
    "            self.encoder_dim = feats.shape[-1]\n",
    "        return feats\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd74a845-885b-4282-9569-6d98a63461ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HybridEncoder(nn.Module):\n",
    "    def __init__(self, cnn_encoder, vit_encoder):\n",
    "        super(HybridEncoder, self).__init__()\n",
    "        self.cnn_encoder = cnn_encoder\n",
    "        self.vit_encoder = vit_encoder\n",
    "        # Combined encoder_dim is the sum of both encoder dimensions.\n",
    "        # (CNN outputs 512 channels; ViT outputs its own embed_dim.)\n",
    "        self.encoder_dim = cnn_encoder.encoder_dim + vit_encoder.encoder_dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Get CNN features: expected shape [B, 49, 512]\n",
    "        cnn_features = self.cnn_encoder(x)\n",
    "\n",
    "        # Get ViT features: expected shape [B, 197, vit_dim] for vit_large_patch16_224\n",
    "        vit_features = self.vit_encoder(x)\n",
    "        # If the ViT output contains a class token, remove it.\n",
    "        if vit_features.shape[1] == 197:\n",
    "            vit_features = vit_features[:, 1:, :]  # Now shape: [B, 196, vit_dim]\n",
    "            B, tokens, D = vit_features.shape\n",
    "            # Reshape tokens into a 14x14 grid: [B, 14, 14, D]\n",
    "            vit_features = vit_features.reshape(B, 14, 14, D)\n",
    "            # Permute to [B, D, 14, 14] for pooling\n",
    "            vit_features = vit_features.permute(0, 3, 1, 2)\n",
    "            # Use adaptive pooling to reduce to a 7x7 grid\n",
    "            vit_features = F.adaptive_avg_pool2d(vit_features, (7, 7))\n",
    "            # Permute back to [B, 7, 7, D] and flatten to [B, 49, D]\n",
    "            vit_features = vit_features.permute(0, 2, 3, 1).reshape(B, -1, D)\n",
    "\n",
    "        # Concatenate the features along the feature dimension (dim=2)\n",
    "        # cnn_features: [B, 49, 512] and vit_features: [B, 49, vit_dim]\n",
    "        hybrid_features = torch.cat([cnn_features, vit_features], dim=2)\n",
    "        return hybrid_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793fd95b-6f1e-4c77-9bdb-efb501e919a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, encoder_dim, decoder_dim, attention_dim):\n",
    "        super(Attention, self).__init__()\n",
    "        self.encoder_att = nn.Linear(encoder_dim, attention_dim)  # transform encoder output\n",
    "        self.decoder_att = nn.Linear(decoder_dim, attention_dim)  # transform decoder hidden\n",
    "        self.full_att    = nn.Linear(attention_dim, 1)\n",
    "        self.relu        = nn.ReLU()\n",
    "        self.softmax     = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, encoder_out, decoder_hidden):\n",
    "        \"\"\"\n",
    "        encoder_out:    [batch_size, num_patches, encoder_dim]\n",
    "        decoder_hidden: [batch_size, decoder_dim]\n",
    "        \"\"\"\n",
    "        att1 = self.encoder_att(encoder_out)                  # [batch_size, num_patches, attention_dim]\n",
    "        att2 = self.decoder_att(decoder_hidden).unsqueeze(1)  # [batch_size, 1, attention_dim]\n",
    "\n",
    "        # sum -> relu -> full_att -> squeeze -> softmax\n",
    "        att  = self.full_att(self.relu(att1 + att2)).squeeze(2)  # [batch_size, num_patches]\n",
    "        alpha = self.softmax(att)\n",
    "        # Weighted sum of the encoder_out\n",
    "        attention_weighted_encoding = (encoder_out * alpha.unsqueeze(2)).sum(dim=1)  # [batch_size, encoder_dim]\n",
    "        return attention_weighted_encoding, alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e35eb8-1577-422f-b680-4fd83a0c79f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, attention_dim, embed_dim, decoder_dim, vocab_size,\n",
    "                 encoder_dim=768, teacher_forcing_ratio=0.5):\n",
    "        super().__init__()\n",
    "\n",
    "        self.tfr       = teacher_forcing_ratio\n",
    "        self.attention = Attention(encoder_dim, decoder_dim, attention_dim)\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.dropout   = nn.Dropout(0.5)\n",
    "\n",
    "        self.rnn1 = nn.GRUCell(embed_dim + encoder_dim, decoder_dim)\n",
    "        self.rnn2 = nn.GRUCell(decoder_dim, decoder_dim)\n",
    "\n",
    "        self.init_h1 = nn.Linear(encoder_dim, decoder_dim)\n",
    "        self.init_h2 = nn.Linear(encoder_dim, decoder_dim)\n",
    "\n",
    "        self.f_beta  = nn.Linear(decoder_dim, encoder_dim)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.fc      = nn.Linear(decoder_dim, vocab_size)\n",
    "\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        nn.init.uniform_(self.embedding.weight, -0.1, 0.1)\n",
    "        nn.init.uniform_(self.fc.weight,       -0.1, 0.1)\n",
    "        nn.init.zeros_(self.fc.bias)\n",
    "\n",
    "    def init_hidden_state(self, encoder_out):\n",
    "        mean_enc = encoder_out.mean(dim=1)\n",
    "        h1 = self.init_h1(mean_enc)\n",
    "        h2 = self.init_h2(mean_enc)\n",
    "        return h1, h2\n",
    "\n",
    "    def forward(self, encoder_out, encoded_captions, caption_lengths):\n",
    "        caption_lengths, sort_ind = caption_lengths.squeeze(1).sort(0, True)\n",
    "        encoder_out      = encoder_out[sort_ind]\n",
    "        encoded_captions = encoded_captions[sort_ind]\n",
    "        embeddings       = self.embedding(encoded_captions)\n",
    "\n",
    "        h1, h2 = self.init_hidden_state(encoder_out)\n",
    "\n",
    "        decode_lens = (caption_lengths - 1).tolist()\n",
    "        T = max(decode_lens)\n",
    "        B = encoder_out.size(0)\n",
    "        V = self.fc.out_features\n",
    "        P = encoder_out.size(1)\n",
    "\n",
    "        predictions = torch.zeros(B, T, V, device=encoder_out.device)\n",
    "        alphas      = torch.zeros(B, T, P, device=encoder_out.device)\n",
    "\n",
    "        prev_tok = encoded_captions[:, 0].clone()\n",
    "\n",
    "        for t in range(T):\n",
    "            Bt = sum(l > t for l in decode_lens)\n",
    "\n",
    "            ctx, alpha = self.attention(encoder_out[:Bt], h1[:Bt])\n",
    "            gate = self.sigmoid(self.f_beta(h1[:Bt])) * ctx\n",
    "\n",
    "            use_tf = torch.rand(1).item() < self.tfr\n",
    "            cur_in = embeddings[:Bt, t] if use_tf else self.embedding(prev_tok[:Bt])\n",
    "            x = torch.cat([cur_in, gate], 1)\n",
    "\n",
    "            h1n = self.rnn1(x, h1[:Bt])\n",
    "            h2n = self.rnn2(h1n, h2[:Bt])\n",
    "\n",
    "            out                 = self.fc(self.dropout(h2n))\n",
    "            predictions[:Bt, t] = out\n",
    "            alphas[:Bt, t]      = alpha\n",
    "\n",
    "            # Get next‐token and update buffer without in-place\n",
    "            next_tokens = out.argmax(1).detach()\n",
    "            prev_new    = prev_tok.clone()\n",
    "            prev_new[:Bt] = next_tokens\n",
    "            prev_tok    = prev_new\n",
    "\n",
    "            # Update hidden states safely (as before)\n",
    "            h1_new = h1.clone(); h1_new[:Bt] = h1n\n",
    "            h2_new = h2.clone(); h2_new[:Bt] = h2n\n",
    "            h1, h2 = h1_new, h2_new\n",
    "        return predictions, encoded_captions, decode_lens, alphas, sort_ind\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64f622d-9133-4436-b189-ae127e5afadf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageCaptioningTrainer:\n",
    "    def __init__(self, encoder, decoder, \n",
    "                 criterion, encoder_optimizer, decoder_optimizer, \n",
    "                 train_loader, val_loader, test_loader, test_data, max_label_length_test,\n",
    "                 device, char_to_idx, idx_to_char, max_label_length,\n",
    "                 model_name, csv_filename=\"training_results.csv\"):\n",
    "        self.encoder = encoder.to(device)\n",
    "        self.decoder = decoder.to(device)\n",
    "        self.criterion = criterion\n",
    "        self.encoder_optimizer = encoder_optimizer\n",
    "        self.decoder_optimizer = decoder_optimizer\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.test_loader = test_loader\n",
    "        self.test_data = test_data\n",
    "        self.max_label_length_test = max_label_length_test\n",
    "        self.device = device\n",
    "        self.char_to_idx = char_to_idx\n",
    "        self.idx_to_char = idx_to_char\n",
    "        self.max_label_length = max_label_length\n",
    "        self.model_name = model_name\n",
    "        self.csv_filename = csv_filename\n",
    "\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "        self.train_cers = []\n",
    "        self.val_cers = []\n",
    "        self.test_cers = []\n",
    "\n",
    "    def fit(self, num_epochs):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Early stopping parameters\n",
    "        best_val_loss = float('inf')\n",
    "        early_stop_counter = 0\n",
    "        early_stop_patience = 6\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
    "            train_loss, train_cer = self.train_one_epoch()\n",
    "            val_loss, val_cer = self.validate_one_epoch(top_n=5)\n",
    "\n",
    "            print(f\"[{epoch+1}/{num_epochs}] \"\n",
    "                  f\"Train Loss: {train_loss:.4f}, Train CER: {train_cer:.4f} | \"\n",
    "                  f\"Val Loss: {val_loss:.4f}, Val CER: {val_cer:.4f}\")\n",
    "\n",
    "            # Store epoch results\n",
    "            self.train_losses.append(train_loss)\n",
    "            self.val_losses.append(val_loss)\n",
    "            self.train_cers.append(train_cer)\n",
    "            self.val_cers.append(val_cer)\n",
    "            \n",
    "            # Early stopping check: if current val_loss is better, reset counter; otherwise, increment.\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                early_stop_counter = 0\n",
    "            else:\n",
    "                early_stop_counter += 1\n",
    "                print(f\"Validation loss did not improve. Early stop counter: {early_stop_counter}/{early_stop_patience}\")\n",
    "            \n",
    "            if early_stop_counter >= early_stop_patience:\n",
    "                print(\"Early stopping triggered.\")\n",
    "                break\n",
    "        \n",
    "        # Calculate total training time\n",
    "        end_time = time.time() \n",
    "        total_time = end_time - start_time\n",
    "        hours = int(total_time // 3600)\n",
    "        minutes = int((total_time % 3600) // 60)\n",
    "\n",
    "        print(f\"\\nTraining completed in {hours}h {minutes}m.\")\n",
    "\n",
    "        num_epochs_recorded = len(self.train_losses)\n",
    "        epoch_cols = [f\"epoch{i+1}\" for i in range(num_epochs_recorded)]\n",
    "\n",
    "        # Create the new data block to insert\n",
    "        new_rows = pd.DataFrame([\n",
    "            [self.model_name, \"training loss\"] + self.train_losses,\n",
    "            [self.model_name, \"validation loss\"] + self.val_losses,\n",
    "            [self.model_name, \"training cer\"] + self.train_cers,\n",
    "            [self.model_name, \"validation cer\"] + self.val_cers\n",
    "        ], columns=[\"model_name\", \"mode\"] + epoch_cols)\n",
    "        \n",
    "        # Check if CSV already exists\n",
    "        if os.path.exists(self.csv_filename):\n",
    "            df_existing = pd.read_csv(self.csv_filename)\n",
    "            df_existing = df_existing[df_existing[\"model_name\"] != self.model_name]\n",
    "            df_updated = pd.concat([df_existing, new_rows], ignore_index=True)\n",
    "        else:\n",
    "            df_updated = new_rows\n",
    "\n",
    "        df_updated[epoch_cols] = np.floor(df_updated[epoch_cols] * 100) / 100 \n",
    "        \n",
    "        # Save the updated CSV\n",
    "        df_updated.to_csv(self.csv_filename, index=False)\n",
    "        print(f\"\\nResults have been written to: {self.csv_filename}\")\n",
    "\n",
    "        # Save model weights\n",
    "        # torch.save(self.encoder.state_dict(), f\"encoder_{self.model_name}.pth\")\n",
    "        # torch.save(self.decoder.state_dict(), f\"decoder_{self.model_name}.pth\")\n",
    "        # print(f\"Encoder and decoder models saved: encoder_{self.model_name}.pth, decoder_{self.model_name}.pth\")\n",
    "\n",
    "    def train_one_epoch(self):\n",
    "        self.encoder.train()\n",
    "        self.decoder.train()\n",
    "        running_loss           = 0.0\n",
    "        total_edit_distance    = 0\n",
    "        total_ref_length       = 0\n",
    "\n",
    "        for batch_idx, (images, labels, label_lengths) in enumerate(self.train_loader):\n",
    "            images        = images.to(self.device, non_blocking=True)\n",
    "            labels        = labels.to(self.device, non_blocking=True)\n",
    "            label_lengths = label_lengths.to(self.device, non_blocking=True)\n",
    "\n",
    "            self.encoder_optimizer.zero_grad()\n",
    "            self.decoder_optimizer.zero_grad()\n",
    "\n",
    "            encoder_out   = self.encoder(images)\n",
    "            caption_lengths = torch.tensor(\n",
    "                [self.max_label_length] * labels.size(0)\n",
    "            ).unsqueeze(1).to(self.device)\n",
    "\n",
    "            outputs, encoded_captions, decode_lengths, alphas, sort_ind = self.decoder(\n",
    "                encoder_out, labels, caption_lengths\n",
    "            )\n",
    "\n",
    "            # Targets = encoded captions without the <SOS>\n",
    "            targets = encoded_captions[:, 1:]\n",
    "\n",
    "            # Flatten for loss\n",
    "            outputs_flat = outputs.view(-1, self.decoder.fc.out_features)\n",
    "            targets_flat = targets.contiguous().view(-1)\n",
    "\n",
    "            loss = self.criterion(outputs_flat, targets_flat)\n",
    "            loss.backward()\n",
    "\n",
    "            self.decoder_optimizer.step()\n",
    "            self.encoder_optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            # Compute CER for the batch (global style)\n",
    "            batch_size = labels.size(0)\n",
    "            _, preds_flat = torch.max(outputs_flat, dim=1)\n",
    "            preds_seq = preds_flat.view(batch_size, -1)\n",
    "\n",
    "            for i in range(batch_size):\n",
    "                pred_indices   = preds_seq[i].detach().cpu().numpy()\n",
    "                target_indices = targets[i].detach().cpu().numpy()\n",
    "\n",
    "                mask          = (target_indices != self.char_to_idx['<PAD>'])\n",
    "                pred_indices  = pred_indices[mask]\n",
    "                target_indices= target_indices[mask]\n",
    "\n",
    "                pred_chars    = [self.idx_to_char.get(idx, '') for idx in pred_indices]\n",
    "                target_chars  = [self.idx_to_char.get(idx, '') for idx in target_indices]\n",
    "                pred_str      = ''.join(pred_chars)\n",
    "                target_str    = ''.join(target_chars)\n",
    "\n",
    "                edit_dist           = editdistance.eval(pred_str, target_str)\n",
    "                total_edit_distance += edit_dist\n",
    "                total_ref_length    += len(target_str)\n",
    "\n",
    "            # if (batch_idx + 1) % 50 == 0:\n",
    "            #     print(f'Batch {batch_idx + 1}/{len(self.train_loader)} - Loss: {loss.item():.4f}')\n",
    "\n",
    "        avg_loss = running_loss / len(self.train_loader)\n",
    "        avg_cer  = total_edit_distance / total_ref_length if total_ref_length > 0 else 0.0\n",
    "        return avg_loss, avg_cer\n",
    "\n",
    "    def validate_one_epoch(self, top_n=5):\n",
    "        self.encoder.eval()\n",
    "        self.decoder.eval()\n",
    "        running_loss         = 0.0\n",
    "        total_edit_distance  = 0\n",
    "        total_ref_length     = 0\n",
    "\n",
    "        # each sample’s CER\n",
    "        sample_cer_info = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (images, labels, label_lengths) in enumerate(self.val_loader):\n",
    "                images        = images.to(self.device, non_blocking=True)\n",
    "                labels        = labels.to(self.device, non_blocking=True)\n",
    "                label_lengths = label_lengths.to(self.device, non_blocking=True)\n",
    "\n",
    "                encoder_out = self.encoder(images)\n",
    "                caption_lengths = torch.tensor(\n",
    "                    [self.max_label_length] * labels.size(0)\n",
    "                ).unsqueeze(1).to(self.device)\n",
    "\n",
    "                outputs, encoded_captions, decode_lengths, alphas, sort_ind = self.decoder(\n",
    "                    encoder_out, labels, caption_lengths\n",
    "                )\n",
    "                targets = encoded_captions[:, 1:]\n",
    "\n",
    "                outputs_flat = outputs.view(-1, self.decoder.fc.out_features)\n",
    "                targets_flat = targets.contiguous().view(-1)\n",
    "\n",
    "                loss = self.criterion(outputs_flat, targets_flat)\n",
    "                running_loss += loss.item()\n",
    "\n",
    "                batch_size = labels.size(0)\n",
    "                _, preds_flat = torch.max(outputs_flat, dim=1)\n",
    "                preds_seq = preds_flat.view(batch_size, -1)\n",
    "\n",
    "                for i in range(batch_size):\n",
    "                    pred_indices   = preds_seq[i].detach().cpu().numpy()\n",
    "                    target_indices = targets[i].detach().cpu().numpy()\n",
    "\n",
    "                    mask           = (target_indices != self.char_to_idx['<PAD>'])\n",
    "                    pred_indices   = pred_indices[mask]\n",
    "                    target_indices = target_indices[mask]\n",
    "\n",
    "                    pred_chars   = [self.idx_to_char.get(idx, '') for idx in pred_indices]\n",
    "                    target_chars = [self.idx_to_char.get(idx, '') for idx in target_indices]\n",
    "                    pred_str     = ''.join(pred_chars)\n",
    "                    target_str   = ''.join(target_chars)\n",
    "\n",
    "                    edit_dist = editdistance.eval(pred_str, target_str)\n",
    "                    ref_len   = len(target_str)\n",
    "                    cer       = edit_dist / ref_len if ref_len > 0 else 0\n",
    "    \n",
    "                    total_edit_distance += edit_dist\n",
    "                    total_ref_length    += ref_len\n",
    "    \n",
    "                    # Store sample info\n",
    "                    # sample_cer_info.append({\n",
    "                    #     \"pred\": pred_str,\n",
    "                    #     \"gt\": target_str,\n",
    "                    #     \"cer\": cer\n",
    "                    # })\n",
    "\n",
    "                    # Print a few samples from the 1st batch\n",
    "                    # if batch_idx == 0 and i < 3:\n",
    "                    #     print(f\"Sample {i + 1}:\")\n",
    "                    #     print(f\"Predicted: {pred_str}\")\n",
    "                    #     print(f\"Target   : {target_str}\\n\")\n",
    "\n",
    "        avg_loss = running_loss / len(self.val_loader)\n",
    "        avg_cer  = total_edit_distance / total_ref_length if total_ref_length > 0 else 0.0\n",
    "\n",
    "        # Sort by CER descending\n",
    "        sample_cer_info.sort(key=lambda x: x[\"cer\"], reverse=True)\n",
    "        # Take top_n\n",
    "        worst_samples = sample_cer_info[:top_n]\n",
    "    \n",
    "        # print(f\"\\n=== Top {top_n} Worst Samples by CER ===\")\n",
    "        # for idx, sample in enumerate(worst_samples):\n",
    "        #     print(f\"[{idx+1}] CER: {sample['cer']:.3f}\")\n",
    "        #     print(f\"   Predicted: {sample['pred']}\")\n",
    "        #     print(f\"   Ground Truth: {sample['gt']}\\n\")\n",
    "       \n",
    "        return avg_loss, avg_cer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f94879-7a3d-41a8-98cf-c64a0dc83d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ground_truth_path = os.path.join(DATA_DIR, 'balinese_transliteration_test.txt')\n",
    "test_images_dir        = os.path.join(DATA_DIR, 'balinese_word_test')\n",
    "\n",
    "test_filenames = []\n",
    "test_labels    = []\n",
    "\n",
    "with open(test_ground_truth_path, 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        line = line.strip()\n",
    "        if line:\n",
    "            parts = line.split(';')\n",
    "            if len(parts) == 2:\n",
    "                filename, label = parts\n",
    "                label = label.lower()\n",
    "                test_filenames.append(filename)\n",
    "                test_labels.append(label)\n",
    "            else:\n",
    "                print(f\"Skipping malformed line: {line}\")\n",
    "\n",
    "test_data = pd.DataFrame({\n",
    "    'filename': test_filenames,\n",
    "    'label': test_labels\n",
    "})\n",
    "\n",
    "# Check for unknown chars in test set\n",
    "test_chars = set(''.join(test_data['label']))\n",
    "unknown_chars = test_chars - set(char_to_idx.keys())\n",
    "print(f\"Unknown characters in test labels: {unknown_chars}\")\n",
    "\n",
    "# Encode test labels\n",
    "max_label_length_test = max(len(lbl) for lbl in test_data['label']) + 2\n",
    "def encode_label_test(label, char_to_idx, max_length):\n",
    "    encoded = (\n",
    "        [char_to_idx['<SOS>']] +\n",
    "        [char_to_idx.get(ch, char_to_idx['<UNK>']) for ch in label] +\n",
    "        [char_to_idx['<EOS>']]\n",
    "    )\n",
    "    if len(encoded) > max_length:\n",
    "        encoded = encoded[:max_length]\n",
    "    else:\n",
    "        encoded += [char_to_idx['<PAD>']] * (max_length - len(encoded))\n",
    "    return encoded\n",
    "\n",
    "test_data['encoded_label'] = test_data['label'].apply(lambda x: encode_label_test(x, char_to_idx, max_label_length_test))\n",
    "test_data['label_length']  = test_data['label'].apply(len)\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=(0.5, 0.5, 0.5),\n",
    "        std=(0.5, 0.5, 0.5)\n",
    "    )\n",
    "])\n",
    "\n",
    "test_dataset = BalineseDataset(test_data, test_images_dir, transform=test_transform)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9384c147-036c-46e8-afd0-bf7cc1ad8ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(encoder, decoder, data_loader, device, char_to_idx, idx_to_char, max_seq_length, test_data):\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "\n",
    "    eos_idx = char_to_idx['<EOS>']\n",
    "    results = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (images, labels, label_lengths) in enumerate(data_loader):\n",
    "            images = images.to(device, non_blocking=True)\n",
    "            labels = labels.to(device, non_blocking=True)\n",
    "\n",
    "            batch_size = images.size(0)\n",
    "            encoder_out = encoder(images)  # [B, num_patches, encoder_dim]\n",
    "\n",
    "            h1, h2 = decoder.init_hidden_state(encoder_out)\n",
    "\n",
    "            # Start tokens (all <SOS>)\n",
    "            inputs = torch.full(\n",
    "                (batch_size,),\n",
    "                fill_value=char_to_idx['<SOS>'],\n",
    "                dtype=torch.long,\n",
    "                device=device\n",
    "            )\n",
    "\n",
    "            all_preds = []\n",
    "\n",
    "            for _ in range(max_seq_length):\n",
    "                embeddings = decoder.embedding(inputs)\n",
    "                attn_ctx, _ = decoder.attention(encoder_out, h1)\n",
    "                gate = decoder.sigmoid(decoder.f_beta(h1))\n",
    "                attn_ctx = gate * attn_ctx\n",
    "                x = torch.cat([embeddings, attn_ctx], dim=1)\n",
    "\n",
    "                h1 = decoder.rnn1(x, h1)\n",
    "                h2 = decoder.rnn2(h1, h2)\n",
    "\n",
    "                logits = decoder.fc(decoder.dropout(h2))\n",
    "                preds_idx = logits.argmax(dim=1)\n",
    "\n",
    "                all_preds.append(preds_idx.cpu().numpy())\n",
    "                inputs = preds_idx\n",
    "\n",
    "            all_preds = np.array(all_preds).T\n",
    "\n",
    "            for i in range(batch_size):\n",
    "                pred_indices = all_preds[i]\n",
    "                if eos_idx in pred_indices:\n",
    "                    pred_indices = pred_indices[:np.where(pred_indices == eos_idx)[0][0]]\n",
    "\n",
    "                pred_str = ''.join([idx_to_char.get(idx, '') for idx in pred_indices])\n",
    "\n",
    "                label_indices = labels[i].cpu().numpy()[1:]  # remove <SOS>\n",
    "                if eos_idx in label_indices:\n",
    "                    label_indices = label_indices[:np.where(label_indices == eos_idx)[0][0]]\n",
    "                else:\n",
    "                    label_indices = label_indices[label_indices != char_to_idx['<PAD>']]\n",
    "\n",
    "                label_str = ''.join([idx_to_char.get(idx, '') for idx in label_indices])\n",
    "                image_filename = test_data.iloc[batch_idx * batch_size + i]['filename']\n",
    "\n",
    "                results.append({\n",
    "                    'image_filename': image_filename,\n",
    "                    'predicted_caption': pred_str,\n",
    "                    'ground_truth_caption': label_str\n",
    "                })\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897c292c-2a83-4f60-a0d4-6156f8ce9011",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_global_cer(results):\n",
    "    total_ed   = 0\n",
    "    total_refs = 0\n",
    "    for r in results:\n",
    "        ref = r['ground_truth_caption']\n",
    "        hyp = r['predicted_caption']\n",
    "        dist = editdistance.eval(ref, hyp)\n",
    "        total_ed   += dist\n",
    "        total_refs += len(ref)\n",
    "    if total_refs == 0:\n",
    "        return 0.0\n",
    "    return total_ed / total_refs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c297dda1-cd38-4fc3-a091-9a3503a71d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_top_worst_samples(results, n=5):\n",
    "    # Calculate CER for each sample\n",
    "    results_with_cer = []\n",
    "    for r in results:\n",
    "        ref = r['ground_truth_caption']\n",
    "        hyp = r['predicted_caption']\n",
    "        dist = editdistance.eval(ref, hyp)\n",
    "        length = len(ref)\n",
    "        cer = dist / length if length > 0 else 0\n",
    "        # Copy the record and add cer\n",
    "        new_r = r.copy()\n",
    "        new_r['cer'] = cer\n",
    "        results_with_cer.append(new_r)\n",
    "\n",
    "    # Sort by CER (descending) and take the top N\n",
    "    results_with_cer.sort(key=lambda x: x['cer'], reverse=True)\n",
    "    worst_samples = results_with_cer[:n]\n",
    "\n",
    "    print(f\"\\n=== Top {n} Worst Samples by CER ===\")\n",
    "    for i, sample in enumerate(worst_samples, start=1):\n",
    "        print(f\"{i}) Image: {sample['image_filename']}\")\n",
    "        print(f\"   CER: {sample['cer']:.4f}\")\n",
    "        print(f\"   Predicted       : {sample['predicted_caption']}\")\n",
    "        print(f\"   Ground Truth    : {sample['ground_truth_caption']}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb63e5c1-f5e6-4053-a166-1d4ab4dab40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure CSV files exist\n",
    "training_csv = \"training_results.csv\"\n",
    "if not os.path.exists(training_csv) or os.path.getsize(training_csv) == 0:\n",
    "    pd.DataFrame(columns=[\"model_name\", \"mode\", \"epoch1\", \"epoch2\"]).to_csv(training_csv, index=False)\n",
    "\n",
    "csv_file = \"test_cer_results.csv\"\n",
    "if not os.path.exists(csv_file) or os.path.getsize(csv_file) == 0:\n",
    "    pd.DataFrame(columns=[\"model_name\", \"test_cer\"]).to_csv(csv_file, index=False)\n",
    "\n",
    "def log_test_cer(model_name, cer_value):\n",
    "    \"\"\"\n",
    "    Logs or updates the test CER for a given model, rounding values to 4 decimals.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(csv_file)\n",
    "    \n",
    "    # Round the new CER value to 4 decimals\n",
    "    cer_rounded = round(cer_value, 4)\n",
    "    \n",
    "    if model_name in df['model_name'].values:\n",
    "        # Update existing row\n",
    "        df.loc[df['model_name'] == model_name, 'test_cer'] = cer_rounded\n",
    "    else:\n",
    "        # Add new row\n",
    "        new_row = pd.DataFrame({\n",
    "            \"model_name\": [model_name],\n",
    "            \"test_cer\":   [cer_rounded]\n",
    "        })\n",
    "        df = pd.concat([df, new_row], ignore_index=True)\n",
    "    \n",
    "    # Ensure all stored values are rounded to 4 decimals\n",
    "    df['test_cer'] = df['test_cer'].round(4)\n",
    "    \n",
    "    # Save back to CSV\n",
    "    df.to_csv(csv_file, index=False)\n",
    "    print(f\"Logged {model_name}: {cer_rounded:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c43587-227f-46a6-aebf-591f6c5c1a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training_pipeline(encoder_class,encoder_kwargs, model_name,vocab_size,encoder_lr, decoder_lr,train_loader,\n",
    "                          val_loader,test_loader,char_to_idx,idx_to_char,max_label_length,max_label_length_test,test_data,\n",
    "                          device, num_epochs=1):\n",
    "    #build encoder & grab its dimension\n",
    "    encoder = encoder_class(**encoder_kwargs).to(device)\n",
    "    # if encoder_dim is None (e.g. Swin), prime it with a dummy batch\n",
    "    if encoder.encoder_dim is None:\n",
    "        with torch.no_grad():\n",
    "            dummy = torch.randn(1, 3, 224, 224, device=device)\n",
    "            _ = encoder(dummy)\n",
    "    enc_dim = encoder.encoder_dim\n",
    "\n",
    "    #build decoder \n",
    "    decoder = DecoderRNN(\n",
    "        attention_dim=256,\n",
    "        embed_dim=256,\n",
    "        decoder_dim=512,\n",
    "        vocab_size=vocab_size,\n",
    "        encoder_dim=enc_dim,\n",
    "        teacher_forcing_ratio=0.5\n",
    "    ).to(device)\n",
    "\n",
    "    # loss, optimizers, trainer\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=char_to_idx['<PAD>'])\n",
    "    encoder_optimizer = optim.Adam(encoder.parameters(), lr=encoder_lr)\n",
    "    decoder_optimizer = optim.Adam(decoder.parameters(), lr=decoder_lr)\n",
    "\n",
    "    # trainer = ImageCaptioningTrainer(\n",
    "    #     encoder=encoder,\n",
    "    #     decoder=decoder,\n",
    "    #     criterion=criterion,\n",
    "    #     encoder_optimizer=encoder_optimizer,\n",
    "    #     decoder_optimizer=decoder_optimizer,\n",
    "    #     train_loader=train_loader,\n",
    "    #     val_loader=val_loader,\n",
    "    #     device=device,\n",
    "    #     char_to_idx=char_to_idx,\n",
    "    #     idx_to_char=idx_to_char,\n",
    "    #     max_label_length=max_label_length,\n",
    "    #     model_name=model_name)\n",
    "    trainer = ImageCaptioningTrainer(\n",
    "        encoder=encoder,\n",
    "        decoder=decoder,\n",
    "        criterion=criterion,\n",
    "        encoder_optimizer=encoder_optimizer,\n",
    "        decoder_optimizer=decoder_optimizer,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        test_loader=test_loader,  \n",
    "        test_data=test_data,      \n",
    "        max_label_length_test=max_label_length_test,  \n",
    "        device=device,\n",
    "        char_to_idx=char_to_idx,\n",
    "        idx_to_char=idx_to_char,\n",
    "        max_label_length=max_label_length,\n",
    "        model_name=model_name\n",
    "    )\n",
    "\n",
    "    trainer.fit(num_epochs)\n",
    "\n",
    "    encoder.eval()\n",
    "    decoder.teacher_forcing_ratio = 0.0\n",
    "\n",
    "    results = inference(\n",
    "        encoder=encoder,\n",
    "        decoder=decoder,\n",
    "        data_loader=test_loader,\n",
    "        device=device,\n",
    "        char_to_idx=char_to_idx,\n",
    "        idx_to_char=idx_to_char,\n",
    "        max_seq_length=max_label_length_test,\n",
    "        test_data=test_data\n",
    "    )\n",
    "\n",
    "    cer = calculate_global_cer(results)\n",
    "    print(f\"{model_name} — Test CER: {cer:.4f}\")\n",
    "    print_top_worst_samples(results, n=5)\n",
    "    log_test_cer(model_name, cer)\n",
    "\n",
    "    del encoder, decoder, trainer\n",
    "    torch.cuda.empty_cache()\n",
    "    print(f\"Memory cleared for {model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0a2980-8a58-4b99-9258-aac3be507afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_training_pipeline(\n",
    "    encoder_class = ResNet18Encoder,\n",
    "    encoder_kwargs= {},                              \n",
    "    model_name    = \"resnet18_encoder\",\n",
    "    vocab_size    = vocab_size,\n",
    "    encoder_lr    = 1e-4,\n",
    "    decoder_lr    = 4e-4,\n",
    "    train_loader  = train_loader,\n",
    "    val_loader    = val_loader,\n",
    "    test_loader   = test_loader,\n",
    "    char_to_idx   = char_to_idx,\n",
    "    idx_to_char   = idx_to_char,\n",
    "    max_label_length      = max_label_length,\n",
    "    max_label_length_test = max_label_length_test,\n",
    "    test_data      = test_data,\n",
    "    device         = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a1bd21-9997-4e3d-9393-58678957806e",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_training_pipeline(\n",
    "    encoder_class = ViTEncoder,\n",
    "    encoder_kwargs= {\"model_name\":\"vit_base_patch16_224\",\"pretrained\":True},\n",
    "    model_name    = \"vit_base_patch16_224\",\n",
    "    vocab_size    = vocab_size,\n",
    "    encoder_lr    = 1e-4,\n",
    "    decoder_lr    = 4e-4,\n",
    "    train_loader  = train_loader,\n",
    "    val_loader    = val_loader,\n",
    "    test_loader   = test_loader,\n",
    "    char_to_idx   = char_to_idx,\n",
    "    idx_to_char   = idx_to_char,\n",
    "    max_label_length      = max_label_length,\n",
    "    max_label_length_test = max_label_length_test,\n",
    "    test_data      = test_data,\n",
    "    device         = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28906722-8ed0-4ee9-bd02-8ab57712e445",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_training_pipeline(\n",
    "    encoder_class = ViTEncoder,\n",
    "    encoder_kwargs= {\"model_name\": \"vit_large_patch16_224\", \"pretrained\": True},\n",
    "    model_name    = \"vit_large_patch16_224\",\n",
    "    vocab_size    = vocab_size,\n",
    "    encoder_lr    = 1e-4,\n",
    "    decoder_lr    = 4e-4,\n",
    "    train_loader  = train_loader,\n",
    "    val_loader    = val_loader,\n",
    "    test_loader   = test_loader,\n",
    "    char_to_idx   = char_to_idx,\n",
    "    idx_to_char   = idx_to_char,\n",
    "    max_label_length      = max_label_length,\n",
    "    max_label_length_test = max_label_length_test,\n",
    "    test_data      = test_data,\n",
    "    device         = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48be8ef4-df37-44ac-bfc6-347cb29ab6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_training_pipeline(\n",
    "    encoder_class = HybridEncoder,\n",
    "    encoder_kwargs= {\n",
    "        \"cnn_encoder\": ResNet18Encoder(pretrained=True),\n",
    "        \"vit_encoder\": ViTEncoder(model_name=\"vit_base_patch16_224\", pretrained=True)\n",
    "    },\n",
    "    model_name    = \"hybrid_cnn_vit_base_encoder\",\n",
    "    vocab_size    = vocab_size,\n",
    "    encoder_lr    = 1e-4,\n",
    "    decoder_lr    = 4e-4,\n",
    "    train_loader  = train_loader,\n",
    "    val_loader    = val_loader,\n",
    "    test_loader   = test_loader,\n",
    "    char_to_idx   = char_to_idx,\n",
    "    idx_to_char   = idx_to_char,\n",
    "    max_label_length      = max_label_length,\n",
    "    max_label_length_test = max_label_length_test,\n",
    "    test_data      = test_data,\n",
    "    device         = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f7e5ce-f746-4935-962e-876d3f5fcccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_training_pipeline(\n",
    "    encoder_class = HybridEncoder,\n",
    "    encoder_kwargs= {\n",
    "        \"cnn_encoder\": ResNet18Encoder(pretrained=True),\n",
    "        \"vit_encoder\": ViTEncoder(model_name=\"vit_large_patch16_224\", pretrained=True)\n",
    "    },\n",
    "    model_name    = \"hybrid_cnn_vit_encoder\",\n",
    "    vocab_size    = vocab_size,\n",
    "    encoder_lr    = 1e-4,\n",
    "    decoder_lr    = 4e-4,\n",
    "    train_loader  = train_loader,\n",
    "    val_loader    = val_loader,\n",
    "    test_loader   = test_loader,\n",
    "    char_to_idx   = char_to_idx,\n",
    "    idx_to_char   = idx_to_char,\n",
    "    max_label_length      = max_label_length,\n",
    "    max_label_length_test = max_label_length_test,\n",
    "    test_data      = test_data,\n",
    "    device         = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca61a07-f184-4f3b-a11c-888c5e5334ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_training_pipeline(\n",
    "    encoder_class=SwinEncoder,\n",
    "    encoder_kwargs={\"model_name\":\"swin_small_patch4_window7_224\",\"pretrained\":True},\n",
    "    model_name=\"swin_small_encoder\",\n",
    "    vocab_size=vocab_size,\n",
    "    encoder_lr=1e-4,\n",
    "    decoder_lr=4e-4,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    test_loader=test_loader,\n",
    "    char_to_idx=char_to_idx,\n",
    "    idx_to_char=idx_to_char,\n",
    "    max_label_length=max_label_length,\n",
    "    max_label_length_test=max_label_length_test,\n",
    "    test_data=test_data,\n",
    "    device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c698ba-ff1f-4667-a7db-4bf51eae88ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_training_pipeline(\n",
    "    encoder_class = SwinEncoder,\n",
    "    encoder_kwargs= {\"model_name\": \"swin_base_patch4_window7_224\", \"pretrained\": True},\n",
    "    model_name    = \"swin_base_encoder\",\n",
    "    vocab_size    = vocab_size,\n",
    "    encoder_lr    = 8e-5,    # lower LR for the larger Swin\n",
    "    decoder_lr    = 3e-4,\n",
    "    train_loader  = train_loader,\n",
    "    val_loader    = val_loader,\n",
    "    test_loader   = test_loader,\n",
    "    char_to_idx   = char_to_idx,\n",
    "    idx_to_char   = idx_to_char,\n",
    "    max_label_length      = max_label_length,\n",
    "    max_label_length_test = max_label_length_test,\n",
    "    test_data      = test_data,\n",
    "    device         = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5618d5cb-091d-4910-aac5-4e7a1b5da558",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_training_pipeline(\n",
    "    encoder_class = SwinEncoder,\n",
    "    encoder_kwargs= {\"model_name\": \"swin_large_patch4_window7_224\", \"pretrained\": True},\n",
    "    model_name    = \"swin_large_encoder\",\n",
    "    vocab_size    = vocab_size,\n",
    "    encoder_lr    = 5e-5,\n",
    "    decoder_lr    = 2e-4,\n",
    "    train_loader  = train_loader,\n",
    "    val_loader    = val_loader,\n",
    "    test_loader   = test_loader,\n",
    "    char_to_idx   = char_to_idx,\n",
    "    idx_to_char   = idx_to_char,\n",
    "    max_label_length      = max_label_length,\n",
    "    max_label_length_test = max_label_length_test,\n",
    "    test_data      = test_data,\n",
    "    device         = device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
