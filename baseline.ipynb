{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52ee18c4-7d20-4008-a971-06f1bd0f39e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import editdistance\n",
    "import time \n",
    "\n",
    "\n",
    "from PIL import Image\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "import timm  \n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "324fc09b-fa47-4a5f-9ec9-9bb6abd60a1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 39\n",
      "Training size: 13972; Validation size: 1050\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "base_dir = os.getcwd()\n",
    "\n",
    "# Same paths as your original code\n",
    "ground_truth_path = os.path.join(base_dir, 'balinese_transliteration_train.txt') \n",
    "images_dir        = os.path.join(base_dir, 'balinese_word_train')\n",
    "\n",
    "filenames = []\n",
    "labels    = []\n",
    "\n",
    "with open(ground_truth_path, 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        line = line.strip()\n",
    "        if line:  # Ensure the line is not empty\n",
    "            parts = line.split(';')\n",
    "            if len(parts) == 2:\n",
    "                filename, label = parts\n",
    "                label = label.lower()\n",
    "                filenames.append(filename)\n",
    "                labels.append(label)\n",
    "            else:\n",
    "                print(f\"Skipping malformed line: {line}\")\n",
    "\n",
    "data = pd.DataFrame({\n",
    "    'filename': filenames,\n",
    "    'label': labels\n",
    "})\n",
    "\n",
    "label_counts = data['label'].value_counts()\n",
    "\n",
    "all_text = ''.join(data['label'])\n",
    "unique_chars = sorted(list(set(all_text)))\n",
    "\n",
    "# Create character->index starting from 1\n",
    "char_to_idx = {char: idx + 1 for idx, char in enumerate(unique_chars)}\n",
    "# Add special tokens\n",
    "char_to_idx['<PAD>'] = 0\n",
    "char_to_idx['<UNK>'] = len(char_to_idx)\n",
    "char_to_idx['<SOS>'] = len(char_to_idx)\n",
    "char_to_idx['<EOS>'] = len(char_to_idx)\n",
    "\n",
    "# Reverse mapping\n",
    "idx_to_char = {v: k for k, v in char_to_idx.items()}\n",
    "\n",
    "vocab_size = len(char_to_idx)\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "\n",
    "def encode_label(label, char_to_idx, max_length):\n",
    "    \"\"\"\n",
    "    Converts a label (string) into a list of indices with <SOS>, <EOS>, padding, etc.\n",
    "    \"\"\"\n",
    "    encoded = (\n",
    "        [char_to_idx['<SOS>']] +\n",
    "        [char_to_idx.get(ch, char_to_idx['<UNK>']) for ch in label] +\n",
    "        [char_to_idx['<EOS>']]\n",
    "    )\n",
    "    # Pad if needed\n",
    "    if len(encoded) < max_length:\n",
    "        encoded += [char_to_idx['<PAD>']] * (max_length - len(encoded))\n",
    "    else:\n",
    "        encoded = encoded[:max_length]\n",
    "    return encoded\n",
    "\n",
    "max_label_length = max(len(label) for label in data['label']) + 2  # +2 for <SOS> and <EOS>\n",
    "data['encoded_label'] = data['label'].apply(lambda x: encode_label(x, char_to_idx, max_label_length))\n",
    "data['label_length']  = data['label'].apply(len)\n",
    "\n",
    "rare_labels = label_counts[label_counts < 3].index  # NEW: words that appear <3 times\n",
    "\n",
    "def custom_split(df, rare_label_list, test_size=0.1, random_state=42):\n",
    "    # Separate rare words from frequent ones\n",
    "    rare_df     = df[df['label'].isin(rare_label_list)]\n",
    "    non_rare_df = df[~df['label'].isin(rare_label_list)]\n",
    "\n",
    "    #  train/val split for non-rare\n",
    "    train_nr, val_nr = train_test_split(non_rare_df, test_size=test_size, \n",
    "                                        random_state=random_state)\n",
    "\n",
    "    # Combine rare samples entirely into training\n",
    "    train_df = pd.concat([train_nr, rare_df], ignore_index=True)\n",
    "    # Shuffle after combining\n",
    "    train_df = train_df.sample(frac=1, random_state=random_state).reset_index(drop=True)\n",
    "\n",
    "    val_df = val_nr.reset_index(drop=True)\n",
    "    return train_df, val_df\n",
    "\n",
    "# Call custom_split instead of direct train_test_split\n",
    "train_data, val_data = custom_split(data, rare_labels, test_size=0.1, random_state=42) \n",
    "\n",
    "print(f\"Training size: {len(train_data)}; Validation size: {len(val_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d307eca6-2574-4032-80d7-844b8e58184d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BalineseDataset(Dataset):\n",
    "    def __init__(self, df, images_dir, transform=None):\n",
    "        self.data       = df.reset_index(drop=True)\n",
    "        self.images_dir = images_dir\n",
    "        self.transform  = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name     = self.data.loc[idx, 'filename']\n",
    "        label        = self.data.loc[idx, 'encoded_label']\n",
    "        label_length = self.data.loc[idx, 'label_length']\n",
    "\n",
    "        img_path = os.path.join(self.images_dir, img_name)\n",
    "        image    = Image.open(img_path).convert('RGB')\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        label = torch.tensor(label, dtype=torch.long)\n",
    "        return image, label, torch.tensor(label_length, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bbb03afe-e10e-45eb-b348-1351e71f38d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=(0.5, 0.5, 0.5),\n",
    "        std=(0.5, 0.5, 0.5)\n",
    "    )\n",
    "])\n",
    "\n",
    "train_dataset = BalineseDataset(train_data, images_dir, transform=transform)\n",
    "val_dataset   = BalineseDataset(val_data,   images_dir, transform=transform)\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,  num_workers=2, pin_memory=True)\n",
    "val_loader   = DataLoader(val_dataset,   batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d2653d9-3262-42fd-8a83-f4b71abae491",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet18Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Encoder that uses a pretrained ResNet18 to extract features of shape \n",
    "    [B, H*W, C], which the DecoderRNN can then attend over.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, pretrained=True):\n",
    "        super(ResNet18Encoder, self).__init__()\n",
    "        resnet = models.resnet18(pretrained=pretrained)\n",
    "\n",
    "        # Remove the classification (fc) layer\n",
    "        modules = list(resnet.children())[:-2]  # remove the avgpool & fc\n",
    "        self.cnn = nn.Sequential(*modules)\n",
    "\n",
    "        # last convolutional block outputs 512 channels\n",
    "        self.encoder_dim = 512\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Input shape:  x -> [batch_size, 3, 224, 224]\n",
    "        Output shape: -> [batch_size, num_patches, encoder_dim]\n",
    "                       where num_patches = H' * W' from the final feature map\n",
    "        \"\"\"\n",
    "        # pass through ResNet (up to layer4)\n",
    "        features = self.cnn(x)  # [B, 512, H', W']\n",
    "\n",
    "        # Flatten the spatial dims\n",
    "        # shape => [B, 512, H', W'] -> [B, H'*W', 512]\n",
    "        b, c, h, w = features.shape\n",
    "        features = features.permute(0, 2, 3, 1)   # [B, H', W', C]\n",
    "        features = features.reshape(b, -1, c)     # [B, H'*W', C]\n",
    "\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f2dad2eb-e42f-4cae-80ec-fba0d744ad74",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet50Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Encoder that uses a pretrained ResNet50 to extract features of shape \n",
    "    [B, H*W, C]. For ResNet50, the final block has 2048 output channels.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, pretrained=True):\n",
    "        super(ResNet50Encoder, self).__init__()\n",
    "        resnet = models.resnet50(pretrained=pretrained)\n",
    "\n",
    "        # remove the avgpool & fc layers\n",
    "        modules = list(resnet.children())[:-2]  \n",
    "        self.cnn = nn.Sequential(*modules)\n",
    "\n",
    "        # For ResNet50, the last block has 2048 output channels\n",
    "        self.encoder_dim = 2048\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x -> [B, 3, 224, 224]\n",
    "        features = self.cnn(x)  # [B, 2048, H', W']\n",
    "\n",
    "        b, c, h, w = features.shape\n",
    "        features = features.permute(0, 2, 3, 1)   # [B, H', W', C]\n",
    "        features = features.reshape(b, -1, c)     # [B, H'*W', C]\n",
    "\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d1c0aac-ffb1-46dd-b122-741af90faf0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViTEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple ViT encoder that extracts patch embeddings as [batch_size, num_patches, hidden_dim].\n",
    "    We'll use timm to load a pretrained ViT. Then we use .forward_features() to get a\n",
    "    feature map of shape [B, C, H', W'] for many timm ViT models, which we flatten.\n",
    "    \"\"\"\n",
    "    def __init__(self, model_name=\"vit_large_patch16_224\", pretrained=True):\n",
    "        super(ViTEncoder, self).__init__()\n",
    "        self.vit = timm.create_model(model_name, pretrained=pretrained)\n",
    "        # Remove or replace the classification head\n",
    "        self.vit.head = nn.Identity()\n",
    "\n",
    "        # timm's ViT typically has an embed_dim attribute\n",
    "        self.encoder_dim = self.vit.embed_dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        :param x: [batch_size, 3, 224, 224]\n",
    "        :return:  [batch_size, num_patches, encoder_dim]\n",
    "        \"\"\"\n",
    "        # forward_features usually returns [B, hidden_dim, H', W'] or [B, hidden_dim]\n",
    "        feats = self.vit.forward_features(x)  # [B, hidden_dim, 14, 14] for vit_base_patch16_224\n",
    "\n",
    "        # Flatten the spatial dimensions\n",
    "        if feats.dim() == 4:  # [B, C, H, W]\n",
    "            b, c, h, w = feats.shape\n",
    "            feats = feats.permute(0, 2, 3, 1).reshape(b, -1, c)  # => [B, H*W, C]\n",
    "\n",
    "        return feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "793fd95b-6f1e-4c77-9bdb-efb501e919a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, encoder_dim, decoder_dim, attention_dim):\n",
    "        super(Attention, self).__init__()\n",
    "        self.encoder_att = nn.Linear(encoder_dim, attention_dim)  # transform encoder output\n",
    "        self.decoder_att = nn.Linear(decoder_dim, attention_dim)  # transform decoder hidden\n",
    "        self.full_att    = nn.Linear(attention_dim, 1)\n",
    "        self.relu        = nn.ReLU()\n",
    "        self.softmax     = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, encoder_out, decoder_hidden):\n",
    "        \"\"\"\n",
    "        encoder_out:    [batch_size, num_patches, encoder_dim]\n",
    "        decoder_hidden: [batch_size, decoder_dim]\n",
    "        \"\"\"\n",
    "        att1 = self.encoder_att(encoder_out)                  # [batch_size, num_patches, attention_dim]\n",
    "        att2 = self.decoder_att(decoder_hidden).unsqueeze(1)  # [batch_size, 1, attention_dim]\n",
    "\n",
    "        # sum -> relu -> full_att -> squeeze -> softmax\n",
    "        att  = self.full_att(self.relu(att1 + att2)).squeeze(2)  # [batch_size, num_patches]\n",
    "        alpha = self.softmax(att)\n",
    "        # Weighted sum of the encoder_out\n",
    "        attention_weighted_encoding = (encoder_out * alpha.unsqueeze(2)).sum(dim=1)  # [batch_size, encoder_dim]\n",
    "        return attention_weighted_encoding, alpha\n",
    "\n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, attention_dim, embed_dim, decoder_dim, vocab_size, encoder_dim=768, teacher_forcing_ratio=0.5):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "\n",
    "        self.attention     = Attention(encoder_dim, decoder_dim, attention_dim)\n",
    "        self.embedding     = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.dropout       = nn.Dropout(p=0.5)\n",
    "\n",
    "        # [embed_dim + encoder_dim] -> decoder_dim\n",
    "        self.lstm1 = nn.LSTMCell(embed_dim + encoder_dim, decoder_dim)\n",
    "        # decoder_dim -> decoder_dim\n",
    "        self.lstm2 = nn.LSTMCell(decoder_dim, decoder_dim)\n",
    "\n",
    "        # For initializing the hidden states of both LSTM layers\n",
    "        self.init_h1 = nn.Linear(encoder_dim, decoder_dim)\n",
    "        self.init_c1 = nn.Linear(encoder_dim, decoder_dim)\n",
    "        self.init_h2 = nn.Linear(encoder_dim, decoder_dim)\n",
    "        self.init_c2 = nn.Linear(encoder_dim, decoder_dim)\n",
    "\n",
    "        # Gating\n",
    "        self.f_beta  = nn.Linear(decoder_dim, encoder_dim)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "        # Final linear layer for output vocab\n",
    "        self.fc = nn.Linear(decoder_dim, vocab_size)\n",
    "\n",
    "        self.teacher_forcing_ratio = teacher_forcing_ratio\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        self.embedding.weight.data.uniform_(-0.1, 0.1)\n",
    "        self.fc.bias.data.fill_(0)\n",
    "        self.fc.weight.data.uniform_(-0.1, 0.1)\n",
    "\n",
    "    def init_hidden_state(self, encoder_out):\n",
    "        # encoder_out: [batch_size, num_patches, encoder_dim]\n",
    "        mean_encoder_out = encoder_out.mean(dim=1)  # [batch_size, encoder_dim]\n",
    "        h1 = self.init_h1(mean_encoder_out)         # [batch_size, decoder_dim]\n",
    "        c1 = self.init_c1(mean_encoder_out)         # [batch_size, decoder_dim]\n",
    "        h2 = self.init_h2(mean_encoder_out)         # [batch_size, decoder_dim]\n",
    "        c2 = self.init_c2(mean_encoder_out)         # [batch_size, decoder_dim]\n",
    "        return (h1, c1, h2, c2)\n",
    "\n",
    "    def forward(self, encoder_out, encoded_captions, caption_lengths):\n",
    "        \"\"\"\n",
    "        encoder_out:      [batch_size, num_patches, encoder_dim]\n",
    "        encoded_captions: [batch_size, max_label_length]\n",
    "        caption_lengths:  [batch_size, 1]\n",
    "        \"\"\"\n",
    "        caption_lengths, sort_ind = caption_lengths.squeeze(1).sort(dim=0, descending=True)\n",
    "        encoder_out      = encoder_out[sort_ind]\n",
    "        encoded_captions = encoded_captions[sort_ind]\n",
    "\n",
    "        embeddings = self.embedding(encoded_captions)\n",
    "\n",
    "        # Initialize hidden states for both LSTM layers\n",
    "        h1, c1, h2, c2 = self.init_hidden_state(encoder_out)\n",
    "\n",
    "        decode_lengths    = (caption_lengths - 1).tolist()\n",
    "        max_decode_length = max(decode_lengths)\n",
    "\n",
    "        batch_size = encoder_out.size(0)\n",
    "        vocab_size = self.fc.out_features\n",
    "\n",
    "        predictions = torch.zeros(batch_size, max_decode_length, vocab_size, device=encoder_out.device)\n",
    "        alphas      = torch.zeros(batch_size, max_decode_length, encoder_out.size(1), device=encoder_out.device)\n",
    "\n",
    "        # We'll feed the first token from the input (<SOS>) or from the previous prediction\n",
    "        prev_tokens = encoded_captions[:, 0].clone()\n",
    "\n",
    "        for t in range(max_decode_length):\n",
    "            batch_size_t = sum([l > t for l in decode_lengths])\n",
    "\n",
    "            attention_weighted_encoding, alpha = self.attention(\n",
    "                encoder_out[:batch_size_t],\n",
    "                h1[:batch_size_t]  # use the first LSTM layer's hidden state for attention\n",
    "            )\n",
    "\n",
    "            # Apply gating\n",
    "            gate = self.sigmoid(self.f_beta(h1[:batch_size_t]))\n",
    "            attention_weighted_encoding = gate * attention_weighted_encoding\n",
    "\n",
    "            # Teacher forcing?\n",
    "            use_teacher_forcing = (torch.rand(1).item() < self.teacher_forcing_ratio)\n",
    "            if use_teacher_forcing:\n",
    "                current_input = embeddings[:batch_size_t, t, :]\n",
    "            else:\n",
    "                current_input = self.embedding(prev_tokens[:batch_size_t].detach())\n",
    "\n",
    "            # first lstm layer\n",
    "            h1_next, c1_next = self.lstm1(\n",
    "                torch.cat([current_input, attention_weighted_encoding], dim=1),\n",
    "                (h1[:batch_size_t], c1[:batch_size_t])\n",
    "            )\n",
    "\n",
    "            # second lstm layer\n",
    "            h2_next, c2_next = self.lstm2(\n",
    "                h1_next, (h2[:batch_size_t], c2[:batch_size_t])\n",
    "            )\n",
    "\n",
    "            # Use the second LSTM layer's output (h2_next) for final prediction\n",
    "            preds = self.fc(self.dropout(h2_next))\n",
    "            predictions[:batch_size_t, t, :] = preds\n",
    "            alphas[:batch_size_t, t, :]      = alpha\n",
    "\n",
    "            # Update prev_tokens with the best predicted token\n",
    "            _, next_tokens = preds.max(dim=1)\n",
    "            prev_tokens_ = prev_tokens.clone()\n",
    "            prev_tokens_[:batch_size_t] = next_tokens.detach()\n",
    "            prev_tokens = prev_tokens_\n",
    "\n",
    "            # Update hidden states\n",
    "            # For samples still in the batch, store the new h1, c1, h2, c2\n",
    "            h1_new = torch.zeros_like(h1)\n",
    "            c1_new = torch.zeros_like(c1)\n",
    "            h2_new = torch.zeros_like(h2)\n",
    "            c2_new = torch.zeros_like(c2)\n",
    "\n",
    "            h1_new[:batch_size_t] = h1_next\n",
    "            c1_new[:batch_size_t] = c1_next\n",
    "            h2_new[:batch_size_t] = h2_next\n",
    "            c2_new[:batch_size_t] = c2_next\n",
    "\n",
    "            h1, c1, h2, c2 = h1_new, c1_new, h2_new, c2_new\n",
    "\n",
    "        return predictions, encoded_captions, decode_lengths, alphas, sort_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c64f622d-9133-4436-b189-ae127e5afadf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageCaptioningTrainer:\n",
    "    def __init__(self, encoder, decoder, \n",
    "                 criterion, encoder_optimizer, decoder_optimizer, \n",
    "                 train_loader, val_loader, device, \n",
    "                 char_to_idx, idx_to_char, max_label_length,\n",
    "                 model_name, csv_filename=\"training_results.csv\"):\n",
    "        self.encoder = encoder.to(device)\n",
    "        self.decoder             = decoder.to(device)\n",
    "        self.criterion           = criterion\n",
    "        self.encoder_optimizer   = encoder_optimizer\n",
    "        self.decoder_optimizer   = decoder_optimizer\n",
    "        self.train_loader        = train_loader\n",
    "        self.val_loader          = val_loader\n",
    "        self.device              = device\n",
    "        self.char_to_idx         = char_to_idx\n",
    "        self.idx_to_char         = idx_to_char\n",
    "        self.max_label_length    = max_label_length\n",
    "        self.model_name = model_name\n",
    "        self.csv_filename = csv_filename\n",
    "\n",
    "        self.train_losses = []\n",
    "        self.val_losses   = []\n",
    "\n",
    "        self.train_cers   = []\n",
    "        self.val_cers     = []\n",
    "\n",
    "    def fit(self, num_epochs):\n",
    "        start_time = time.time()\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
    "            train_loss, train_cer = self.train_one_epoch()\n",
    "            val_loss,   val_cer   = self.validate_one_epoch(top_n=5)\n",
    "\n",
    "            print(f\"[{epoch+1}/{num_epochs}] \"\n",
    "                  f\"Train Loss: {train_loss:.4f}, Train CER: {train_cer:.4f} | \"\n",
    "                  f\"Val Loss: {val_loss:.4f}, Val CER: {val_cer:.4f}\")\n",
    "\n",
    "            # Store epoch results\n",
    "            self.train_losses.append(train_loss)\n",
    "            self.val_losses.append(val_loss)\n",
    "            self.train_cers.append(train_cer)\n",
    "            self.val_cers.append(val_cer)\n",
    "            \n",
    "        \n",
    "        # Calculate total training time\n",
    "        end_time = time.time() \n",
    "        total_time = end_time - start_time\n",
    "        hours = int(total_time // 3600)\n",
    "        minutes = int((total_time % 3600) // 60)\n",
    "\n",
    "        print(f\"\\nTraining completed in {hours}h {minutes}m.\")\n",
    "\n",
    "        num_epochs = len(self.train_losses)\n",
    "        epoch_cols = [f\"epoch{i+1}\" for i in range(num_epochs)]\n",
    "\n",
    "        df_wide = pd.DataFrame(columns=[\"model_name\", \"mode\"] + epoch_cols)\n",
    "        df_wide.loc[0] = [self.model_name, \"training loss\"] + self.train_losses\n",
    "        df_wide.loc[1] = [self.model_name, \"validation loss\"] + self.val_losses\n",
    "        df_wide.loc[2] = [self.model_name, \"training cer\"] + self.train_cers\n",
    "        df_wide.loc[3] = [self.model_name, \"validation cer\"] + self.val_cers\n",
    "\n",
    "        df_wide.to_csv(self.csv_filename, index=False)\n",
    "        print(f\"\\nResults have been written to: {self.csv_filename}\")\n",
    "\n",
    "        # Save model weights\n",
    "        torch.save(self.encoder.state_dict(), f\"encoder_{self.model_name}.pth\")\n",
    "        torch.save(self.decoder.state_dict(), f\"decoder_{self.model_name}.pth\")\n",
    "        print(f\"Encoder and decoder models saved: encoder_{self.model_name}.pth, decoder_{self.model_name}.pth\")\n",
    "        \n",
    "\n",
    "    def train_one_epoch(self):\n",
    "        self.encoder.train()\n",
    "        self.decoder.train()\n",
    "        running_loss           = 0.0\n",
    "        total_edit_distance    = 0\n",
    "        total_ref_length       = 0\n",
    "\n",
    "        for batch_idx, (images, labels, label_lengths) in enumerate(self.train_loader):\n",
    "            images        = images.to(self.device, non_blocking=True)\n",
    "            labels        = labels.to(self.device, non_blocking=True)\n",
    "            label_lengths = label_lengths.to(self.device, non_blocking=True)\n",
    "\n",
    "            self.encoder_optimizer.zero_grad()\n",
    "            self.decoder_optimizer.zero_grad()\n",
    "\n",
    "            encoder_out   = self.encoder(images)\n",
    "            caption_lengths = torch.tensor(\n",
    "                [self.max_label_length] * labels.size(0)\n",
    "            ).unsqueeze(1).to(self.device)\n",
    "\n",
    "            outputs, encoded_captions, decode_lengths, alphas, sort_ind = self.decoder(\n",
    "                encoder_out, labels, caption_lengths\n",
    "            )\n",
    "\n",
    "            # Targets = encoded captions without the <SOS>\n",
    "            targets = encoded_captions[:, 1:]\n",
    "\n",
    "            # Flatten for loss\n",
    "            outputs_flat = outputs.view(-1, self.decoder.fc.out_features)\n",
    "            targets_flat = targets.contiguous().view(-1)\n",
    "\n",
    "            loss = self.criterion(outputs_flat, targets_flat)\n",
    "            loss.backward()\n",
    "\n",
    "            self.decoder_optimizer.step()\n",
    "            self.encoder_optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            # Compute CER for the batch (global style)\n",
    "            batch_size = labels.size(0)\n",
    "            _, preds_flat = torch.max(outputs_flat, dim=1)\n",
    "            preds_seq = preds_flat.view(batch_size, -1)\n",
    "\n",
    "            for i in range(batch_size):\n",
    "                pred_indices   = preds_seq[i].detach().cpu().numpy()\n",
    "                target_indices = targets[i].detach().cpu().numpy()\n",
    "\n",
    "                mask          = (target_indices != self.char_to_idx['<PAD>'])\n",
    "                pred_indices  = pred_indices[mask]\n",
    "                target_indices= target_indices[mask]\n",
    "\n",
    "                pred_chars    = [self.idx_to_char.get(idx, '') for idx in pred_indices]\n",
    "                target_chars  = [self.idx_to_char.get(idx, '') for idx in target_indices]\n",
    "                pred_str      = ''.join(pred_chars)\n",
    "                target_str    = ''.join(target_chars)\n",
    "\n",
    "                edit_dist           = editdistance.eval(pred_str, target_str)\n",
    "                total_edit_distance += edit_dist\n",
    "                total_ref_length    += len(target_str)\n",
    "\n",
    "            if (batch_idx + 1) % 50 == 0:\n",
    "                print(f'Batch {batch_idx + 1}/{len(self.train_loader)} - Loss: {loss.item():.4f}')\n",
    "\n",
    "        avg_loss = running_loss / len(self.train_loader)\n",
    "        avg_cer  = total_edit_distance / total_ref_length if total_ref_length > 0 else 0.0\n",
    "        return avg_loss, avg_cer\n",
    "\n",
    "    def validate_one_epoch(self, top_n=5):\n",
    "        self.encoder.eval()\n",
    "        self.decoder.eval()\n",
    "        running_loss         = 0.0\n",
    "        total_edit_distance  = 0\n",
    "        total_ref_length     = 0\n",
    "\n",
    "        # each sampleâ€™s CER\n",
    "        sample_cer_info = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (images, labels, label_lengths) in enumerate(self.val_loader):\n",
    "                images        = images.to(self.device, non_blocking=True)\n",
    "                labels        = labels.to(self.device, non_blocking=True)\n",
    "                label_lengths = label_lengths.to(self.device, non_blocking=True)\n",
    "\n",
    "                encoder_out = self.encoder(images)\n",
    "                caption_lengths = torch.tensor(\n",
    "                    [self.max_label_length] * labels.size(0)\n",
    "                ).unsqueeze(1).to(self.device)\n",
    "\n",
    "                outputs, encoded_captions, decode_lengths, alphas, sort_ind = self.decoder(\n",
    "                    encoder_out, labels, caption_lengths\n",
    "                )\n",
    "                targets = encoded_captions[:, 1:]\n",
    "\n",
    "                outputs_flat = outputs.view(-1, self.decoder.fc.out_features)\n",
    "                targets_flat = targets.contiguous().view(-1)\n",
    "\n",
    "                loss = self.criterion(outputs_flat, targets_flat)\n",
    "                running_loss += loss.item()\n",
    "\n",
    "                batch_size = labels.size(0)\n",
    "                _, preds_flat = torch.max(outputs_flat, dim=1)\n",
    "                preds_seq = preds_flat.view(batch_size, -1)\n",
    "\n",
    "                for i in range(batch_size):\n",
    "                    pred_indices   = preds_seq[i].detach().cpu().numpy()\n",
    "                    target_indices = targets[i].detach().cpu().numpy()\n",
    "\n",
    "                    mask           = (target_indices != self.char_to_idx['<PAD>'])\n",
    "                    pred_indices   = pred_indices[mask]\n",
    "                    target_indices = target_indices[mask]\n",
    "\n",
    "                    pred_chars   = [self.idx_to_char.get(idx, '') for idx in pred_indices]\n",
    "                    target_chars = [self.idx_to_char.get(idx, '') for idx in target_indices]\n",
    "                    pred_str     = ''.join(pred_chars)\n",
    "                    target_str   = ''.join(target_chars)\n",
    "\n",
    "                    edit_dist = editdistance.eval(pred_str, target_str)\n",
    "                    ref_len   = len(target_str)\n",
    "                    cer       = edit_dist / ref_len if ref_len > 0 else 0\n",
    "    \n",
    "                    total_edit_distance += edit_dist\n",
    "                    total_ref_length    += ref_len\n",
    "    \n",
    "                    # Store sample info\n",
    "                    sample_cer_info.append({\n",
    "                        \"pred\": pred_str,\n",
    "                        \"gt\": target_str,\n",
    "                        \"cer\": cer\n",
    "                    })\n",
    "\n",
    "                    # Print a few samples from the 1st batch\n",
    "                    # if batch_idx == 0 and i < 3:\n",
    "                    #     print(f\"Sample {i + 1}:\")\n",
    "                    #     print(f\"Predicted: {pred_str}\")\n",
    "                    #     print(f\"Target   : {target_str}\\n\")\n",
    "\n",
    "        avg_loss = running_loss / len(self.val_loader)\n",
    "        avg_cer  = total_edit_distance / total_ref_length if total_ref_length > 0 else 0.0\n",
    "\n",
    "        # Sort by CER descending\n",
    "        sample_cer_info.sort(key=lambda x: x[\"cer\"], reverse=True)\n",
    "        # Take top_n\n",
    "        worst_samples = sample_cer_info[:top_n]\n",
    "    \n",
    "        print(f\"\\n=== Top {top_n} Worst Samples by CER ===\")\n",
    "        for idx, sample in enumerate(worst_samples):\n",
    "            print(f\"[{idx+1}] CER: {sample['cer']:.3f}\")\n",
    "            print(f\"   Predicted: {sample['pred']}\")\n",
    "            print(f\"   Ground Truth: {sample['gt']}\\n\")\n",
    "       \n",
    "        return avg_loss, avg_cer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5ba4712d-3ce3-49af-b8e2-f45f1ac80cf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/apps/miniforge3/24.3.0-0/miniforge3/envs/dsks_2024.06/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/apps/miniforge3/24.3.0-0/miniforge3/envs/dsks_2024.06/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/20\n",
      "Batch 50/437 - Loss: 2.3589\n",
      "Batch 100/437 - Loss: 2.2616\n",
      "Batch 150/437 - Loss: 2.1788\n",
      "Batch 200/437 - Loss: 1.9089\n",
      "Batch 250/437 - Loss: 1.6270\n",
      "Batch 300/437 - Loss: 1.7671\n",
      "Batch 350/437 - Loss: 1.4903\n",
      "Batch 400/437 - Loss: 1.6171\n",
      "\n",
      "=== Top 5 Worst Samples by CER ===\n",
      "[1] CER: 2.100\n",
      "   Predicted: .<EOS><EOS><EOS><EOS><EOS>\n",
      "   Ground Truth: panti<EOS>\n",
      "\n",
      "[2] CER: 1.750\n",
      "   Predicted: men<EOS><EOS><EOS><EOS><EOS>\n",
      "   Ground Truth: rendang<EOS>\n",
      "\n",
      "[3] CER: 1.417\n",
      "   Predicted: nana<EOS><EOS><EOS><EOS>\n",
      "   Ground Truth: tanhana<EOS>\n",
      "\n",
      "[4] CER: 1.357\n",
      "   Predicted: rengaa<EOS><EOS><EOS><EOS>\n",
      "   Ground Truth: sangkeria<EOS>\n",
      "\n",
      "[5] CER: 1.333\n",
      "   Predicted: ng<EOS><EOS><EOS>\n",
      "   Ground Truth: ring<EOS>\n",
      "\n",
      "[1/20] Train Loss: 1.9360, Train CER: 0.6483 | Val Loss: 1.0881, Val CER: 0.2709\n",
      "\n",
      "Epoch 2/20\n",
      "Batch 50/437 - Loss: 1.3167\n",
      "Batch 100/437 - Loss: 1.2486\n",
      "Batch 150/437 - Loss: 1.5137\n",
      "Batch 200/437 - Loss: 0.9759\n",
      "Batch 250/437 - Loss: 1.2014\n",
      "Batch 300/437 - Loss: 1.0807\n",
      "Batch 350/437 - Loss: 1.1580\n",
      "Batch 400/437 - Loss: 0.8716\n",
      "\n",
      "=== Top 5 Worst Samples by CER ===\n",
      "[1] CER: 2.100\n",
      "   Predicted: .<EOS><EOS><EOS><EOS><EOS>\n",
      "   Ground Truth: panti<EOS>\n",
      "\n",
      "[2] CER: 1.545\n",
      "   Predicted: enn<EOS><EOS><EOS><EOS>\n",
      "   Ground Truth: tengen<EOS>\n",
      "\n",
      "[3] CER: 1.417\n",
      "   Predicted: rana<EOS><EOS><EOS><EOS>\n",
      "   Ground Truth: tanhana<EOS>\n",
      "\n",
      "[4] CER: 1.417\n",
      "   Predicted: gena<EOS><EOS><EOS><EOS>\n",
      "   Ground Truth: jenyana<EOS>\n",
      "\n",
      "[5] CER: 1.364\n",
      "   Predicted: pra<EOS><EOS><EOS><EOS>\n",
      "   Ground Truth: prabhu<EOS>\n",
      "\n",
      "[2/20] Train Loss: 1.1123, Train CER: 0.3131 | Val Loss: 0.5924, Val CER: 0.1633\n",
      "\n",
      "Epoch 3/20\n",
      "Batch 50/437 - Loss: 0.6489\n",
      "Batch 100/437 - Loss: 0.7966\n",
      "Batch 150/437 - Loss: 0.7014\n",
      "Batch 200/437 - Loss: 0.4623\n",
      "Batch 250/437 - Loss: 0.2499\n",
      "Batch 300/437 - Loss: 0.4924\n",
      "Batch 350/437 - Loss: 0.7456\n",
      "Batch 400/437 - Loss: 0.6753\n",
      "\n",
      "=== Top 5 Worst Samples by CER ===\n",
      "[1] CER: 2.100\n",
      "   Predicted: .<EOS><EOS><EOS><EOS><EOS>\n",
      "   Ground Truth: panti<EOS>\n",
      "\n",
      "[2] CER: 1.700\n",
      "   Predicted: an<EOS><EOS><EOS><EOS>\n",
      "   Ground Truth: sinta<EOS>\n",
      "\n",
      "[3] CER: 1.455\n",
      "   Predicted: een<EOS><EOS><EOS><EOS>\n",
      "   Ground Truth: tengen<EOS>\n",
      "\n",
      "[4] CER: 1.375\n",
      "   Predicted: u<EOS><EOS><EOS>\n",
      "   Ground Truth: wre<EOS>\n",
      "\n",
      "[5] CER: 1.333\n",
      "   Predicted: ki<EOS><EOS><EOS>\n",
      "   Ground Truth: nang<EOS>\n",
      "\n",
      "[3/20] Train Loss: 0.7051, Train CER: 0.1860 | Val Loss: 0.4394, Val CER: 0.1239\n",
      "\n",
      "Epoch 4/20\n",
      "Batch 50/437 - Loss: 0.6432\n",
      "Batch 100/437 - Loss: 0.2958\n",
      "Batch 150/437 - Loss: 0.6381\n",
      "Batch 200/437 - Loss: 0.3648\n",
      "Batch 250/437 - Loss: 0.3383\n",
      "Batch 300/437 - Loss: 0.5180\n",
      "Batch 350/437 - Loss: 0.5220\n",
      "Batch 400/437 - Loss: 0.3741\n",
      "\n",
      "=== Top 5 Worst Samples by CER ===\n",
      "[1] CER: 1.300\n",
      "   Predicted: .<EOS><EOS>ga<EOS>\n",
      "   Ground Truth: panti<EOS>\n",
      "\n",
      "[2] CER: 1.083\n",
      "   Predicted: dada<EOS>i<EOS><EOS>\n",
      "   Ground Truth: bhatari<EOS>\n",
      "\n",
      "[3] CER: 1.000\n",
      "   Predicted: was\n",
      "   Ground Truth: di<EOS>\n",
      "\n",
      "[4] CER: 1.000\n",
      "   Predicted: ka\n",
      "   Ground Truth: n<EOS>\n",
      "\n",
      "[5] CER: 1.000\n",
      "   Predicted: ng\n",
      "   Ground Truth: 2<EOS>\n",
      "\n",
      "[4/20] Train Loss: 0.4883, Train CER: 0.1245 | Val Loss: 0.3650, Val CER: 0.0934\n",
      "\n",
      "Epoch 5/20\n",
      "Batch 50/437 - Loss: 0.3365\n",
      "Batch 100/437 - Loss: 0.2227\n",
      "Batch 150/437 - Loss: 0.3117\n",
      "Batch 200/437 - Loss: 0.2211\n",
      "Batch 250/437 - Loss: 0.1441\n",
      "Batch 300/437 - Loss: 0.6900\n",
      "Batch 350/437 - Loss: 0.2658\n",
      "Batch 400/437 - Loss: 0.2288\n",
      "\n",
      "=== Top 5 Worst Samples by CER ===\n",
      "[1] CER: 1.600\n",
      "   Predicted: .a<EOS><EOS><EOS><EOS>\n",
      "   Ground Truth: panti<EOS>\n",
      "\n",
      "[2] CER: 1.182\n",
      "   Predicted: ri<EOS>g<EOS>g<EOS>\n",
      "   Ground Truth: sareng<EOS>\n",
      "\n",
      "[3] CER: 1.100\n",
      "   Predicted: ape<EOS><EOS><EOS>\n",
      "   Ground Truth: areng<EOS>\n",
      "\n",
      "[4] CER: 1.000\n",
      "   Predicted: lw\n",
      "   Ground Truth: 3<EOS>\n",
      "\n",
      "[5] CER: 1.000\n",
      "   Predicted: pi\n",
      "   Ground Truth: i<EOS>\n",
      "\n",
      "[5/20] Train Loss: 0.3526, Train CER: 0.0892 | Val Loss: 0.3550, Val CER: 0.0922\n",
      "\n",
      "Epoch 6/20\n",
      "Batch 50/437 - Loss: 0.3128\n",
      "Batch 100/437 - Loss: 0.3352\n",
      "Batch 150/437 - Loss: 0.2548\n",
      "Batch 200/437 - Loss: 0.0968\n",
      "Batch 250/437 - Loss: 0.2195\n",
      "Batch 300/437 - Loss: 0.2491\n",
      "Batch 350/437 - Loss: 0.2232\n",
      "Batch 400/437 - Loss: 0.2747\n",
      "\n",
      "=== Top 5 Worst Samples by CER ===\n",
      "[1] CER: 1.100\n",
      "   Predicted: san<EOS><EOS><EOS>\n",
      "   Ground Truth: sinta<EOS>\n",
      "\n",
      "[2] CER: 1.100\n",
      "   Predicted: .an<EOS><EOS><EOS>\n",
      "   Ground Truth: panti<EOS>\n",
      "\n",
      "[3] CER: 1.000\n",
      "   Predicted: ri\n",
      "   Ground Truth: i<EOS>\n",
      "\n",
      "[4] CER: 1.000\n",
      "   Predicted: reta\n",
      "   Ground Truth: sah<EOS>\n",
      "\n",
      "[5] CER: 1.000\n",
      "   Predicted: sesto\n",
      "   Ground Truth: tong<EOS>\n",
      "\n",
      "[6/20] Train Loss: 0.2598, Train CER: 0.0664 | Val Loss: 0.3562, Val CER: 0.0862\n",
      "\n",
      "Epoch 7/20\n",
      "Batch 50/437 - Loss: 0.1404\n",
      "Batch 100/437 - Loss: 0.2715\n",
      "Batch 150/437 - Loss: 0.1445\n",
      "Batch 200/437 - Loss: 0.2867\n",
      "Batch 250/437 - Loss: 0.2079\n",
      "Batch 300/437 - Loss: 0.1217\n",
      "Batch 350/437 - Loss: 0.1177\n",
      "Batch 400/437 - Loss: 0.2756\n",
      "\n",
      "=== Top 5 Worst Samples by CER ===\n",
      "[1] CER: 2.100\n",
      "   Predicted: .<EOS><EOS><EOS><EOS><EOS>\n",
      "   Ground Truth: panti<EOS>\n",
      "\n",
      "[2] CER: 1.375\n",
      "   Predicted: 3<EOS><EOS><EOS>\n",
      "   Ground Truth: ong<EOS>\n",
      "\n",
      "[3] CER: 1.222\n",
      "   Predicted: ka<EOS><EOS><EOS>\n",
      "   Ground Truth: nang<EOS>\n",
      "\n",
      "[4] CER: 1.222\n",
      "   Predicted: ra<EOS><EOS><EOS>\n",
      "   Ground Truth: ring<EOS>\n",
      "\n",
      "[5] CER: 1.222\n",
      "   Predicted: i<EOS><EOS>g<EOS>\n",
      "   Ground Truth: ring<EOS>\n",
      "\n",
      "[7/20] Train Loss: 0.2017, Train CER: 0.0498 | Val Loss: 0.3364, Val CER: 0.0882\n",
      "\n",
      "Epoch 8/20\n",
      "Batch 50/437 - Loss: 0.0911\n",
      "Batch 100/437 - Loss: 0.0543\n",
      "Batch 150/437 - Loss: 0.1339\n",
      "Batch 200/437 - Loss: 0.0881\n",
      "Batch 250/437 - Loss: 0.1411\n",
      "Batch 300/437 - Loss: 0.1143\n",
      "Batch 350/437 - Loss: 0.1833\n",
      "Batch 400/437 - Loss: 0.1845\n",
      "\n",
      "=== Top 5 Worst Samples by CER ===\n",
      "[1] CER: 1.700\n",
      "   Predicted: .<EOS><EOS><EOS>e<EOS>\n",
      "   Ground Truth: panti<EOS>\n",
      "\n",
      "[2] CER: 1.000\n",
      "   Predicted: nga\n",
      "   Ground Truth: la<EOS>\n",
      "\n",
      "[3] CER: 1.000\n",
      "   Predicted: was\n",
      "   Ground Truth: di<EOS>\n",
      "\n",
      "[4] CER: 1.000\n",
      "   Predicted: ca\n",
      "   Ground Truth: n<EOS>\n",
      "\n",
      "[5] CER: 1.000\n",
      "   Predicted: dn\n",
      "   Ground Truth: 2<EOS>\n",
      "\n",
      "[8/20] Train Loss: 0.1588, Train CER: 0.0409 | Val Loss: 0.4156, Val CER: 0.0999\n",
      "\n",
      "Epoch 9/20\n",
      "Batch 50/437 - Loss: 0.0957\n",
      "Batch 100/437 - Loss: 0.3123\n",
      "Batch 150/437 - Loss: 0.0880\n",
      "Batch 200/437 - Loss: 0.0817\n",
      "Batch 250/437 - Loss: 0.0441\n",
      "Batch 300/437 - Loss: 0.1420\n",
      "Batch 350/437 - Loss: 0.0485\n",
      "Batch 400/437 - Loss: 0.1453\n",
      "\n",
      "=== Top 5 Worst Samples by CER ===\n",
      "[1] CER: 2.100\n",
      "   Predicted: .<EOS><EOS><EOS><EOS><EOS>\n",
      "   Ground Truth: panti<EOS>\n",
      "\n",
      "[2] CER: 1.545\n",
      "   Predicted: re<EOS><EOS><EOS>g<EOS>\n",
      "   Ground Truth: sareng<EOS>\n",
      "\n",
      "[3] CER: 1.200\n",
      "   Predicted: kan<EOS><EOS><EOS>\n",
      "   Ground Truth: sinta<EOS>\n",
      "\n",
      "[4] CER: 1.167\n",
      "   Predicted: dadai<EOS><EOS><EOS>\n",
      "   Ground Truth: bhatari<EOS>\n",
      "\n",
      "[5] CER: 1.000\n",
      "   Predicted: ri\n",
      "   Ground Truth: i<EOS>\n",
      "\n",
      "[9/20] Train Loss: 0.1342, Train CER: 0.0348 | Val Loss: 0.3521, Val CER: 0.0796\n",
      "\n",
      "Epoch 10/20\n",
      "Batch 50/437 - Loss: 0.0246\n",
      "Batch 100/437 - Loss: 0.0524\n",
      "Batch 150/437 - Loss: 0.1764\n",
      "Batch 200/437 - Loss: 0.2015\n",
      "Batch 250/437 - Loss: 0.0977\n",
      "Batch 300/437 - Loss: 0.0935\n",
      "Batch 350/437 - Loss: 0.1181\n",
      "Batch 400/437 - Loss: 0.0823\n",
      "\n",
      "=== Top 5 Worst Samples by CER ===\n",
      "[1] CER: 2.100\n",
      "   Predicted: .<EOS><EOS><EOS><EOS><EOS>\n",
      "   Ground Truth: panti<EOS>\n",
      "\n",
      "[2] CER: 1.100\n",
      "   Predicted: ira<EOS><EOS><EOS>\n",
      "   Ground Truth: irung<EOS>\n",
      "\n",
      "[3] CER: 1.000\n",
      "   Predicted: pi\n",
      "   Ground Truth: i<EOS>\n",
      "\n",
      "[4] CER: 1.000\n",
      "   Predicted: weta\n",
      "   Ground Truth: sah<EOS>\n",
      "\n",
      "[5] CER: 1.000\n",
      "   Predicted: sesto\n",
      "   Ground Truth: tong<EOS>\n",
      "\n",
      "[10/20] Train Loss: 0.0944, Train CER: 0.0240 | Val Loss: 0.4394, Val CER: 0.1039\n",
      "\n",
      "Epoch 11/20\n",
      "Batch 50/437 - Loss: 0.0379\n",
      "Batch 100/437 - Loss: 0.2358\n",
      "Batch 150/437 - Loss: 0.0864\n",
      "Batch 200/437 - Loss: 0.0196\n",
      "Batch 250/437 - Loss: 0.0430\n",
      "Batch 300/437 - Loss: 0.0385\n",
      "Batch 350/437 - Loss: 0.2475\n",
      "Batch 400/437 - Loss: 0.2308\n",
      "\n",
      "=== Top 5 Worst Samples by CER ===\n",
      "[1] CER: 2.100\n",
      "   Predicted: .<EOS><EOS><EOS><EOS><EOS>\n",
      "   Ground Truth: panti<EOS>\n",
      "\n",
      "[2] CER: 1.500\n",
      "   Predicted: anak<EOS><EOS><EOS><EOS>\n",
      "   Ground Truth: tanhana<EOS>\n",
      "\n",
      "[3] CER: 1.250\n",
      "   Predicted: daddi<EOS><EOS><EOS>\n",
      "   Ground Truth: bhatari<EOS>\n",
      "\n",
      "[4] CER: 1.200\n",
      "   Predicted: ump<EOS><EOS><EOS>\n",
      "   Ground Truth: utama<EOS>\n",
      "\n",
      "[5] CER: 1.200\n",
      "   Predicted: aan<EOS><EOS><EOS>\n",
      "   Ground Truth: sinta<EOS>\n",
      "\n",
      "[11/20] Train Loss: 0.1073, Train CER: 0.0277 | Val Loss: 0.4501, Val CER: 0.0894\n",
      "\n",
      "Epoch 12/20\n",
      "Batch 50/437 - Loss: 0.1170\n",
      "Batch 100/437 - Loss: 0.0650\n",
      "Batch 150/437 - Loss: 0.0557\n",
      "Batch 200/437 - Loss: 0.0214\n",
      "Batch 250/437 - Loss: 0.1635\n",
      "Batch 300/437 - Loss: 0.0488\n",
      "Batch 350/437 - Loss: 0.0758\n",
      "Batch 400/437 - Loss: 0.0294\n",
      "\n",
      "=== Top 5 Worst Samples by CER ===\n",
      "[1] CER: 2.100\n",
      "   Predicted: .<EOS><EOS><EOS><EOS><EOS>\n",
      "   Ground Truth: panti<EOS>\n",
      "\n",
      "[2] CER: 1.222\n",
      "   Predicted: ka<EOS><EOS><EOS>\n",
      "   Ground Truth: nang<EOS>\n",
      "\n",
      "[3] CER: 1.222\n",
      "   Predicted: ku<EOS><EOS><EOS>\n",
      "   Ground Truth: buta<EOS>\n",
      "\n",
      "[4] CER: 1.222\n",
      "   Predicted: ja<EOS><EOS><EOS>\n",
      "   Ground Truth: sang<EOS>\n",
      "\n",
      "[5] CER: 1.111\n",
      "   Predicted: ri<EOS><EOS><EOS>\n",
      "   Ground Truth: ring<EOS>\n",
      "\n",
      "[12/20] Train Loss: 0.0883, Train CER: 0.0212 | Val Loss: 0.3962, Val CER: 0.0884\n",
      "\n",
      "Epoch 13/20\n",
      "Batch 50/437 - Loss: 0.0760\n",
      "Batch 100/437 - Loss: 0.3909\n",
      "Batch 150/437 - Loss: 0.0365\n",
      "Batch 200/437 - Loss: 0.0822\n",
      "Batch 250/437 - Loss: 0.0727\n",
      "Batch 300/437 - Loss: 0.0575\n",
      "Batch 350/437 - Loss: 0.0882\n",
      "Batch 400/437 - Loss: 0.0722\n",
      "\n",
      "=== Top 5 Worst Samples by CER ===\n",
      "[1] CER: 2.100\n",
      "   Predicted: .<EOS><EOS><EOS><EOS><EOS>\n",
      "   Ground Truth: panti<EOS>\n",
      "\n",
      "[2] CER: 1.375\n",
      "   Predicted: ,<EOS><EOS><EOS>\n",
      "   Ground Truth: nga<EOS>\n",
      "\n",
      "[3] CER: 1.222\n",
      "   Predicted: ja<EOS><EOS><EOS>\n",
      "   Ground Truth: sang<EOS>\n",
      "\n",
      "[4] CER: 1.167\n",
      "   Predicted: dadai<EOS><EOS><EOS>\n",
      "   Ground Truth: bhatari<EOS>\n",
      "\n",
      "[5] CER: 1.100\n",
      "   Predicted: ira<EOS><EOS><EOS>\n",
      "   Ground Truth: irung<EOS>\n",
      "\n",
      "[13/20] Train Loss: 0.0937, Train CER: 0.0241 | Val Loss: 0.4548, Val CER: 0.0924\n",
      "\n",
      "Epoch 14/20\n",
      "Batch 50/437 - Loss: 0.0827\n",
      "Batch 100/437 - Loss: 0.1949\n",
      "Batch 150/437 - Loss: 0.0251\n",
      "Batch 200/437 - Loss: 0.3032\n",
      "Batch 250/437 - Loss: 0.0137\n",
      "Batch 300/437 - Loss: 0.1971\n",
      "Batch 350/437 - Loss: 0.0227\n",
      "Batch 400/437 - Loss: 0.0361\n",
      "\n",
      "=== Top 5 Worst Samples by CER ===\n",
      "[1] CER: 2.100\n",
      "   Predicted: .<EOS><EOS><EOS><EOS><EOS>\n",
      "   Ground Truth: panti<EOS>\n",
      "\n",
      "[2] CER: 1.545\n",
      "   Predicted: re<EOS><EOS><EOS>g<EOS>\n",
      "   Ground Truth: sareng<EOS>\n",
      "\n",
      "[3] CER: 1.375\n",
      "   Predicted: ,<EOS><EOS><EOS>\n",
      "   Ground Truth: nga<EOS>\n",
      "\n",
      "[4] CER: 1.222\n",
      "   Predicted: ra<EOS><EOS><EOS>\n",
      "   Ground Truth: ring<EOS>\n",
      "\n",
      "[5] CER: 1.222\n",
      "   Predicted: ja<EOS><EOS><EOS>\n",
      "   Ground Truth: sang<EOS>\n",
      "\n",
      "[14/20] Train Loss: 0.0731, Train CER: 0.0182 | Val Loss: 0.3915, Val CER: 0.0738\n",
      "\n",
      "Epoch 15/20\n",
      "Batch 50/437 - Loss: 0.1156\n",
      "Batch 100/437 - Loss: 0.0159\n",
      "Batch 150/437 - Loss: 0.0796\n",
      "Batch 200/437 - Loss: 0.1374\n",
      "Batch 250/437 - Loss: 0.0115\n",
      "Batch 300/437 - Loss: 0.0706\n",
      "Batch 350/437 - Loss: 0.0616\n",
      "Batch 400/437 - Loss: 0.0587\n",
      "\n",
      "=== Top 5 Worst Samples by CER ===\n",
      "[1] CER: 2.100\n",
      "   Predicted: .<EOS><EOS><EOS><EOS><EOS>\n",
      "   Ground Truth: panti<EOS>\n",
      "\n",
      "[2] CER: 1.222\n",
      "   Predicted: ra<EOS><EOS><EOS>\n",
      "   Ground Truth: ring<EOS>\n",
      "\n",
      "[3] CER: 1.200\n",
      "   Predicted: wma<EOS><EOS><EOS>\n",
      "   Ground Truth: utama<EOS>\n",
      "\n",
      "[4] CER: 1.200\n",
      "   Predicted: sad<EOS><EOS><EOS>\n",
      "   Ground Truth: sinta<EOS>\n",
      "\n",
      "[5] CER: 1.083\n",
      "   Predicted: dada<EOS><EOS>i<EOS>\n",
      "   Ground Truth: bhatari<EOS>\n",
      "\n",
      "[15/20] Train Loss: 0.0617, Train CER: 0.0141 | Val Loss: 0.3579, Val CER: 0.0739\n",
      "\n",
      "Epoch 16/20\n",
      "Batch 50/437 - Loss: 0.1214\n",
      "Batch 100/437 - Loss: 0.0332\n",
      "Batch 150/437 - Loss: 0.0955\n",
      "Batch 200/437 - Loss: 0.0331\n",
      "Batch 250/437 - Loss: 0.0256\n",
      "Batch 300/437 - Loss: 0.0895\n",
      "Batch 350/437 - Loss: 0.0301\n",
      "Batch 400/437 - Loss: 0.0918\n",
      "\n",
      "=== Top 5 Worst Samples by CER ===\n",
      "[1] CER: 1.909\n",
      "   Predicted: si<EOS><EOS><EOS><EOS><EOS>\n",
      "   Ground Truth: sareng<EOS>\n",
      "\n",
      "[2] CER: 1.333\n",
      "   Predicted: ke<EOS><EOS><EOS>\n",
      "   Ground Truth: nang<EOS>\n",
      "\n",
      "[3] CER: 1.222\n",
      "   Predicted: ra<EOS><EOS><EOS>\n",
      "   Ground Truth: ring<EOS>\n",
      "\n",
      "[4] CER: 1.100\n",
      "   Predicted: ira<EOS><EOS><EOS>\n",
      "   Ground Truth: irung<EOS>\n",
      "\n",
      "[5] CER: 1.000\n",
      "   Predicted: nga\n",
      "   Ground Truth: la<EOS>\n",
      "\n",
      "[16/20] Train Loss: 0.0551, Train CER: 0.0142 | Val Loss: 0.4025, Val CER: 0.0801\n",
      "\n",
      "Epoch 17/20\n",
      "Batch 50/437 - Loss: 0.0176\n",
      "Batch 100/437 - Loss: 0.1060\n",
      "Batch 150/437 - Loss: 0.0187\n",
      "Batch 200/437 - Loss: 0.0076\n",
      "Batch 250/437 - Loss: 0.0576\n",
      "Batch 300/437 - Loss: 0.0249\n",
      "Batch 350/437 - Loss: 0.0560\n",
      "Batch 400/437 - Loss: 0.0218\n",
      "\n",
      "=== Top 5 Worst Samples by CER ===\n",
      "[1] CER: 1.500\n",
      "   Predicted: ddad<EOS><EOS><EOS><EOS>\n",
      "   Ground Truth: bhatari<EOS>\n",
      "\n",
      "[2] CER: 1.333\n",
      "   Predicted: na<EOS><EOS><EOS>\n",
      "   Ground Truth: ring<EOS>\n",
      "\n",
      "[3] CER: 1.200\n",
      "   Predicted: ump<EOS><EOS><EOS>\n",
      "   Ground Truth: utama<EOS>\n",
      "\n",
      "[4] CER: 1.200\n",
      "   Predicted: iin<EOS><EOS><EOS>\n",
      "   Ground Truth: irung<EOS>\n",
      "\n",
      "[5] CER: 1.100\n",
      "   Predicted: san<EOS><EOS><EOS>\n",
      "   Ground Truth: sinta<EOS>\n",
      "\n",
      "[17/20] Train Loss: 0.0603, Train CER: 0.0151 | Val Loss: 0.3919, Val CER: 0.0737\n",
      "\n",
      "Epoch 18/20\n",
      "Batch 50/437 - Loss: 0.0125\n",
      "Batch 100/437 - Loss: 0.2192\n",
      "Batch 150/437 - Loss: 0.0405\n",
      "Batch 200/437 - Loss: 0.0484\n",
      "Batch 250/437 - Loss: 0.0129\n",
      "Batch 300/437 - Loss: 0.0687\n",
      "Batch 350/437 - Loss: 0.0222\n",
      "Batch 400/437 - Loss: 0.0294\n",
      "\n",
      "=== Top 5 Worst Samples by CER ===\n",
      "[1] CER: 1.700\n",
      "   Predicted: .<EOS><EOS><EOS>e<EOS>\n",
      "   Ground Truth: panti<EOS>\n",
      "\n",
      "[2] CER: 1.300\n",
      "   Predicted: wmm<EOS><EOS><EOS>\n",
      "   Ground Truth: utama<EOS>\n",
      "\n",
      "[3] CER: 1.091\n",
      "   Predicted: sru<EOS><EOS>g<EOS>\n",
      "   Ground Truth: sareng<EOS>\n",
      "\n",
      "[4] CER: 1.000\n",
      "   Predicted: ri\n",
      "   Ground Truth: i<EOS>\n",
      "\n",
      "[5] CER: 1.000\n",
      "   Predicted: wad\n",
      "   Ground Truth: di<EOS>\n",
      "\n",
      "[18/20] Train Loss: 0.0568, Train CER: 0.0140 | Val Loss: 0.3957, Val CER: 0.0758\n",
      "\n",
      "Epoch 19/20\n",
      "Batch 50/437 - Loss: 0.0783\n",
      "Batch 100/437 - Loss: 0.1673\n",
      "Batch 150/437 - Loss: 0.1464\n",
      "Batch 200/437 - Loss: 0.0678\n",
      "Batch 250/437 - Loss: 0.0177\n",
      "Batch 300/437 - Loss: 0.0552\n",
      "Batch 350/437 - Loss: 0.0241\n",
      "Batch 400/437 - Loss: 0.0200\n",
      "\n",
      "=== Top 5 Worst Samples by CER ===\n",
      "[1] CER: 1.909\n",
      "   Predicted: se<EOS><EOS><EOS><EOS><EOS>\n",
      "   Ground Truth: sareng<EOS>\n",
      "\n",
      "[2] CER: 1.222\n",
      "   Predicted: ra<EOS><EOS><EOS>\n",
      "   Ground Truth: ring<EOS>\n",
      "\n",
      "[3] CER: 1.200\n",
      "   Predicted: .a<EOS>e<EOS><EOS>\n",
      "   Ground Truth: panti<EOS>\n",
      "\n",
      "[4] CER: 1.167\n",
      "   Predicted: dadad<EOS><EOS><EOS>\n",
      "   Ground Truth: bhatari<EOS>\n",
      "\n",
      "[5] CER: 1.100\n",
      "   Predicted: uma<EOS><EOS><EOS>\n",
      "   Ground Truth: utama<EOS>\n",
      "\n",
      "[19/20] Train Loss: 0.0610, Train CER: 0.0149 | Val Loss: 0.3708, Val CER: 0.0688\n",
      "\n",
      "Epoch 20/20\n",
      "Batch 50/437 - Loss: 0.0495\n",
      "Batch 100/437 - Loss: 0.3927\n",
      "Batch 150/437 - Loss: 0.0131\n",
      "Batch 200/437 - Loss: 0.1455\n",
      "Batch 250/437 - Loss: 0.0758\n",
      "Batch 300/437 - Loss: 0.0520\n",
      "Batch 350/437 - Loss: 0.0341\n",
      "Batch 400/437 - Loss: 0.0073\n",
      "\n",
      "=== Top 5 Worst Samples by CER ===\n",
      "[1] CER: 2.100\n",
      "   Predicted: .<EOS><EOS><EOS><EOS><EOS>\n",
      "   Ground Truth: panti<EOS>\n",
      "\n",
      "[2] CER: 1.455\n",
      "   Predicted: se<EOS>e<EOS><EOS><EOS>\n",
      "   Ground Truth: sareng<EOS>\n",
      "\n",
      "[3] CER: 1.364\n",
      "   Predicted: me<EOS>g<EOS><EOS><EOS>\n",
      "   Ground Truth: meneng<EOS>\n",
      "\n",
      "[4] CER: 1.222\n",
      "   Predicted: ra<EOS><EOS><EOS>\n",
      "   Ground Truth: ring<EOS>\n",
      "\n",
      "[5] CER: 1.100\n",
      "   Predicted: ira<EOS><EOS><EOS>\n",
      "   Ground Truth: irung<EOS>\n",
      "\n",
      "[20/20] Train Loss: 0.0397, Train CER: 0.0097 | Val Loss: 0.3559, Val CER: 0.0729\n",
      "\n",
      "Training completed in 0h 7m.\n",
      "\n",
      "Results have been written to: training_results.csv\n",
      "Encoder and decoder models saved: encoder_resnet18_encoder.pth, decoder_resnet18_encoder.pth\n"
     ]
    }
   ],
   "source": [
    "cnn_encoder = ResNet18Encoder(pretrained=True)\n",
    "cnn_decoder = DecoderRNN(\n",
    "    attention_dim=256,\n",
    "    embed_dim=256,\n",
    "    decoder_dim=512,\n",
    "    vocab_size=vocab_size,\n",
    "    encoder_dim=cnn_encoder.encoder_dim,  # 512 for ResNet18\n",
    "    teacher_forcing_ratio=0.5\n",
    ")\n",
    "\n",
    "cnn_encoder.to(device)\n",
    "cnn_decoder.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=char_to_idx['<PAD>'])\n",
    "encoder_optimizer = optim.Adam(cnn_encoder.parameters(), lr=1e-4)\n",
    "decoder_optimizer = optim.Adam(cnn_decoder.parameters(), lr=4e-4)\n",
    "\n",
    "trainer = ImageCaptioningTrainer(\n",
    "    encoder=cnn_encoder,\n",
    "    decoder=cnn_decoder,\n",
    "    criterion=criterion,\n",
    "    encoder_optimizer=encoder_optimizer,\n",
    "    decoder_optimizer=decoder_optimizer,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    device=device,\n",
    "    char_to_idx=char_to_idx,\n",
    "    idx_to_char=idx_to_char,\n",
    "    max_label_length=max_label_length,\n",
    "    model_name=\"resnet18_encoder\"\n",
    ")\n",
    "\n",
    "num_epochs = 20\n",
    "trainer.fit(num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "65646241-eb91-4467-9a60-fe7ef8e708f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory cleared\n"
     ]
    }
   ],
   "source": [
    "# Manually delete references to free GPU memory\n",
    "del cnn_encoder\n",
    "del cnn_decoder\n",
    "del trainer\n",
    "\n",
    "# Empty the PyTorch CUDA cache\n",
    "torch.cuda.empty_cache()\n",
    "print(\"Memory cleared\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a8f024c4-c7f2-4f91-8717-168f67d84057",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/apps/miniforge3/24.3.0-0/miniforge3/envs/dsks_2024.06/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/apps/miniforge3/24.3.0-0/miniforge3/envs/dsks_2024.06/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/20\n",
      "Batch 50/437 - Loss: 3.6605\n",
      "Batch 100/437 - Loss: 3.6441\n",
      "Batch 150/437 - Loss: 3.6573\n",
      "Batch 200/437 - Loss: 3.6487\n",
      "Batch 250/437 - Loss: 3.6518\n",
      "Batch 300/437 - Loss: 3.6620\n",
      "Batch 350/437 - Loss: 3.6564\n",
      "Batch 400/437 - Loss: 3.6561\n",
      "\n",
      "=== Top 5 Worst Samples by CER ===\n",
      "[1] CER: 1.769\n",
      "   Predicted: ggggg<UNK><UNK><UNK><UNK>\n",
      "   Ground Truth: dumalada<EOS>\n",
      "\n",
      "[2] CER: 1.692\n",
      "   Predicted: 99ggg<UNK><UNK><UNK><UNK>\n",
      "   Ground Truth: mangkana<EOS>\n",
      "\n",
      "[3] CER: 1.200\n",
      "   Predicted: gggg<UNK><UNK>\n",
      "   Ground Truth: kabeh<EOS>\n",
      "\n",
      "[4] CER: 1.200\n",
      "   Predicted: gggg<UNK><UNK>\n",
      "   Ground Truth: nyepi<EOS>\n",
      "\n",
      "[5] CER: 1.100\n",
      "   Predicted: annn<UNK><UNK>\n",
      "   Ground Truth: nahan<EOS>\n",
      "\n",
      "[1/20] Train Loss: 3.6591, Train CER: 0.9443 | Val Loss: 3.6501, Val CER: 0.9622\n",
      "\n",
      "Epoch 2/20\n",
      "Batch 50/437 - Loss: 3.6494\n",
      "Batch 100/437 - Loss: 3.6592\n",
      "Batch 150/437 - Loss: 3.6597\n",
      "Batch 200/437 - Loss: 3.6516\n",
      "Batch 250/437 - Loss: 3.6632\n",
      "Batch 300/437 - Loss: 3.6699\n",
      "Batch 350/437 - Loss: 3.6606\n",
      "Batch 400/437 - Loss: 3.6534\n",
      "\n",
      "=== Top 5 Worst Samples by CER ===\n",
      "[1] CER: 2.000\n",
      "   Predicted: 9ggg<UNK><UNK><UNK><UNK><UNK>\n",
      "   Ground Truth: mangkana<EOS>\n",
      "\n",
      "[2] CER: 1.769\n",
      "   Predicted: gggg<UNK><UNK><UNK><UNK>u\n",
      "   Ground Truth: dumalada<EOS>\n",
      "\n",
      "[3] CER: 1.200\n",
      "   Predicted: gggg<UNK><UNK>\n",
      "   Ground Truth: kabeh<EOS>\n",
      "\n",
      "[4] CER: 1.200\n",
      "   Predicted: ,,gg<UNK><UNK>\n",
      "   Ground Truth: danda<EOS>\n",
      "\n",
      "[5] CER: 1.200\n",
      "   Predicted: gggg<UNK><UNK>\n",
      "   Ground Truth: nyepi<EOS>\n",
      "\n",
      "[2/20] Train Loss: 3.6593, Train CER: 0.9438 | Val Loss: 3.6498, Val CER: 0.9655\n",
      "\n",
      "Epoch 3/20\n",
      "Batch 50/437 - Loss: 3.6680\n",
      "Batch 100/437 - Loss: 3.6640\n",
      "Batch 150/437 - Loss: 3.6625\n",
      "Batch 200/437 - Loss: 3.6648\n",
      "Batch 250/437 - Loss: 3.6665\n",
      "Batch 300/437 - Loss: 3.6663\n",
      "Batch 350/437 - Loss: 3.6620\n",
      "Batch 400/437 - Loss: 3.6497\n",
      "\n",
      "=== Top 5 Worst Samples by CER ===\n",
      "[1] CER: 2.000\n",
      "   Predicted: 99gg<UNK><UNK><UNK><UNK><UNK>\n",
      "   Ground Truth: mangkana<EOS>\n",
      "\n",
      "[2] CER: 1.462\n",
      "   Predicted: ggggg<UNK><UNK><UNK>u\n",
      "   Ground Truth: dumalada<EOS>\n",
      "\n",
      "[3] CER: 1.308\n",
      "   Predicted: gggggg<UNK><UNK><UNK>\n",
      "   Ground Truth: alungguh<EOS>\n",
      "\n",
      "[4] CER: 1.200\n",
      "   Predicted: gggg<UNK><UNK>\n",
      "   Ground Truth: kabeh<EOS>\n",
      "\n",
      "[5] CER: 1.200\n",
      "   Predicted: gggg<UNK><UNK>\n",
      "   Ground Truth: nyepi<EOS>\n",
      "\n",
      "[3/20] Train Loss: 3.6597, Train CER: 0.9461 | Val Loss: 3.6496, Val CER: 0.9626\n",
      "\n",
      "Epoch 4/20\n",
      "Batch 50/437 - Loss: 3.6730\n",
      "Batch 100/437 - Loss: 3.6537\n",
      "Batch 150/437 - Loss: 3.6631\n",
      "Batch 200/437 - Loss: 3.6719\n",
      "Batch 250/437 - Loss: 3.6621\n",
      "Batch 300/437 - Loss: 3.6361\n",
      "Batch 350/437 - Loss: 3.6549\n",
      "Batch 400/437 - Loss: 3.6554\n",
      "\n",
      "=== Top 5 Worst Samples by CER ===\n",
      "[1] CER: 1.769\n",
      "   Predicted: ggggg<UNK><UNK><UNK><UNK>\n",
      "   Ground Truth: dumalada<EOS>\n",
      "\n",
      "[2] CER: 1.692\n",
      "   Predicted: 9gggg<UNK><UNK><UNK><UNK>\n",
      "   Ground Truth: mangkana<EOS>\n",
      "\n",
      "[3] CER: 1.200\n",
      "   Predicted: gggg<UNK><UNK>\n",
      "   Ground Truth: kabeh<EOS>\n",
      "\n",
      "[4] CER: 1.100\n",
      "   Predicted: annn<UNK><UNK>\n",
      "   Ground Truth: nahan<EOS>\n",
      "\n",
      "[5] CER: 1.000\n",
      "   Predicted: gg\n",
      "   Ground Truth: ,<EOS>\n",
      "\n",
      "[4/20] Train Loss: 3.6591, Train CER: 0.9460 | Val Loss: 3.6504, Val CER: 0.9637\n",
      "\n",
      "Epoch 5/20\n",
      "Batch 50/437 - Loss: 3.6600\n",
      "Batch 100/437 - Loss: 3.6737\n",
      "Batch 150/437 - Loss: 3.6726\n",
      "Batch 200/437 - Loss: 3.6473\n",
      "Batch 250/437 - Loss: 3.6429\n",
      "Batch 300/437 - Loss: 3.6745\n",
      "Batch 350/437 - Loss: 3.6687\n",
      "Batch 400/437 - Loss: 3.6667\n",
      "\n",
      "=== Top 5 Worst Samples by CER ===\n",
      "[1] CER: 1.692\n",
      "   Predicted: 99ggg<UNK><UNK><UNK><UNK>\n",
      "   Ground Truth: mangkana<EOS>\n",
      "\n",
      "[2] CER: 1.200\n",
      "   Predicted: gggg<UNK><UNK>\n",
      "   Ground Truth: kabeh<EOS>\n",
      "\n",
      "[3] CER: 1.200\n",
      "   Predicted: gggg<UNK><UNK>\n",
      "   Ground Truth: nyepi<EOS>\n",
      "\n",
      "[4] CER: 1.100\n",
      "   Predicted: annn<UNK><UNK>\n",
      "   Ground Truth: nahan<EOS>\n",
      "\n",
      "[5] CER: 1.000\n",
      "   Predicted: gg\n",
      "   Ground Truth: ,<EOS>\n",
      "\n",
      "[5/20] Train Loss: 3.6595, Train CER: 0.9425 | Val Loss: 3.6501, Val CER: 0.9641\n",
      "\n",
      "Epoch 6/20\n",
      "Batch 50/437 - Loss: 3.6596\n",
      "Batch 100/437 - Loss: 3.6479\n",
      "Batch 150/437 - Loss: 3.6608\n",
      "Batch 200/437 - Loss: 3.6588\n",
      "Batch 250/437 - Loss: 3.6758\n",
      "Batch 300/437 - Loss: 3.6529\n",
      "Batch 350/437 - Loss: 3.6591\n",
      "Batch 400/437 - Loss: 3.6541\n",
      "\n",
      "=== Top 5 Worst Samples by CER ===\n",
      "[1] CER: 1.769\n",
      "   Predicted: ggggg<UNK><UNK><UNK><UNK>\n",
      "   Ground Truth: dumalada<EOS>\n",
      "\n",
      "[2] CER: 1.692\n",
      "   Predicted: 99ggg<UNK><UNK><UNK><UNK>\n",
      "   Ground Truth: mangkana<EOS>\n",
      "\n",
      "[3] CER: 1.200\n",
      "   Predicted: gggg<UNK><UNK>\n",
      "   Ground Truth: kabeh<EOS>\n",
      "\n",
      "[4] CER: 1.200\n",
      "   Predicted: gggg<UNK><UNK>\n",
      "   Ground Truth: nyepi<EOS>\n",
      "\n",
      "[5] CER: 1.100\n",
      "   Predicted: 8ggg<UNK><UNK>\n",
      "   Ground Truth: gusti<EOS>\n",
      "\n",
      "[6/20] Train Loss: 3.6594, Train CER: 0.9470 | Val Loss: 3.6501, Val CER: 0.9646\n",
      "\n",
      "Epoch 7/20\n",
      "Batch 50/437 - Loss: 3.6686\n",
      "Batch 100/437 - Loss: 3.6581\n",
      "Batch 150/437 - Loss: 3.6541\n",
      "Batch 200/437 - Loss: 3.6704\n",
      "Batch 250/437 - Loss: 3.6596\n",
      "Batch 300/437 - Loss: 3.6587\n",
      "Batch 350/437 - Loss: 3.6694\n",
      "Batch 400/437 - Loss: 3.6625\n",
      "\n",
      "=== Top 5 Worst Samples by CER ===\n",
      "[1] CER: 2.077\n",
      "   Predicted: gggg<UNK><UNK><UNK><UNK><UNK>\n",
      "   Ground Truth: dumalada<EOS>\n",
      "\n",
      "[2] CER: 2.000\n",
      "   Predicted: 99gg<UNK><UNK><UNK><UNK><UNK>\n",
      "   Ground Truth: mangkana<EOS>\n",
      "\n",
      "[3] CER: 1.308\n",
      "   Predicted: gggggg<UNK><UNK><UNK>\n",
      "   Ground Truth: alungguh<EOS>\n",
      "\n",
      "[4] CER: 1.200\n",
      "   Predicted: gggg<UNK><UNK>\n",
      "   Ground Truth: kabeh<EOS>\n",
      "\n",
      "[5] CER: 1.200\n",
      "   Predicted: gggg<UNK><UNK>\n",
      "   Ground Truth: nyepi<EOS>\n",
      "\n",
      "[7/20] Train Loss: 3.6596, Train CER: 0.9453 | Val Loss: 3.6494, Val CER: 0.9642\n",
      "\n",
      "Epoch 8/20\n",
      "Batch 50/437 - Loss: 3.6506\n",
      "Batch 100/437 - Loss: 3.6540\n",
      "Batch 150/437 - Loss: 3.6478\n",
      "Batch 200/437 - Loss: 3.6405\n",
      "Batch 250/437 - Loss: 3.6553\n",
      "Batch 300/437 - Loss: 3.6685\n",
      "Batch 350/437 - Loss: 3.6584\n",
      "Batch 400/437 - Loss: 3.6738\n",
      "\n",
      "=== Top 5 Worst Samples by CER ===\n",
      "[1] CER: 1.692\n",
      "   Predicted: 9gggg<UNK><UNK><UNK><UNK>\n",
      "   Ground Truth: mangkana<EOS>\n",
      "\n",
      "[2] CER: 1.308\n",
      "   Predicted: gggggg<UNK><UNK><UNK>\n",
      "   Ground Truth: alungguh<EOS>\n",
      "\n",
      "[3] CER: 1.200\n",
      "   Predicted: gggg<UNK><UNK>\n",
      "   Ground Truth: kabeh<EOS>\n",
      "\n",
      "[4] CER: 1.200\n",
      "   Predicted: ,,gg<UNK><UNK>\n",
      "   Ground Truth: danda<EOS>\n",
      "\n",
      "[5] CER: 1.200\n",
      "   Predicted: gggg<UNK><UNK>\n",
      "   Ground Truth: nyepi<EOS>\n",
      "\n",
      "[8/20] Train Loss: 3.6593, Train CER: 0.9435 | Val Loss: 3.6500, Val CER: 0.9637\n",
      "\n",
      "Epoch 9/20\n",
      "Batch 50/437 - Loss: 3.6666\n",
      "Batch 100/437 - Loss: 3.6678\n",
      "Batch 150/437 - Loss: 3.6377\n",
      "Batch 200/437 - Loss: 3.6686\n",
      "Batch 250/437 - Loss: 3.6586\n",
      "Batch 300/437 - Loss: 3.6486\n",
      "Batch 350/437 - Loss: 3.6614\n",
      "Batch 400/437 - Loss: 3.6542\n",
      "\n",
      "=== Top 5 Worst Samples by CER ===\n",
      "[1] CER: 1.692\n",
      "   Predicted: 99ggg<UNK><UNK><UNK><UNK>\n",
      "   Ground Truth: mangkana<EOS>\n",
      "\n",
      "[2] CER: 1.462\n",
      "   Predicted: ggggg<UNK><UNK><UNK>u\n",
      "   Ground Truth: dumalada<EOS>\n",
      "\n",
      "[3] CER: 1.200\n",
      "   Predicted: gggg<UNK><UNK>\n",
      "   Ground Truth: kabeh<EOS>\n",
      "\n",
      "[4] CER: 1.200\n",
      "   Predicted: gggg<UNK><UNK>\n",
      "   Ground Truth: nyepi<EOS>\n",
      "\n",
      "[5] CER: 1.100\n",
      "   Predicted: annn<UNK><UNK>\n",
      "   Ground Truth: nahan<EOS>\n",
      "\n",
      "[9/20] Train Loss: 3.6595, Train CER: 0.9447 | Val Loss: 3.6495, Val CER: 0.9628\n",
      "\n",
      "Epoch 10/20\n",
      "Batch 50/437 - Loss: 3.6497\n",
      "Batch 100/437 - Loss: 3.6660\n",
      "Batch 150/437 - Loss: 3.6554\n",
      "Batch 200/437 - Loss: 3.6527\n",
      "Batch 250/437 - Loss: 3.6608\n",
      "Batch 300/437 - Loss: 3.6574\n",
      "Batch 350/437 - Loss: 3.6697\n",
      "Batch 400/437 - Loss: 3.6406\n",
      "\n",
      "=== Top 5 Worst Samples by CER ===\n",
      "[1] CER: 1.769\n",
      "   Predicted: ggggg<UNK><UNK><UNK><UNK>\n",
      "   Ground Truth: dumalada<EOS>\n",
      "\n",
      "[2] CER: 1.692\n",
      "   Predicted: 99ggg<UNK><UNK><UNK><UNK>\n",
      "   Ground Truth: mangkana<EOS>\n",
      "\n",
      "[3] CER: 1.200\n",
      "   Predicted: gggg<UNK><UNK>\n",
      "   Ground Truth: kabeh<EOS>\n",
      "\n",
      "[4] CER: 1.200\n",
      "   Predicted: gggg<UNK><UNK>\n",
      "   Ground Truth: nyepi<EOS>\n",
      "\n",
      "[5] CER: 1.154\n",
      "   Predicted: gggggg<UNK><UNK>u\n",
      "   Ground Truth: dumalada<EOS>\n",
      "\n",
      "[10/20] Train Loss: 3.6593, Train CER: 0.9440 | Val Loss: 3.6499, Val CER: 0.9634\n",
      "\n",
      "Epoch 11/20\n",
      "Batch 50/437 - Loss: 3.6553\n",
      "Batch 100/437 - Loss: 3.6689\n",
      "Batch 150/437 - Loss: 3.6780\n",
      "Batch 200/437 - Loss: 3.6477\n",
      "Batch 250/437 - Loss: 3.6471\n",
      "Batch 300/437 - Loss: 3.6579\n",
      "Batch 350/437 - Loss: 3.6705\n",
      "Batch 400/437 - Loss: 3.6582\n",
      "\n",
      "=== Top 5 Worst Samples by CER ===\n",
      "[1] CER: 1.692\n",
      "   Predicted: 99ggg<UNK><UNK><UNK><UNK>\n",
      "   Ground Truth: mangkana<EOS>\n",
      "\n",
      "[2] CER: 1.200\n",
      "   Predicted: gggg<UNK><UNK>\n",
      "   Ground Truth: kabeh<EOS>\n",
      "\n",
      "[3] CER: 1.200\n",
      "   Predicted: gggg<UNK><UNK>\n",
      "   Ground Truth: nyepi<EOS>\n",
      "\n",
      "[4] CER: 1.100\n",
      "   Predicted: annn<UNK><UNK>\n",
      "   Ground Truth: nahan<EOS>\n",
      "\n",
      "[5] CER: 1.000\n",
      "   Predicted: gg\n",
      "   Ground Truth: ,<EOS>\n",
      "\n",
      "[11/20] Train Loss: 3.6591, Train CER: 0.9447 | Val Loss: 3.6502, Val CER: 0.9624\n",
      "\n",
      "Epoch 12/20\n",
      "Batch 50/437 - Loss: 3.6455\n",
      "Batch 100/437 - Loss: 3.6512\n",
      "Batch 150/437 - Loss: 3.6686\n",
      "Batch 200/437 - Loss: 3.6717\n",
      "Batch 250/437 - Loss: 3.6555\n",
      "Batch 300/437 - Loss: 3.6624\n",
      "Batch 350/437 - Loss: 3.6498\n",
      "Batch 400/437 - Loss: 3.6627\n",
      "\n",
      "=== Top 5 Worst Samples by CER ===\n",
      "[1] CER: 2.000\n",
      "   Predicted: 99gg<UNK><UNK><UNK><UNK><UNK>\n",
      "   Ground Truth: mangkana<EOS>\n",
      "\n",
      "[2] CER: 1.200\n",
      "   Predicted: gggg<UNK><UNK>\n",
      "   Ground Truth: kabeh<EOS>\n",
      "\n",
      "[3] CER: 1.200\n",
      "   Predicted: ,,gg<UNK><UNK>\n",
      "   Ground Truth: danda<EOS>\n",
      "\n",
      "[4] CER: 1.200\n",
      "   Predicted: gggg<UNK><UNK>\n",
      "   Ground Truth: nyepi<EOS>\n",
      "\n",
      "[5] CER: 1.154\n",
      "   Predicted: ggggg<UNK><UNK>uu\n",
      "   Ground Truth: dumalada<EOS>\n",
      "\n",
      "[12/20] Train Loss: 3.6595, Train CER: 0.9458 | Val Loss: 3.6501, Val CER: 0.9642\n",
      "\n",
      "Epoch 13/20\n",
      "Batch 50/437 - Loss: 3.6417\n",
      "Batch 100/437 - Loss: 3.6602\n",
      "Batch 150/437 - Loss: 3.6777\n",
      "Batch 200/437 - Loss: 3.6523\n",
      "Batch 250/437 - Loss: 3.6518\n",
      "Batch 300/437 - Loss: 3.6631\n",
      "Batch 350/437 - Loss: 3.6597\n",
      "Batch 400/437 - Loss: 3.6615\n",
      "\n",
      "=== Top 5 Worst Samples by CER ===\n",
      "[1] CER: 2.000\n",
      "   Predicted: 99gg<UNK><UNK><UNK><UNK><UNK>\n",
      "   Ground Truth: mangkana<EOS>\n",
      "\n",
      "[2] CER: 1.769\n",
      "   Predicted: ggggg<UNK><UNK><UNK><UNK>\n",
      "   Ground Truth: dumalada<EOS>\n",
      "\n",
      "[3] CER: 1.308\n",
      "   Predicted: gggggg<UNK><UNK><UNK>\n",
      "   Ground Truth: alungguh<EOS>\n",
      "\n",
      "[4] CER: 1.200\n",
      "   Predicted: gggg<UNK><UNK>\n",
      "   Ground Truth: kabeh<EOS>\n",
      "\n",
      "[5] CER: 1.200\n",
      "   Predicted: gggg<UNK><UNK>\n",
      "   Ground Truth: nyepi<EOS>\n",
      "\n",
      "[13/20] Train Loss: 3.6589, Train CER: 0.9455 | Val Loss: 3.6502, Val CER: 0.9642\n",
      "\n",
      "Epoch 14/20\n",
      "Batch 50/437 - Loss: 3.6583\n",
      "Batch 100/437 - Loss: 3.6648\n",
      "Batch 150/437 - Loss: 3.6604\n",
      "Batch 200/437 - Loss: 3.6710\n",
      "Batch 250/437 - Loss: 3.6751\n",
      "Batch 300/437 - Loss: 3.6498\n",
      "Batch 350/437 - Loss: 3.6546\n",
      "Batch 400/437 - Loss: 3.6879\n",
      "\n",
      "=== Top 5 Worst Samples by CER ===\n",
      "[1] CER: 1.692\n",
      "   Predicted: 9gggg<UNK><UNK><UNK><UNK>\n",
      "   Ground Truth: mangkana<EOS>\n",
      "\n",
      "[2] CER: 1.692\n",
      "   Predicted: ggggg<UNK><UNK><UNK><UNK>\n",
      "   Ground Truth: alungguh<EOS>\n",
      "\n",
      "[3] CER: 1.200\n",
      "   Predicted: gggg<UNK><UNK>\n",
      "   Ground Truth: kabeh<EOS>\n",
      "\n",
      "[4] CER: 1.200\n",
      "   Predicted: ,,gg<UNK><UNK>\n",
      "   Ground Truth: danda<EOS>\n",
      "\n",
      "[5] CER: 1.200\n",
      "   Predicted: gggg<UNK><UNK>\n",
      "   Ground Truth: nyepi<EOS>\n",
      "\n",
      "[14/20] Train Loss: 3.6591, Train CER: 0.9452 | Val Loss: 3.6506, Val CER: 0.9662\n",
      "\n",
      "Epoch 15/20\n",
      "Batch 50/437 - Loss: 3.6580\n",
      "Batch 100/437 - Loss: 3.6728\n",
      "Batch 150/437 - Loss: 3.6714\n",
      "Batch 200/437 - Loss: 3.6658\n",
      "Batch 250/437 - Loss: 3.6494\n",
      "Batch 300/437 - Loss: 3.6642\n",
      "Batch 350/437 - Loss: 3.6596\n",
      "Batch 400/437 - Loss: 3.6458\n",
      "\n",
      "=== Top 5 Worst Samples by CER ===\n",
      "[1] CER: 1.692\n",
      "   Predicted: 99ggg<UNK><UNK><UNK><UNK>\n",
      "   Ground Truth: mangkana<EOS>\n",
      "\n",
      "[2] CER: 1.273\n",
      "   Predicted: aggg<UNK><UNK><UNK>\n",
      "   Ground Truth: anggen<EOS>\n",
      "\n",
      "[3] CER: 1.200\n",
      "   Predicted: gggg<UNK><UNK>\n",
      "   Ground Truth: kabeh<EOS>\n",
      "\n",
      "[4] CER: 1.200\n",
      "   Predicted: gggg<UNK><UNK>\n",
      "   Ground Truth: nyepi<EOS>\n",
      "\n",
      "[5] CER: 1.154\n",
      "   Predicted: gggg4<UNK><UNK>uu\n",
      "   Ground Truth: dumalada<EOS>\n",
      "\n",
      "[15/20] Train Loss: 3.6594, Train CER: 0.9449 | Val Loss: 3.6492, Val CER: 0.9622\n",
      "\n",
      "Epoch 16/20\n",
      "Batch 50/437 - Loss: 3.6567\n",
      "Batch 400/437 - Loss: 3.6738\n",
      "\n",
      "=== Top 5 Worst Samples by CER ===\n",
      "[1] CER: 1.769\n",
      "   Predicted: ggggg<UNK><UNK><UNK><UNK>\n",
      "   Ground Truth: dumalada<EOS>\n",
      "\n",
      "[2] CER: 1.692\n",
      "   Predicted: 99ggg<UNK><UNK><UNK><UNK>\n",
      "   Ground Truth: mangkana<EOS>\n",
      "\n",
      "[3] CER: 1.200\n",
      "   Predicted: gggg<UNK><UNK>\n",
      "   Ground Truth: kabeh<EOS>\n",
      "\n",
      "[4] CER: 1.200\n",
      "   Predicted: gggg<UNK><UNK>\n",
      "   Ground Truth: nyepi<EOS>\n",
      "\n",
      "[5] CER: 1.100\n",
      "   Predicted: nnnn<UNK><UNK>\n",
      "   Ground Truth: nahan<EOS>\n",
      "\n",
      "[16/20] Train Loss: 3.6598, Train CER: 0.9453 | Val Loss: 3.6500, Val CER: 0.9622\n",
      "\n",
      "Epoch 17/20\n",
      "Batch 50/437 - Loss: 3.6713\n",
      "Batch 100/437 - Loss: 3.6566\n",
      "Batch 150/437 - Loss: 3.6606\n",
      "Batch 200/437 - Loss: 3.6652\n",
      "Batch 250/437 - Loss: 3.6423\n",
      "Batch 300/437 - Loss: 3.6537\n",
      "Batch 350/437 - Loss: 3.6713\n",
      "Batch 400/437 - Loss: 3.6752\n",
      "\n",
      "=== Top 5 Worst Samples by CER ===\n",
      "[1] CER: 1.769\n",
      "   Predicted: ggggg<UNK><UNK><UNK><UNK>\n",
      "   Ground Truth: dumalada<EOS>\n",
      "\n",
      "[2] CER: 1.308\n",
      "   Predicted: gggggg<UNK><UNK><UNK>\n",
      "   Ground Truth: alungguh<EOS>\n",
      "\n",
      "[3] CER: 1.200\n",
      "   Predicted: gggg<UNK><UNK>\n",
      "   Ground Truth: kabeh<EOS>\n",
      "\n",
      "[4] CER: 1.100\n",
      "   Predicted: annn<UNK><UNK>\n",
      "   Ground Truth: nahan<EOS>\n",
      "\n",
      "[5] CER: 1.000\n",
      "   Predicted: gg\n",
      "   Ground Truth: ,<EOS>\n",
      "\n",
      "[17/20] Train Loss: 3.6593, Train CER: 0.9446 | Val Loss: 3.6495, Val CER: 0.9608\n",
      "\n",
      "Epoch 18/20\n",
      "Batch 50/437 - Loss: 3.6543\n",
      "Batch 100/437 - Loss: 3.6669\n",
      "Batch 150/437 - Loss: 3.6666\n",
      "Batch 200/437 - Loss: 3.6538\n",
      "Batch 250/437 - Loss: 3.6533\n",
      "Batch 300/437 - Loss: 3.6557\n",
      "Batch 350/437 - Loss: 3.6649\n",
      "Batch 400/437 - Loss: 3.6538\n",
      "\n",
      "=== Top 5 Worst Samples by CER ===\n",
      "[1] CER: 2.000\n",
      "   Predicted: 9ggg<UNK><UNK><UNK><UNK><UNK>\n",
      "   Ground Truth: mangkana<EOS>\n",
      "\n",
      "[2] CER: 1.769\n",
      "   Predicted: ggggg<UNK><UNK><UNK><UNK>\n",
      "   Ground Truth: dumalada<EOS>\n",
      "\n",
      "[3] CER: 1.200\n",
      "   Predicted: gggg<UNK><UNK>\n",
      "   Ground Truth: kabeh<EOS>\n",
      "\n",
      "[4] CER: 1.200\n",
      "   Predicted: ,,gg<UNK><UNK>\n",
      "   Ground Truth: danda<EOS>\n",
      "\n",
      "[5] CER: 1.200\n",
      "   Predicted: gggg<UNK><UNK>\n",
      "   Ground Truth: nyepi<EOS>\n",
      "\n",
      "[18/20] Train Loss: 3.6593, Train CER: 0.9456 | Val Loss: 3.6506, Val CER: 0.9640\n",
      "\n",
      "Epoch 19/20\n",
      "Batch 50/437 - Loss: 3.6649\n",
      "Batch 100/437 - Loss: 3.6593\n",
      "Batch 150/437 - Loss: 3.6666\n",
      "Batch 200/437 - Loss: 3.6556\n",
      "Batch 250/437 - Loss: 3.6504\n",
      "Batch 300/437 - Loss: 3.6565\n",
      "Batch 350/437 - Loss: 3.6483\n",
      "Batch 400/437 - Loss: 3.6718\n",
      "\n",
      "=== Top 5 Worst Samples by CER ===\n",
      "[1] CER: 2.000\n",
      "   Predicted: 99gg<UNK><UNK><UNK><UNK><UNK>\n",
      "   Ground Truth: mangkana<EOS>\n",
      "\n",
      "[2] CER: 1.462\n",
      "   Predicted: ggggg<UNK><UNK><UNK>u\n",
      "   Ground Truth: dumalada<EOS>\n",
      "\n",
      "[3] CER: 1.200\n",
      "   Predicted: gggg<UNK><UNK>\n",
      "   Ground Truth: kabeh<EOS>\n",
      "\n",
      "[4] CER: 1.200\n",
      "   Predicted: gggg<UNK><UNK>\n",
      "   Ground Truth: nyepi<EOS>\n",
      "\n",
      "[5] CER: 1.100\n",
      "   Predicted: annn<UNK><UNK>\n",
      "   Ground Truth: nahan<EOS>\n",
      "\n",
      "[19/20] Train Loss: 3.6597, Train CER: 0.9447 | Val Loss: 3.6501, Val CER: 0.9631\n",
      "\n",
      "Epoch 20/20\n",
      "Batch 50/437 - Loss: 3.6345\n",
      "Batch 100/437 - Loss: 3.6707\n",
      "Batch 150/437 - Loss: 3.6643\n",
      "Batch 200/437 - Loss: 3.6651\n",
      "Batch 250/437 - Loss: 3.6578\n",
      "Batch 300/437 - Loss: 3.6488\n",
      "Batch 350/437 - Loss: 3.6603\n",
      "Batch 400/437 - Loss: 3.6657\n",
      "\n",
      "=== Top 5 Worst Samples by CER ===\n",
      "[1] CER: 2.000\n",
      "   Predicted: 99gg<UNK><UNK><UNK><UNK><UNK>\n",
      "   Ground Truth: mangkana<EOS>\n",
      "\n",
      "[2] CER: 1.308\n",
      "   Predicted: gggggg<UNK><UNK><UNK>\n",
      "   Ground Truth: alungguh<EOS>\n",
      "\n",
      "[3] CER: 1.200\n",
      "   Predicted: gggg<UNK><UNK>\n",
      "   Ground Truth: kabeh<EOS>\n",
      "\n",
      "[4] CER: 1.200\n",
      "   Predicted: gggg<UNK><UNK>\n",
      "   Ground Truth: nyepi<EOS>\n",
      "\n",
      "[5] CER: 1.154\n",
      "   Predicted: ggggg<UNK><UNK>uu\n",
      "   Ground Truth: dumalada<EOS>\n",
      "\n",
      "[20/20] Train Loss: 3.6594, Train CER: 0.9443 | Val Loss: 3.6508, Val CER: 0.9637\n",
      "\n",
      "Training completed in 0h 17m.\n",
      "\n",
      "Results have been written to: training_results.csv\n",
      "Encoder and decoder models saved: encoder_resnet50_encoder.pth, decoder_resnet50_encoder.pth\n"
     ]
    }
   ],
   "source": [
    "cnn_encoder_50 = ResNet50Encoder(pretrained=True)\n",
    "cnn_decoder_50 = DecoderRNN(\n",
    "    attention_dim=256,\n",
    "    embed_dim=256,\n",
    "    decoder_dim=512,\n",
    "    vocab_size=vocab_size,\n",
    "    encoder_dim=cnn_encoder_50.encoder_dim,  # 2048 for ResNet50\n",
    "    teacher_forcing_ratio=0.5\n",
    ")\n",
    "\n",
    "cnn_encoder_50 = cnn_encoder_50.to(device)\n",
    "cnn_decoder_50 = cnn_decoder_50.to(device)\n",
    "\n",
    "trainer = ImageCaptioningTrainer(\n",
    "    encoder=cnn_encoder_50,\n",
    "    decoder=cnn_decoder_50,\n",
    "    criterion=criterion,\n",
    "    encoder_optimizer=encoder_optimizer,\n",
    "    decoder_optimizer=decoder_optimizer,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    device=device,\n",
    "    char_to_idx=char_to_idx,\n",
    "    idx_to_char=idx_to_char,\n",
    "    max_label_length=max_label_length,\n",
    "    model_name=\"resnet50_encoder\"\n",
    ")\n",
    "num_epochs = 20\n",
    "trainer.fit(num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3c75cffa-9cb4-4fe8-89af-012c1e986ac0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory cleared\n"
     ]
    }
   ],
   "source": [
    "# Manually delete references to free GPU memory\n",
    "del cnn_encoder_50\n",
    "del cnn_decoder_50\n",
    "del trainer\n",
    "\n",
    "# Empty the PyTorch CUDA cache\n",
    "torch.cuda.empty_cache()\n",
    "print(\"Memory cleared\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0cb93b75-b5db-4269-a76d-6f55e7a8b342",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_vit_base = ViTEncoder(model_name=\"vit_base_patch16_224\", pretrained=True)\n",
    "decoder_vit_base = DecoderRNN(\n",
    "    attention_dim=256,\n",
    "    embed_dim=256,\n",
    "    decoder_dim=512,\n",
    "    vocab_size=vocab_size,\n",
    "    encoder_dim=encoder_vit_base.encoder_dim,  \n",
    "    teacher_forcing_ratio=0.5\n",
    ").to(device)\n",
    "\n",
    "encoder_vit_base = encoder_vit_base.to(device)\n",
    "decoder_vit_base = decoder_vit_base.to(device)\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=char_to_idx['<PAD>'])\n",
    "encoder_optimizer_base = optim.Adam(encoder_vit_base.parameters(), lr=1e-4)\n",
    "decoder_optimizer_base = optim.Adam(decoder_vit_base.parameters(), lr=4e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1296941e-a3db-45cf-a81a-8a76aa7cb3de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/20\n",
      "Batch 50/437 - Loss: 2.4082\n",
      "Batch 100/437 - Loss: 2.1473\n",
      "Batch 150/437 - Loss: 2.1296\n",
      "Batch 200/437 - Loss: 2.1781\n",
      "Batch 250/437 - Loss: 2.0242\n",
      "Batch 300/437 - Loss: 1.7679\n",
      "Batch 350/437 - Loss: 1.7292\n",
      "Batch 400/437 - Loss: 1.8599\n",
      "\n",
      "=== Top 5 Worst Samples by CER ===\n",
      "[1] CER: 1.824\n",
      "   Predicted: pitiaaa<EOS><EOS><EOS><EOS><EOS><EOS>\n",
      "   Ground Truth: awighnamastu<EOS>\n",
      "\n",
      "[2] CER: 1.750\n",
      "   Predicted: kan<EOS><EOS><EOS><EOS><EOS>\n",
      "   Ground Truth: tanhana<EOS>\n",
      "\n",
      "[3] CER: 1.706\n",
      "   Predicted: kinanan<EOS><EOS><EOS><EOS><EOS><EOS>\n",
      "   Ground Truth: niwatakawaca<EOS>\n",
      "\n",
      "[4] CER: 1.667\n",
      "   Predicted: s<EOS><EOS><EOS><EOS>\n",
      "   Ground Truth: sang<EOS>\n",
      "\n",
      "[5] CER: 1.600\n",
      "   Predicted: nn<EOS><EOS><EOS><EOS>\n",
      "   Ground Truth: endah<EOS>\n",
      "\n",
      "[1/20] Train Loss: 2.0565, Train CER: 0.6770 | Val Loss: 1.3701, Val CER: 0.3958\n",
      "\n",
      "Epoch 2/20\n",
      "Batch 50/437 - Loss: 1.6057\n",
      "Batch 100/437 - Loss: 1.4500\n",
      "Batch 150/437 - Loss: 1.4843\n",
      "Batch 200/437 - Loss: 1.4110\n",
      "Batch 250/437 - Loss: 1.5711\n",
      "Batch 300/437 - Loss: 1.2974\n",
      "Batch 350/437 - Loss: 0.9986\n",
      "Batch 400/437 - Loss: 0.9292\n",
      "\n",
      "=== Top 5 Worst Samples by CER ===\n",
      "[1] CER: 1.600\n",
      "   Predicted: .a<EOS><EOS><EOS><EOS>\n",
      "   Ground Truth: panti<EOS>\n",
      "\n",
      "[2] CER: 1.455\n",
      "   Predicted: nen<EOS><EOS><EOS><EOS>\n",
      "   Ground Truth: tengen<EOS>\n",
      "\n",
      "[3] CER: 1.364\n",
      "   Predicted: sa<EOS><EOS><EOS>g<EOS>\n",
      "   Ground Truth: sareng<EOS>\n",
      "\n",
      "[4] CER: 1.091\n",
      "   Predicted: nina<EOS><EOS><EOS>\n",
      "   Ground Truth: bineda<EOS>\n",
      "\n",
      "[5] CER: 1.083\n",
      "   Predicted: kanaa<EOS><EOS><EOS>\n",
      "   Ground Truth: jenyana<EOS>\n",
      "\n",
      "[2/20] Train Loss: 1.3232, Train CER: 0.3871 | Val Loss: 0.7182, Val CER: 0.1792\n",
      "\n",
      "Epoch 3/20\n",
      "Batch 50/437 - Loss: 0.7495\n",
      "Batch 100/437 - Loss: 0.8279\n",
      "Batch 150/437 - Loss: 0.8280\n",
      "Batch 200/437 - Loss: 0.8181\n",
      "Batch 250/437 - Loss: 0.5346\n",
      "Batch 300/437 - Loss: 0.6269\n",
      "Batch 350/437 - Loss: 0.8160\n",
      "Batch 400/437 - Loss: 0.6092\n",
      "\n",
      "=== Top 5 Worst Samples by CER ===\n",
      "[1] CER: 1.000\n",
      "   Predicted: nga\n",
      "   Ground Truth: ca<EOS>\n",
      "\n",
      "[2] CER: 1.000\n",
      "   Predicted: jesta\n",
      "   Ground Truth: tong<EOS>\n",
      "\n",
      "[3] CER: 1.000\n",
      "   Predicted: tann\n",
      "   Ground Truth: iki<EOS>\n",
      "\n",
      "[4] CER: 1.000\n",
      "   Predicted: wad\n",
      "   Ground Truth: di<EOS>\n",
      "\n",
      "[5] CER: 1.000\n",
      "   Predicted: iru<EOS><EOS><EOS>\n",
      "   Ground Truth: irung<EOS>\n",
      "\n",
      "[3/20] Train Loss: 0.8209, Train CER: 0.2261 | Val Loss: 0.4522, Val CER: 0.1128\n",
      "\n",
      "Epoch 4/20\n",
      "Batch 50/437 - Loss: 0.6122\n",
      "Batch 100/437 - Loss: 0.7109\n",
      "Batch 150/437 - Loss: 0.4372\n",
      "Batch 200/437 - Loss: 0.7050\n",
      "Batch 250/437 - Loss: 0.3433\n",
      "Batch 300/437 - Loss: 0.4945\n",
      "Batch 350/437 - Loss: 0.4332\n",
      "Batch 400/437 - Loss: 0.4165\n",
      "\n",
      "=== Top 5 Worst Samples by CER ===\n",
      "[1] CER: 1.200\n",
      "   Predicted: wpa<EOS><EOS><EOS>\n",
      "   Ground Truth: utama<EOS>\n",
      "\n",
      "[2] CER: 1.200\n",
      "   Predicted: .<EOS>a<EOS>a<EOS>\n",
      "   Ground Truth: panti<EOS>\n",
      "\n",
      "[3] CER: 1.000\n",
      "   Predicted: wad\n",
      "   Ground Truth: di<EOS>\n",
      "\n",
      "[4] CER: 1.000\n",
      "   Predicted: ka\n",
      "   Ground Truth: n<EOS>\n",
      "\n",
      "[5] CER: 1.000\n",
      "   Predicted: ne\n",
      "   Ground Truth: 2<EOS>\n",
      "\n",
      "[4/20] Train Loss: 0.5751, Train CER: 0.1536 | Val Loss: 0.3890, Val CER: 0.1003\n",
      "\n",
      "Epoch 5/20\n",
      "Batch 50/437 - Loss: 0.3729\n",
      "Batch 100/437 - Loss: 0.6689\n",
      "Batch 150/437 - Loss: 0.2046\n",
      "Batch 200/437 - Loss: 0.4519\n",
      "Batch 250/437 - Loss: 0.3904\n",
      "Batch 300/437 - Loss: 0.3431\n",
      "Batch 350/437 - Loss: 0.4639\n",
      "Batch 400/437 - Loss: 0.3014\n",
      "\n",
      "=== Top 5 Worst Samples by CER ===\n",
      "[1] CER: 1.412\n",
      "   Predicted: awilnag<EOS><EOS><EOS><EOS>a<EOS>\n",
      "   Ground Truth: awighnamastu<EOS>\n",
      "\n",
      "[2] CER: 1.364\n",
      "   Predicted: sa<EOS><EOS><EOS>g<EOS>\n",
      "   Ground Truth: sareng<EOS>\n",
      "\n",
      "[3] CER: 1.333\n",
      "   Predicted: nang<EOS><EOS><EOS><EOS>\n",
      "   Ground Truth: langkir<EOS>\n",
      "\n",
      "[4] CER: 1.222\n",
      "   Predicted: ta<EOS><EOS><EOS>\n",
      "   Ground Truth: sang<EOS>\n",
      "\n",
      "[5] CER: 1.200\n",
      "   Predicted: yaa<EOS><EOS><EOS>\n",
      "   Ground Truth: panti<EOS>\n",
      "\n",
      "[5/20] Train Loss: 0.4623, Train CER: 0.1223 | Val Loss: 0.4596, Val CER: 0.1291\n",
      "\n",
      "Epoch 6/20\n",
      "Batch 50/437 - Loss: 0.5144\n",
      "Batch 100/437 - Loss: 0.5247\n",
      "Batch 150/437 - Loss: 0.2773\n",
      "Batch 200/437 - Loss: 0.4246\n",
      "Batch 250/437 - Loss: 0.3582\n",
      "Batch 300/437 - Loss: 0.4434\n",
      "Batch 350/437 - Loss: 0.2267\n",
      "Batch 400/437 - Loss: 0.2603\n",
      "\n",
      "=== Top 5 Worst Samples by CER ===\n",
      "[1] CER: 1.500\n",
      "   Predicted: i<EOS><EOS><EOS>g<EOS>\n",
      "   Ground Truth: irung<EOS>\n",
      "\n",
      "[2] CER: 1.250\n",
      "   Predicted: o<EOS><EOS><EOS>\n",
      "   Ground Truth: ong<EOS>\n",
      "\n",
      "[3] CER: 1.200\n",
      "   Predicted: ra<EOS>g<EOS><EOS>\n",
      "   Ground Truth: panti<EOS>\n",
      "\n",
      "[4] CER: 1.100\n",
      "   Predicted: aar<EOS><EOS><EOS>\n",
      "   Ground Truth: warna<EOS>\n",
      "\n",
      "[5] CER: 1.100\n",
      "   Predicted: jat<EOS><EOS><EOS>\n",
      "   Ground Truth: jagat<EOS>\n",
      "\n",
      "[6/20] Train Loss: 0.3824, Train CER: 0.0988 | Val Loss: 0.2787, Val CER: 0.0813\n",
      "\n",
      "Epoch 7/20\n",
      "Batch 50/437 - Loss: 0.3416\n",
      "Batch 100/437 - Loss: 0.2971\n",
      "Batch 150/437 - Loss: 0.1706\n",
      "Batch 200/437 - Loss: 0.4520\n",
      "Batch 250/437 - Loss: 0.3342\n",
      "Batch 300/437 - Loss: 0.5959\n",
      "Batch 350/437 - Loss: 0.2621\n",
      "Batch 400/437 - Loss: 0.1398\n",
      "\n",
      "=== Top 5 Worst Samples by CER ===\n",
      "[1] CER: 1.375\n",
      "   Predicted: p<EOS><EOS><EOS>\n",
      "   Ground Truth: wre<EOS>\n",
      "\n",
      "[2] CER: 1.200\n",
      "   Predicted: .<EOS><EOS>gi<EOS>\n",
      "   Ground Truth: panti<EOS>\n",
      "\n",
      "[3] CER: 1.111\n",
      "   Predicted: sa<EOS><EOS><EOS>\n",
      "   Ground Truth: sang<EOS>\n",
      "\n",
      "[4] CER: 1.000\n",
      "   Predicted: bw\n",
      "   Ground Truth: 3<EOS>\n",
      "\n",
      "[5] CER: 1.000\n",
      "   Predicted: wad\n",
      "   Ground Truth: di<EOS>\n",
      "\n",
      "[7/20] Train Loss: 0.3232, Train CER: 0.0841 | Val Loss: 0.2585, Val CER: 0.0627\n",
      "\n",
      "Epoch 8/20\n",
      "Batch 50/437 - Loss: 0.2167\n",
      "Batch 100/437 - Loss: 0.1990\n",
      "Batch 150/437 - Loss: 0.1096\n",
      "Batch 200/437 - Loss: 0.2323\n",
      "Batch 250/437 - Loss: 0.1818\n",
      "Batch 300/437 - Loss: 0.3761\n",
      "Batch 350/437 - Loss: 0.4354\n",
      "Batch 400/437 - Loss: 0.1615\n",
      "\n",
      "=== Top 5 Worst Samples by CER ===\n",
      "[1] CER: 1.545\n",
      "   Predicted: re<EOS>a<EOS><EOS><EOS>\n",
      "   Ground Truth: meneng<EOS>\n",
      "\n",
      "[2] CER: 1.375\n",
      "   Predicted: r<EOS><EOS><EOS>\n",
      "   Ground Truth: wre<EOS>\n",
      "\n",
      "[3] CER: 1.200\n",
      "   Predicted: .<EOS><EOS>ta<EOS>\n",
      "   Ground Truth: panti<EOS>\n",
      "\n",
      "[4] CER: 1.083\n",
      "   Predicted: daari<EOS><EOS><EOS>\n",
      "   Ground Truth: bhatari<EOS>\n",
      "\n",
      "[5] CER: 1.000\n",
      "   Predicted: wad\n",
      "   Ground Truth: di<EOS>\n",
      "\n",
      "[8/20] Train Loss: 0.2794, Train CER: 0.0721 | Val Loss: 0.2587, Val CER: 0.0627\n",
      "\n",
      "Epoch 9/20\n",
      "Batch 50/437 - Loss: 0.1394\n",
      "Batch 100/437 - Loss: 0.1687\n",
      "Batch 150/437 - Loss: 0.1935\n",
      "Batch 200/437 - Loss: 0.1676\n",
      "Batch 250/437 - Loss: 0.3432\n",
      "Batch 300/437 - Loss: 0.4303\n",
      "Batch 350/437 - Loss: 0.2461\n",
      "Batch 400/437 - Loss: 0.1502\n",
      "\n",
      "=== Top 5 Worst Samples by CER ===\n",
      "[1] CER: 1.375\n",
      "   Predicted: r<EOS><EOS><EOS>\n",
      "   Ground Truth: wre<EOS>\n",
      "\n",
      "[2] CER: 1.000\n",
      "   Predicted: jona<EOS><EOS>a<EOS>\n",
      "   Ground Truth: jenyana<EOS>\n",
      "\n",
      "[3] CER: 1.000\n",
      "   Predicted: wad\n",
      "   Ground Truth: di<EOS>\n",
      "\n",
      "[4] CER: 1.000\n",
      "   Predicted: ka\n",
      "   Ground Truth: n<EOS>\n",
      "\n",
      "[5] CER: 1.000\n",
      "   Predicted: iru<EOS><EOS><EOS>\n",
      "   Ground Truth: irung<EOS>\n",
      "\n",
      "[9/20] Train Loss: 0.2112, Train CER: 0.0513 | Val Loss: 0.2507, Val CER: 0.0667\n",
      "\n",
      "Epoch 10/20\n",
      "Batch 50/437 - Loss: 0.2217\n",
      "Batch 100/437 - Loss: 0.0438\n",
      "Batch 150/437 - Loss: 0.1094\n",
      "Batch 200/437 - Loss: 0.1523\n",
      "Batch 250/437 - Loss: 0.2731\n",
      "Batch 300/437 - Loss: 0.1630\n",
      "Batch 350/437 - Loss: 0.1087\n",
      "Batch 400/437 - Loss: 0.1135\n",
      "\n",
      "=== Top 5 Worst Samples by CER ===\n",
      "[1] CER: 1.600\n",
      "   Predicted: da<EOS><EOS><EOS><EOS>\n",
      "   Ground Truth: panti<EOS>\n",
      "\n",
      "[2] CER: 1.417\n",
      "   Predicted: data<EOS><EOS><EOS><EOS>\n",
      "   Ground Truth: bhatari<EOS>\n",
      "\n",
      "[3] CER: 1.375\n",
      "   Predicted: w<EOS><EOS><EOS>\n",
      "   Ground Truth: sah<EOS>\n",
      "\n",
      "[4] CER: 1.250\n",
      "   Predicted: w<EOS><EOS><EOS>\n",
      "   Ground Truth: wre<EOS>\n",
      "\n",
      "[5] CER: 1.222\n",
      "   Predicted: ra<EOS><EOS><EOS>\n",
      "   Ground Truth: ring<EOS>\n",
      "\n",
      "[10/20] Train Loss: 0.1877, Train CER: 0.0481 | Val Loss: 0.2465, Val CER: 0.0613\n",
      "\n",
      "Epoch 11/20\n",
      "Batch 50/437 - Loss: 0.1998\n",
      "Batch 100/437 - Loss: 0.1038\n",
      "Batch 150/437 - Loss: 0.1079\n",
      "Batch 200/437 - Loss: 0.2253\n",
      "Batch 250/437 - Loss: 0.3872\n",
      "Batch 300/437 - Loss: 0.1906\n",
      "Batch 350/437 - Loss: 0.1025\n",
      "Batch 400/437 - Loss: 0.4092\n",
      "\n",
      "=== Top 5 Worst Samples by CER ===\n",
      "[1] CER: 1.667\n",
      "   Predicted: tan<EOS><EOS><EOS><EOS><EOS>\n",
      "   Ground Truth: tanhana<EOS>\n",
      "\n",
      "[2] CER: 1.375\n",
      "   Predicted: p<EOS><EOS><EOS>\n",
      "   Ground Truth: wre<EOS>\n",
      "\n",
      "[3] CER: 1.222\n",
      "   Predicted: ra<EOS><EOS><EOS>\n",
      "   Ground Truth: wara<EOS>\n",
      "\n",
      "[4] CER: 1.200\n",
      "   Predicted: ha<EOS><EOS>a<EOS>\n",
      "   Ground Truth: panti<EOS>\n",
      "\n",
      "[5] CER: 1.111\n",
      "   Predicted: sa<EOS><EOS><EOS>\n",
      "   Ground Truth: sang<EOS>\n",
      "\n",
      "[11/20] Train Loss: 0.1518, Train CER: 0.0366 | Val Loss: 0.2676, Val CER: 0.0691\n",
      "\n",
      "Epoch 12/20\n",
      "Batch 50/437 - Loss: 0.0552\n",
      "Batch 100/437 - Loss: 0.1454\n",
      "Batch 150/437 - Loss: 0.2738\n",
      "Batch 200/437 - Loss: 0.3164\n",
      "Batch 250/437 - Loss: 0.0299\n",
      "Batch 300/437 - Loss: 0.0298\n",
      "Batch 350/437 - Loss: 0.2585\n",
      "Batch 400/437 - Loss: 0.3040\n",
      "\n",
      "=== Top 5 Worst Samples by CER ===\n",
      "[1] CER: 1.500\n",
      "   Predicted: pa<EOS><EOS><EOS><EOS>\n",
      "   Ground Truth: panti<EOS>\n",
      "\n",
      "[2] CER: 1.375\n",
      "   Predicted: a<EOS><EOS><EOS>\n",
      "   Ground Truth: sah<EOS>\n",
      "\n",
      "[3] CER: 1.222\n",
      "   Predicted: iw<EOS><EOS><EOS>\n",
      "   Ground Truth: kiwa<EOS>\n",
      "\n",
      "[4] CER: 1.100\n",
      "   Predicted: rri<EOS><EOS><EOS>\n",
      "   Ground Truth: gring<EOS>\n",
      "\n",
      "[5] CER: 1.100\n",
      "   Predicted: up<EOS>a<EOS><EOS>\n",
      "   Ground Truth: utama<EOS>\n",
      "\n",
      "[12/20] Train Loss: 0.1511, Train CER: 0.0384 | Val Loss: 0.2452, Val CER: 0.0614\n",
      "\n",
      "Epoch 13/20\n",
      "Batch 50/437 - Loss: 0.1401\n",
      "Batch 100/437 - Loss: 0.2266\n",
      "Batch 150/437 - Loss: 0.2301\n",
      "Batch 200/437 - Loss: 0.1534\n",
      "Batch 250/437 - Loss: 0.2672\n",
      "Batch 300/437 - Loss: 0.1832\n",
      "Batch 350/437 - Loss: 0.2358\n",
      "Batch 400/437 - Loss: 0.1926\n",
      "\n",
      "=== Top 5 Worst Samples by CER ===\n",
      "[1] CER: 1.222\n",
      "   Predicted: sh<EOS><EOS><EOS>\n",
      "   Ground Truth: sang<EOS>\n",
      "\n",
      "[2] CER: 1.200\n",
      "   Predicted: da<EOS><EOS>a<EOS>\n",
      "   Ground Truth: panti<EOS>\n",
      "\n",
      "[3] CER: 1.000\n",
      "   Predicted: ng\n",
      "   Ground Truth: 3<EOS>\n",
      "\n",
      "[4] CER: 1.000\n",
      "   Predicted: wad\n",
      "   Ground Truth: di<EOS>\n",
      "\n",
      "[5] CER: 1.000\n",
      "   Predicted: nga\n",
      "   Ground Truth: da<EOS>\n",
      "\n",
      "[13/20] Train Loss: 0.1704, Train CER: 0.0416 | Val Loss: 0.2975, Val CER: 0.0696\n",
      "\n",
      "Epoch 14/20\n",
      "Batch 50/437 - Loss: 0.0898\n",
      "Batch 100/437 - Loss: 0.0824\n",
      "Batch 150/437 - Loss: 0.0428\n",
      "Batch 200/437 - Loss: 0.3332\n",
      "Batch 250/437 - Loss: 0.0673\n",
      "Batch 300/437 - Loss: 0.3347\n",
      "Batch 350/437 - Loss: 0.1900\n",
      "Batch 400/437 - Loss: 0.3107\n",
      "\n",
      "=== Top 5 Worst Samples by CER ===\n",
      "[1] CER: 1.600\n",
      "   Predicted: ya<EOS><EOS><EOS><EOS>\n",
      "   Ground Truth: panti<EOS>\n",
      "\n",
      "[2] CER: 1.375\n",
      "   Predicted: p<EOS><EOS><EOS>\n",
      "   Ground Truth: wre<EOS>\n",
      "\n",
      "[3] CER: 1.200\n",
      "   Predicted: upe<EOS><EOS><EOS>\n",
      "   Ground Truth: utama<EOS>\n",
      "\n",
      "[4] CER: 1.000\n",
      "   Predicted: ny\n",
      "   Ground Truth: 3<EOS>\n",
      "\n",
      "[5] CER: 1.000\n",
      "   Predicted: wad\n",
      "   Ground Truth: di<EOS>\n",
      "\n",
      "[14/20] Train Loss: 0.1698, Train CER: 0.0429 | Val Loss: 0.2685, Val CER: 0.0627\n",
      "\n",
      "Epoch 15/20\n",
      "Batch 50/437 - Loss: 0.1417\n",
      "Batch 100/437 - Loss: 0.0746\n",
      "Batch 150/437 - Loss: 0.1792\n",
      "Batch 200/437 - Loss: 0.2516\n",
      "Batch 250/437 - Loss: 0.0995\n",
      "Batch 300/437 - Loss: 0.0603\n",
      "Batch 350/437 - Loss: 0.1631\n",
      "Batch 400/437 - Loss: 0.4341\n",
      "\n",
      "=== Top 5 Worst Samples by CER ===\n",
      "[1] CER: 1.500\n",
      "   Predicted: pa<EOS><EOS><EOS><EOS>\n",
      "   Ground Truth: panti<EOS>\n",
      "\n",
      "[2] CER: 1.375\n",
      "   Predicted: s<EOS><EOS><EOS>\n",
      "   Ground Truth: wre<EOS>\n",
      "\n",
      "[3] CER: 1.300\n",
      "   Predicted: wre<EOS><EOS><EOS>\n",
      "   Ground Truth: utama<EOS>\n",
      "\n",
      "[4] CER: 1.111\n",
      "   Predicted: sa<EOS><EOS><EOS>\n",
      "   Ground Truth: sang<EOS>\n",
      "\n",
      "[5] CER: 1.100\n",
      "   Predicted: ira<EOS><EOS><EOS>\n",
      "   Ground Truth: irung<EOS>\n",
      "\n",
      "[15/20] Train Loss: 0.1462, Train CER: 0.0366 | Val Loss: 0.2787, Val CER: 0.0656\n",
      "\n",
      "Epoch 16/20\n",
      "Batch 50/437 - Loss: 0.0223\n",
      "Batch 100/437 - Loss: 0.1087\n",
      "Batch 150/437 - Loss: 0.0444\n",
      "Batch 200/437 - Loss: 0.0928\n",
      "Batch 250/437 - Loss: 0.1279\n",
      "Batch 300/437 - Loss: 0.0195\n",
      "Batch 350/437 - Loss: 0.1088\n",
      "Batch 400/437 - Loss: 0.1616\n",
      "\n",
      "=== Top 5 Worst Samples by CER ===\n",
      "[1] CER: 1.375\n",
      "   Predicted: p<EOS><EOS><EOS>\n",
      "   Ground Truth: wre<EOS>\n",
      "\n",
      "[2] CER: 1.222\n",
      "   Predicted: ni<EOS><EOS><EOS>\n",
      "   Ground Truth: nang<EOS>\n",
      "\n",
      "[3] CER: 1.222\n",
      "   Predicted: ra<EOS><EOS><EOS>\n",
      "   Ground Truth: ring<EOS>\n",
      "\n",
      "[4] CER: 1.111\n",
      "   Predicted: sa<EOS><EOS><EOS>\n",
      "   Ground Truth: sang<EOS>\n",
      "\n",
      "[5] CER: 1.000\n",
      "   Predicted: wong\n",
      "   Ground Truth: sah<EOS>\n",
      "\n",
      "[16/20] Train Loss: 0.1033, Train CER: 0.0253 | Val Loss: 0.2381, Val CER: 0.0617\n",
      "\n",
      "Epoch 17/20\n",
      "Batch 50/437 - Loss: 0.0218\n",
      "Batch 100/437 - Loss: 0.0693\n",
      "Batch 150/437 - Loss: 0.0582\n",
      "Batch 200/437 - Loss: 0.0253\n",
      "Batch 250/437 - Loss: 0.0256\n",
      "Batch 300/437 - Loss: 0.3268\n",
      "Batch 350/437 - Loss: 0.0391\n",
      "Batch 400/437 - Loss: 0.1120\n",
      "\n",
      "=== Top 5 Worst Samples by CER ===\n",
      "[1] CER: 1.000\n",
      "   Predicted: wad\n",
      "   Ground Truth: di<EOS>\n",
      "\n",
      "[2] CER: 1.000\n",
      "   Predicted: ka\n",
      "   Ground Truth: n<EOS>\n",
      "\n",
      "[3] CER: 1.000\n",
      "   Predicted: pa\n",
      "   Ground Truth: .<EOS>\n",
      "\n",
      "[4] CER: 1.000\n",
      "   Predicted: ng\n",
      "   Ground Truth: 1<EOS>\n",
      "\n",
      "[5] CER: 1.000\n",
      "   Predicted: nga\n",
      "   Ground Truth: ar<EOS>\n",
      "\n",
      "[17/20] Train Loss: 0.0948, Train CER: 0.0236 | Val Loss: 0.2396, Val CER: 0.0578\n",
      "\n",
      "Epoch 18/20\n",
      "Batch 50/437 - Loss: 0.0271\n",
      "Batch 100/437 - Loss: 0.0566\n",
      "Batch 150/437 - Loss: 0.2413\n",
      "Batch 200/437 - Loss: 0.1766\n",
      "Batch 250/437 - Loss: 0.3418\n",
      "Batch 300/437 - Loss: 0.2324\n",
      "Batch 350/437 - Loss: 0.0961\n",
      "Batch 400/437 - Loss: 0.1124\n",
      "\n",
      "=== Top 5 Worst Samples by CER ===\n",
      "[1] CER: 2.100\n",
      "   Predicted: .<EOS><EOS><EOS><EOS><EOS>\n",
      "   Ground Truth: panti<EOS>\n",
      "\n",
      "[2] CER: 1.500\n",
      "   Predicted: adai<EOS><EOS><EOS><EOS>\n",
      "   Ground Truth: bhatari<EOS>\n",
      "\n",
      "[3] CER: 1.375\n",
      "   Predicted: r<EOS><EOS><EOS>\n",
      "   Ground Truth: wre<EOS>\n",
      "\n",
      "[4] CER: 1.000\n",
      "   Predicted: wad\n",
      "   Ground Truth: di<EOS>\n",
      "\n",
      "[5] CER: 1.000\n",
      "   Predicted: iru<EOS><EOS><EOS>\n",
      "   Ground Truth: irung<EOS>\n",
      "\n",
      "[18/20] Train Loss: 0.1097, Train CER: 0.0266 | Val Loss: 0.3046, Val CER: 0.0629\n",
      "\n",
      "Epoch 19/20\n",
      "Batch 50/437 - Loss: 0.0811\n",
      "Batch 100/437 - Loss: 0.1218\n",
      "Batch 150/437 - Loss: 0.1146\n",
      "Batch 200/437 - Loss: 0.0901\n",
      "Batch 250/437 - Loss: 0.3304\n",
      "Batch 300/437 - Loss: 0.0579\n",
      "Batch 350/437 - Loss: 0.0445\n",
      "Batch 400/437 - Loss: 0.3609\n",
      "\n",
      "=== Top 5 Worst Samples by CER ===\n",
      "[1] CER: 1.000\n",
      "   Predicted: paks\n",
      "   Ground Truth: iki<EOS>\n",
      "\n",
      "[2] CER: 1.000\n",
      "   Predicted: wad\n",
      "   Ground Truth: di<EOS>\n",
      "\n",
      "[3] CER: 1.000\n",
      "   Predicted: ng\n",
      "   Ground Truth: 2<EOS>\n",
      "\n",
      "[4] CER: 1.000\n",
      "   Predicted: pa\n",
      "   Ground Truth: .<EOS>\n",
      "\n",
      "[5] CER: 1.000\n",
      "   Predicted: ba\n",
      "   Ground Truth: .<EOS>\n",
      "\n",
      "[19/20] Train Loss: 0.1329, Train CER: 0.0317 | Val Loss: 0.3337, Val CER: 0.0676\n",
      "\n",
      "Epoch 20/20\n",
      "Batch 50/437 - Loss: 0.0492\n",
      "Batch 100/437 - Loss: 0.1192\n",
      "Batch 150/437 - Loss: 0.1790\n",
      "Batch 200/437 - Loss: 0.1299\n",
      "Batch 250/437 - Loss: 0.1127\n",
      "Batch 300/437 - Loss: 0.0524\n",
      "Batch 350/437 - Loss: 0.0935\n",
      "Batch 400/437 - Loss: 0.1681\n",
      "\n",
      "=== Top 5 Worst Samples by CER ===\n",
      "[1] CER: 1.375\n",
      "   Predicted: 3<EOS><EOS><EOS>\n",
      "   Ground Truth: ong<EOS>\n",
      "\n",
      "[2] CER: 1.222\n",
      "   Predicted: ta<EOS><EOS><EOS>\n",
      "   Ground Truth: sang<EOS>\n",
      "\n",
      "[3] CER: 1.000\n",
      "   Predicted: bu\n",
      "   Ground Truth: 3<EOS>\n",
      "\n",
      "[4] CER: 1.000\n",
      "   Predicted: wad\n",
      "   Ground Truth: di<EOS>\n",
      "\n",
      "[5] CER: 1.000\n",
      "   Predicted: iru<EOS><EOS><EOS>\n",
      "   Ground Truth: irung<EOS>\n",
      "\n",
      "[20/20] Train Loss: 0.1381, Train CER: 0.0349 | Val Loss: 0.2651, Val CER: 0.0555\n",
      "\n",
      "Training completed in 0h 44m.\n",
      "\n",
      "Results have been written to: training_results.csv\n",
      "Encoder and decoder models saved: encoder_vit_base_patch16_224.pth, decoder_vit_base_patch16_224.pth\n"
     ]
    }
   ],
   "source": [
    "trainer_base = ImageCaptioningTrainer(\n",
    "    encoder=encoder_vit_base,\n",
    "    decoder=decoder_vit_base,\n",
    "    criterion=criterion,                   \n",
    "    encoder_optimizer=encoder_optimizer_base,\n",
    "    decoder_optimizer=decoder_optimizer_base,\n",
    "    train_loader=train_loader,             \n",
    "    val_loader=val_loader,\n",
    "    device=device,\n",
    "    char_to_idx=char_to_idx,\n",
    "    idx_to_char=idx_to_char,\n",
    "    max_label_length=max_label_length,\n",
    "    model_name=\"vit_base_patch16_224\"     \n",
    ")\n",
    "\n",
    "num_epochs = 20\n",
    "trainer_base.fit(num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "03ba64b4-1d56-4251-8fa2-867c20f144e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory cleared\n"
     ]
    }
   ],
   "source": [
    "# Manually delete references to free GPU memory\n",
    "del encoder_vit_base\n",
    "del decoder_vit_base\n",
    "del trainer_base\n",
    "\n",
    "# Empty the PyTorch CUDA cache\n",
    "torch.cuda.empty_cache()\n",
    "print(\"Memory cleared\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "32c54344-6499-4417-b171-9dfa5df0af4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_vit_large = ViTEncoder(model_name=\"vit_large_patch16_224\", pretrained=True)\n",
    "decoder_vit_large = DecoderRNN(\n",
    "    attention_dim=256,\n",
    "    embed_dim=256,\n",
    "    decoder_dim=512,\n",
    "    vocab_size=vocab_size,\n",
    "    encoder_dim=encoder_vit_large.encoder_dim,\n",
    "    teacher_forcing_ratio=0.5\n",
    ")\n",
    "\n",
    "encoder_vit_large = encoder_vit_large.to(device)\n",
    "decoder_vit_large = decoder_vit_large.to(device)\n",
    "\n",
    "encoder_optimizer_large = optim.Adam(encoder_vit_large.parameters(), lr=1e-4)\n",
    "decoder_optimizer_large = optim.Adam(decoder_vit_large.parameters(), lr=4e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "da8647e0-a810-42d1-a922-a11e25b339f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/20\n",
      "Batch 50/437 - Loss: 2.3285\n",
      "Batch 100/437 - Loss: 2.2843\n",
      "Batch 150/437 - Loss: 2.1461\n",
      "Batch 200/437 - Loss: 1.6658\n",
      "Batch 250/437 - Loss: 2.0728\n",
      "Batch 300/437 - Loss: 1.8506\n",
      "Batch 350/437 - Loss: 1.6636\n",
      "Batch 400/437 - Loss: 1.5089\n",
      "\n",
      "=== Top 5 Worst Samples by CER ===\n",
      "[1] CER: 1.471\n",
      "   Predicted: maaniann<EOS><EOS><EOS><EOS><EOS>\n",
      "   Ground Truth: awighnamastu<EOS>\n",
      "\n",
      "[2] CER: 1.357\n",
      "   Predicted: saniii<EOS><EOS><EOS><EOS>\n",
      "   Ground Truth: candrania<EOS>\n",
      "\n",
      "[3] CER: 1.286\n",
      "   Predicted: sariii<EOS><EOS><EOS><EOS>\n",
      "   Ground Truth: saluiring<EOS>\n",
      "\n",
      "[4] CER: 1.200\n",
      "   Predicted: neg<EOS><EOS><EOS>\n",
      "   Ground Truth: nyepi<EOS>\n",
      "\n",
      "[5] CER: 1.200\n",
      "   Predicted: ra<EOS>a<EOS><EOS>\n",
      "   Ground Truth: panti<EOS>\n",
      "\n",
      "[1/20] Train Loss: 1.9144, Train CER: 0.6260 | Val Loss: 0.9997, Val CER: 0.2580\n",
      "\n",
      "Epoch 2/20\n",
      "Batch 50/437 - Loss: 1.5137\n",
      "Batch 100/437 - Loss: 1.0624\n",
      "Batch 150/437 - Loss: 1.3307\n",
      "Batch 200/437 - Loss: 1.0746\n",
      "Batch 250/437 - Loss: 1.0882\n",
      "Batch 300/437 - Loss: 0.7877\n",
      "Batch 350/437 - Loss: 0.8998\n",
      "Batch 400/437 - Loss: 0.8929\n",
      "\n",
      "=== Top 5 Worst Samples by CER ===\n",
      "[1] CER: 1.700\n",
      "   Predicted: .<EOS><EOS><EOS>a<EOS>\n",
      "   Ground Truth: panti<EOS>\n",
      "\n",
      "[2] CER: 1.500\n",
      "   Predicted: en<EOS><EOS><EOS><EOS>\n",
      "   Ground Truth: endan<EOS>\n",
      "\n",
      "[3] CER: 1.250\n",
      "   Predicted: e<EOS><EOS><EOS>\n",
      "   Ground Truth: eda<EOS>\n",
      "\n",
      "[4] CER: 1.250\n",
      "   Predicted: dari<EOS>g<EOS><EOS>\n",
      "   Ground Truth: bhatari<EOS>\n",
      "\n",
      "[5] CER: 1.200\n",
      "   Predicted: tmu<EOS><EOS><EOS>\n",
      "   Ground Truth: utama<EOS>\n",
      "\n",
      "[2/20] Train Loss: 1.0619, Train CER: 0.3074 | Val Loss: 0.4886, Val CER: 0.1387\n",
      "\n",
      "Epoch 3/20\n",
      "Batch 50/437 - Loss: 0.4261\n",
      "Batch 100/437 - Loss: 0.4783\n",
      "Batch 150/437 - Loss: 0.5910\n",
      "Batch 200/437 - Loss: 0.9102\n",
      "Batch 250/437 - Loss: 0.7483\n",
      "Batch 300/437 - Loss: 0.7069\n",
      "Batch 350/437 - Loss: 0.6150\n",
      "Batch 400/437 - Loss: 0.5251\n",
      "\n",
      "=== Top 5 Worst Samples by CER ===\n",
      "[1] CER: 1.667\n",
      "   Predicted: ren<EOS><EOS><EOS><EOS><EOS>\n",
      "   Ground Truth: rendang<EOS>\n",
      "\n",
      "[2] CER: 1.600\n",
      "   Predicted: .a<EOS><EOS><EOS><EOS>\n",
      "   Ground Truth: panti<EOS>\n",
      "\n",
      "[3] CER: 1.375\n",
      "   Predicted: u<EOS><EOS><EOS>\n",
      "   Ground Truth: ong<EOS>\n",
      "\n",
      "[4] CER: 1.333\n",
      "   Predicted: ba<EOS><EOS><EOS>\n",
      "   Ground Truth: nyuh<EOS>\n",
      "\n",
      "[5] CER: 1.222\n",
      "   Predicted: .<EOS><EOS>g<EOS>\n",
      "   Ground Truth: ring<EOS>\n",
      "\n",
      "[3/20] Train Loss: 0.6324, Train CER: 0.1761 | Val Loss: 0.3577, Val CER: 0.0951\n",
      "\n",
      "Epoch 4/20\n",
      "Batch 50/437 - Loss: 0.4951\n",
      "Batch 100/437 - Loss: 0.4581\n",
      "Batch 150/437 - Loss: 0.3712\n",
      "Batch 200/437 - Loss: 0.5885\n",
      "Batch 250/437 - Loss: 0.6151\n",
      "Batch 300/437 - Loss: 0.7085\n",
      "Batch 350/437 - Loss: 0.4468\n",
      "Batch 400/437 - Loss: 0.5168\n",
      "\n",
      "=== Top 5 Worst Samples by CER ===\n",
      "[1] CER: 2.100\n",
      "   Predicted: .<EOS><EOS><EOS><EOS><EOS>\n",
      "   Ground Truth: panti<EOS>\n",
      "\n",
      "[2] CER: 1.333\n",
      "   Predicted: ng<EOS><EOS><EOS>\n",
      "   Ground Truth: ring<EOS>\n",
      "\n",
      "[3] CER: 1.222\n",
      "   Predicted: sh<EOS><EOS><EOS>\n",
      "   Ground Truth: sang<EOS>\n",
      "\n",
      "[4] CER: 1.000\n",
      "   Predicted: nen\n",
      "   Ground Truth: ma<EOS>\n",
      "\n",
      "[5] CER: 1.000\n",
      "   Predicted: ng\n",
      "   Ground Truth: 3<EOS>\n",
      "\n",
      "[4/20] Train Loss: 0.4633, Train CER: 0.1269 | Val Loss: 0.3238, Val CER: 0.0761\n",
      "\n",
      "Epoch 5/20\n",
      "Batch 50/437 - Loss: 0.4560\n",
      "Batch 100/437 - Loss: 0.2617\n",
      "Batch 150/437 - Loss: 0.2917\n",
      "Batch 200/437 - Loss: 0.2978\n",
      "Batch 250/437 - Loss: 0.2766\n",
      "Batch 300/437 - Loss: 0.5745\n",
      "Batch 350/437 - Loss: 0.2818\n",
      "Batch 400/437 - Loss: 0.3599\n",
      "\n",
      "=== Top 5 Worst Samples by CER ===\n",
      "[1] CER: 1.182\n",
      "   Predicted: sitt<EOS><EOS><EOS>\n",
      "   Ground Truth: sangit<EOS>\n",
      "\n",
      "[2] CER: 1.000\n",
      "   Predicted: aaram<EOS><EOS><EOS>\n",
      "   Ground Truth: kagamel<EOS>\n",
      "\n",
      "[3] CER: 1.000\n",
      "   Predicted: maak\n",
      "   Ground Truth: iki<EOS>\n",
      "\n",
      "[4] CER: 1.000\n",
      "   Predicted: wad\n",
      "   Ground Truth: di<EOS>\n",
      "\n",
      "[5] CER: 1.000\n",
      "   Predicted: nn\n",
      "   Ground Truth: 2<EOS>\n",
      "\n",
      "[5/20] Train Loss: 0.3476, Train CER: 0.0938 | Val Loss: 0.2514, Val CER: 0.0692\n",
      "\n",
      "Epoch 6/20\n",
      "Batch 50/437 - Loss: 0.2418\n",
      "Batch 100/437 - Loss: 0.4768\n",
      "Batch 150/437 - Loss: 0.2531\n",
      "Batch 200/437 - Loss: 0.1762\n",
      "Batch 250/437 - Loss: 0.2262\n",
      "Batch 300/437 - Loss: 0.2968\n",
      "Batch 350/437 - Loss: 0.2006\n",
      "Batch 400/437 - Loss: 0.1435\n",
      "\n",
      "=== Top 5 Worst Samples by CER ===\n",
      "[1] CER: 1.375\n",
      "   Predicted: r<EOS><EOS><EOS>\n",
      "   Ground Truth: wre<EOS>\n",
      "\n",
      "[2] CER: 1.222\n",
      "   Predicted: ra<EOS><EOS><EOS>\n",
      "   Ground Truth: ring<EOS>\n",
      "\n",
      "[3] CER: 1.111\n",
      "   Predicted: su<EOS><EOS><EOS>\n",
      "   Ground Truth: susu<EOS>\n",
      "\n",
      "[4] CER: 1.100\n",
      "   Predicted: ira<EOS><EOS><EOS>\n",
      "   Ground Truth: irung<EOS>\n",
      "\n",
      "[5] CER: 1.000\n",
      "   Predicted: nga\n",
      "   Ground Truth: la<EOS>\n",
      "\n",
      "[6/20] Train Loss: 0.3004, Train CER: 0.0770 | Val Loss: 0.2523, Val CER: 0.0747\n",
      "\n",
      "Epoch 7/20\n",
      "Batch 50/437 - Loss: 0.1998\n",
      "Batch 100/437 - Loss: 0.1177\n",
      "Batch 150/437 - Loss: 0.2866\n",
      "Batch 200/437 - Loss: 0.3292\n",
      "Batch 250/437 - Loss: 0.0933\n",
      "Batch 300/437 - Loss: 0.1987\n",
      "Batch 350/437 - Loss: 0.2692\n",
      "Batch 400/437 - Loss: 0.0878\n",
      "\n",
      "=== Top 5 Worst Samples by CER ===\n",
      "[1] CER: 2.100\n",
      "   Predicted: .<EOS><EOS><EOS><EOS><EOS>\n",
      "   Ground Truth: panti<EOS>\n",
      "\n",
      "[2] CER: 1.222\n",
      "   Predicted: ni<EOS><EOS><EOS>\n",
      "   Ground Truth: nang<EOS>\n",
      "\n",
      "[3] CER: 1.200\n",
      "   Predicted: umm<EOS><EOS><EOS>\n",
      "   Ground Truth: utama<EOS>\n",
      "\n",
      "[4] CER: 1.000\n",
      "   Predicted: wewa\n",
      "   Ground Truth: sah<EOS>\n",
      "\n",
      "[5] CER: 1.000\n",
      "   Predicted: wad\n",
      "   Ground Truth: di<EOS>\n",
      "\n",
      "[7/20] Train Loss: 0.2442, Train CER: 0.0637 | Val Loss: 0.2604, Val CER: 0.0698\n",
      "\n",
      "Epoch 8/20\n",
      "Batch 50/437 - Loss: 0.2108\n",
      "Batch 100/437 - Loss: 0.1922\n",
      "Batch 150/437 - Loss: 0.3104\n",
      "Batch 200/437 - Loss: 0.2268\n",
      "Batch 250/437 - Loss: 0.0773\n",
      "Batch 300/437 - Loss: 0.1613\n",
      "Batch 350/437 - Loss: 0.4137\n",
      "Batch 400/437 - Loss: 0.2402\n",
      "\n",
      "=== Top 5 Worst Samples by CER ===\n",
      "[1] CER: 1.222\n",
      "   Predicted: ni<EOS><EOS><EOS>\n",
      "   Ground Truth: nang<EOS>\n",
      "\n",
      "[2] CER: 1.000\n",
      "   Predicted: nga\n",
      "   Ground Truth: da<EOS>\n",
      "\n",
      "[3] CER: 1.000\n",
      "   Predicted: wad\n",
      "   Ground Truth: di<EOS>\n",
      "\n",
      "[4] CER: 1.000\n",
      "   Predicted: nga\n",
      "   Ground Truth: da<EOS>\n",
      "\n",
      "[5] CER: 1.000\n",
      "   Predicted: yia\n",
      "   Ground Truth: pa<EOS>\n",
      "\n",
      "[8/20] Train Loss: 0.2122, Train CER: 0.0550 | Val Loss: 0.2502, Val CER: 0.0631\n",
      "\n",
      "Epoch 9/20\n",
      "Batch 50/437 - Loss: 0.0853\n",
      "Batch 100/437 - Loss: 0.1553\n",
      "Batch 150/437 - Loss: 0.1334\n",
      "Batch 200/437 - Loss: 0.1181\n",
      "Batch 250/437 - Loss: 0.0433\n",
      "Batch 300/437 - Loss: 0.2663\n",
      "Batch 350/437 - Loss: 0.1084\n",
      "Batch 400/437 - Loss: 0.2161\n",
      "\n",
      "=== Top 5 Worst Samples by CER ===\n",
      "[1] CER: 1.375\n",
      "   Predicted: 8<EOS><EOS><EOS>\n",
      "   Ground Truth: wre<EOS>\n",
      "\n",
      "[2] CER: 1.375\n",
      "   Predicted: u<EOS><EOS><EOS>\n",
      "   Ground Truth: ong<EOS>\n",
      "\n",
      "[3] CER: 1.200\n",
      "   Predicted: wum<EOS><EOS><EOS>\n",
      "   Ground Truth: utama<EOS>\n",
      "\n",
      "[4] CER: 1.167\n",
      "   Predicted: tatun<EOS><EOS><EOS>\n",
      "   Ground Truth: babakan<EOS>\n",
      "\n",
      "[5] CER: 1.000\n",
      "   Predicted: wad\n",
      "   Ground Truth: di<EOS>\n",
      "\n",
      "[9/20] Train Loss: 0.1824, Train CER: 0.0463 | Val Loss: 0.2434, Val CER: 0.0640\n",
      "\n",
      "Epoch 10/20\n",
      "Batch 50/437 - Loss: 0.0865\n",
      "Batch 100/437 - Loss: 0.1182\n",
      "Batch 150/437 - Loss: 0.1656\n",
      "Batch 200/437 - Loss: 0.3258\n",
      "Batch 250/437 - Loss: 0.2101\n",
      "Batch 300/437 - Loss: 0.1649\n",
      "Batch 350/437 - Loss: 0.0789\n",
      "Batch 400/437 - Loss: 0.2284\n",
      "\n",
      "=== Top 5 Worst Samples by CER ===\n",
      "[1] CER: 1.200\n",
      "   Predicted: niu<EOS><EOS><EOS>\n",
      "   Ground Truth: irung<EOS>\n",
      "\n",
      "[2] CER: 1.111\n",
      "   Predicted: sa<EOS><EOS><EOS>\n",
      "   Ground Truth: sang<EOS>\n",
      "\n",
      "[3] CER: 1.000\n",
      "   Predicted: wad\n",
      "   Ground Truth: di<EOS>\n",
      "\n",
      "[4] CER: 1.000\n",
      "   Predicted: ra\n",
      "   Ground Truth: n<EOS>\n",
      "\n",
      "[5] CER: 1.000\n",
      "   Predicted: nga\n",
      "   Ground Truth: ar<EOS>\n",
      "\n",
      "[10/20] Train Loss: 0.1730, Train CER: 0.0425 | Val Loss: 0.2528, Val CER: 0.0594\n",
      "\n",
      "Epoch 11/20\n",
      "Batch 50/437 - Loss: 0.1877\n",
      "Batch 100/437 - Loss: 0.4032\n",
      "Batch 150/437 - Loss: 0.0885\n",
      "Batch 200/437 - Loss: 0.2436\n",
      "Batch 250/437 - Loss: 0.3060\n",
      "Batch 300/437 - Loss: 0.0730\n",
      "Batch 350/437 - Loss: 0.2341\n",
      "Batch 400/437 - Loss: 0.2934\n",
      "\n",
      "=== Top 5 Worst Samples by CER ===\n",
      "[1] CER: 1.417\n",
      "   Predicted: talg<EOS><EOS><EOS><EOS>\n",
      "   Ground Truth: tanhana<EOS>\n",
      "\n",
      "[2] CER: 1.364\n",
      "   Predicted: sa<EOS><EOS><EOS>g<EOS>\n",
      "   Ground Truth: sareng<EOS>\n",
      "\n",
      "[3] CER: 1.222\n",
      "   Predicted: ta<EOS><EOS><EOS>\n",
      "   Ground Truth: wasa<EOS>\n",
      "\n",
      "[4] CER: 1.111\n",
      "   Predicted: na<EOS><EOS><EOS>\n",
      "   Ground Truth: nang<EOS>\n",
      "\n",
      "[5] CER: 1.100\n",
      "   Predicted: ira<EOS><EOS><EOS>\n",
      "   Ground Truth: irung<EOS>\n",
      "\n",
      "[11/20] Train Loss: 0.1921, Train CER: 0.0482 | Val Loss: 0.2266, Val CER: 0.0566\n",
      "\n",
      "Epoch 12/20\n",
      "Batch 50/437 - Loss: 0.0760\n",
      "Batch 100/437 - Loss: 0.0747\n",
      "Batch 150/437 - Loss: 0.1112\n",
      "Batch 200/437 - Loss: 0.1453\n",
      "Batch 250/437 - Loss: 0.1874\n",
      "Batch 300/437 - Loss: 0.2935\n",
      "Batch 350/437 - Loss: 0.3190\n",
      "Batch 400/437 - Loss: 0.0758\n",
      "\n",
      "=== Top 5 Worst Samples by CER ===\n",
      "[1] CER: 1.200\n",
      "   Predicted: nra<EOS><EOS><EOS>\n",
      "   Ground Truth: irung<EOS>\n",
      "\n",
      "[2] CER: 1.000\n",
      "   Predicted: rong\n",
      "   Ground Truth: sah<EOS>\n",
      "\n",
      "[3] CER: 1.000\n",
      "   Predicted: was\n",
      "   Ground Truth: di<EOS>\n",
      "\n",
      "[4] CER: 1.000\n",
      "   Predicted: pa\n",
      "   Ground Truth: .<EOS>\n",
      "\n",
      "[5] CER: 1.000\n",
      "   Predicted: da\n",
      "   Ground Truth: n<EOS>\n",
      "\n",
      "[12/20] Train Loss: 0.1324, Train CER: 0.0339 | Val Loss: 0.2041, Val CER: 0.0473\n",
      "\n",
      "Epoch 13/20\n",
      "Batch 50/437 - Loss: 0.0767\n",
      "Batch 100/437 - Loss: 0.0527\n",
      "Batch 150/437 - Loss: 0.2439\n",
      "Batch 200/437 - Loss: 0.4700\n",
      "Batch 250/437 - Loss: 0.0957\n",
      "Batch 300/437 - Loss: 0.1246\n",
      "Batch 350/437 - Loss: 0.0950\n",
      "Batch 400/437 - Loss: 0.0858\n",
      "\n",
      "=== Top 5 Worst Samples by CER ===\n",
      "[1] CER: 1.222\n",
      "   Predicted: ni<EOS><EOS><EOS>\n",
      "   Ground Truth: nang<EOS>\n",
      "\n",
      "[2] CER: 1.000\n",
      "   Predicted: nga\n",
      "   Ground Truth: la<EOS>\n",
      "\n",
      "[3] CER: 1.000\n",
      "   Predicted: karom<EOS><EOS><EOS>\n",
      "   Ground Truth: kagamel<EOS>\n",
      "\n",
      "[4] CER: 1.000\n",
      "   Predicted: wad\n",
      "   Ground Truth: di<EOS>\n",
      "\n",
      "[5] CER: 1.000\n",
      "   Predicted: iru<EOS><EOS><EOS>\n",
      "   Ground Truth: irung<EOS>\n",
      "\n",
      "[13/20] Train Loss: 0.1554, Train CER: 0.0391 | Val Loss: 0.2353, Val CER: 0.0539\n",
      "\n",
      "Epoch 14/20\n",
      "Batch 50/437 - Loss: 0.1463\n",
      "Batch 100/437 - Loss: 0.2032\n",
      "Batch 150/437 - Loss: 0.0327\n",
      "Batch 200/437 - Loss: 0.0350\n",
      "Batch 250/437 - Loss: 0.0950\n",
      "Batch 300/437 - Loss: 0.0860\n",
      "Batch 350/437 - Loss: 0.2619\n",
      "Batch 400/437 - Loss: 0.0314\n",
      "\n",
      "=== Top 5 Worst Samples by CER ===\n",
      "[1] CER: 1.700\n",
      "   Predicted: .<EOS><EOS><EOS>a<EOS>\n",
      "   Ground Truth: panti<EOS>\n",
      "\n",
      "[2] CER: 1.222\n",
      "   Predicted: ra<EOS><EOS><EOS>\n",
      "   Ground Truth: ring<EOS>\n",
      "\n",
      "[3] CER: 1.000\n",
      "   Predicted: wad\n",
      "   Ground Truth: di<EOS>\n",
      "\n",
      "[4] CER: 1.000\n",
      "   Predicted: iru<EOS><EOS><EOS>\n",
      "   Ground Truth: irung<EOS>\n",
      "\n",
      "[5] CER: 1.000\n",
      "   Predicted: da\n",
      "   Ground Truth: n<EOS>\n",
      "\n",
      "[14/20] Train Loss: 0.1129, Train CER: 0.0270 | Val Loss: 0.2434, Val CER: 0.0528\n",
      "\n",
      "Epoch 15/20\n",
      "Batch 50/437 - Loss: 0.0191\n",
      "Batch 100/437 - Loss: 0.0368\n",
      "Batch 150/437 - Loss: 0.0983\n",
      "Batch 200/437 - Loss: 0.1219\n",
      "Batch 250/437 - Loss: 0.0972\n",
      "Batch 300/437 - Loss: 0.1609\n",
      "Batch 350/437 - Loss: 0.3190\n",
      "Batch 400/437 - Loss: 0.1687\n",
      "\n",
      "=== Top 5 Worst Samples by CER ===\n",
      "[1] CER: 1.100\n",
      "   Predicted: aru<EOS><EOS><EOS>\n",
      "   Ground Truth: irung<EOS>\n",
      "\n",
      "[2] CER: 1.083\n",
      "   Predicted: aaar<EOS>r<EOS><EOS>\n",
      "   Ground Truth: bhatari<EOS>\n",
      "\n",
      "[3] CER: 1.000\n",
      "   Predicted: on\n",
      "   Ground Truth: 3<EOS>\n",
      "\n",
      "[4] CER: 1.000\n",
      "   Predicted: oong\n",
      "   Ground Truth: sah<EOS>\n",
      "\n",
      "[5] CER: 1.000\n",
      "   Predicted: wad\n",
      "   Ground Truth: di<EOS>\n",
      "\n",
      "[15/20] Train Loss: 0.1222, Train CER: 0.0303 | Val Loss: 0.2454, Val CER: 0.0606\n",
      "\n",
      "Epoch 16/20\n",
      "Batch 50/437 - Loss: 0.0645\n",
      "Batch 100/437 - Loss: 0.1420\n",
      "Batch 150/437 - Loss: 0.0197\n",
      "Batch 200/437 - Loss: 0.2813\n",
      "Batch 250/437 - Loss: 0.3279\n",
      "Batch 300/437 - Loss: 0.1337\n",
      "Batch 350/437 - Loss: 0.3347\n",
      "Batch 400/437 - Loss: 0.0339\n",
      "\n",
      "=== Top 5 Worst Samples by CER ===\n",
      "[1] CER: 1.667\n",
      "   Predicted: n<EOS><EOS><EOS><EOS>\n",
      "   Ground Truth: nang<EOS>\n",
      "\n",
      "[2] CER: 1.600\n",
      "   Predicted: .<EOS><EOS>a<EOS><EOS>\n",
      "   Ground Truth: panti<EOS>\n",
      "\n",
      "[3] CER: 1.500\n",
      "   Predicted: aras<EOS><EOS><EOS><EOS>\n",
      "   Ground Truth: kagamel<EOS>\n",
      "\n",
      "[4] CER: 1.200\n",
      "   Predicted: tum<EOS><EOS><EOS>\n",
      "   Ground Truth: utama<EOS>\n",
      "\n",
      "[5] CER: 1.167\n",
      "   Predicted: iwaru<EOS><EOS><EOS>\n",
      "   Ground Truth: bhatari<EOS>\n",
      "\n",
      "[16/20] Train Loss: 0.1242, Train CER: 0.0310 | Val Loss: 0.2322, Val CER: 0.0570\n",
      "\n",
      "Epoch 17/20\n",
      "Batch 50/437 - Loss: 0.0223\n",
      "Batch 100/437 - Loss: 0.0453\n",
      "Batch 150/437 - Loss: 0.0452\n",
      "Batch 200/437 - Loss: 0.1265\n",
      "Batch 250/437 - Loss: 0.0289\n",
      "Batch 300/437 - Loss: 0.1639\n",
      "Batch 350/437 - Loss: 0.1232\n",
      "Batch 400/437 - Loss: 0.0492\n",
      "\n",
      "=== Top 5 Worst Samples by CER ===\n",
      "[1] CER: 1.200\n",
      "   Predicted: wum<EOS><EOS><EOS>\n",
      "   Ground Truth: utama<EOS>\n",
      "\n",
      "[2] CER: 1.083\n",
      "   Predicted: aatak<EOS><EOS><EOS>\n",
      "   Ground Truth: tanhana<EOS>\n",
      "\n",
      "[3] CER: 1.000\n",
      "   Predicted: wad\n",
      "   Ground Truth: di<EOS>\n",
      "\n",
      "[4] CER: 1.000\n",
      "   Predicted: pa\n",
      "   Ground Truth: .<EOS>\n",
      "\n",
      "[5] CER: 1.000\n",
      "   Predicted: da\n",
      "   Ground Truth: n<EOS>\n",
      "\n",
      "[17/20] Train Loss: 0.0802, Train CER: 0.0193 | Val Loss: 0.2157, Val CER: 0.0499\n",
      "\n",
      "Epoch 18/20\n",
      "Batch 50/437 - Loss: 0.0506\n",
      "Batch 100/437 - Loss: 0.0193\n",
      "Batch 150/437 - Loss: 0.0238\n",
      "Batch 200/437 - Loss: 0.0398\n",
      "Batch 250/437 - Loss: 0.0523\n",
      "Batch 300/437 - Loss: 0.0502\n",
      "Batch 350/437 - Loss: 0.0225\n",
      "Batch 400/437 - Loss: 0.0216\n",
      "\n",
      "=== Top 5 Worst Samples by CER ===\n",
      "[1] CER: 1.250\n",
      "   Predicted: w<EOS><EOS><EOS>\n",
      "   Ground Truth: wre<EOS>\n",
      "\n",
      "[2] CER: 1.222\n",
      "   Predicted: ra<EOS><EOS><EOS>\n",
      "   Ground Truth: ring<EOS>\n",
      "\n",
      "[3] CER: 1.111\n",
      "   Predicted: sa<EOS><EOS><EOS>\n",
      "   Ground Truth: sang<EOS>\n",
      "\n",
      "[4] CER: 1.000\n",
      "   Predicted: cona\n",
      "   Ground Truth: sah<EOS>\n",
      "\n",
      "[5] CER: 1.000\n",
      "   Predicted: wasi\n",
      "   Ground Truth: iki<EOS>\n",
      "\n",
      "[18/20] Train Loss: 0.0639, Train CER: 0.0155 | Val Loss: 0.2495, Val CER: 0.0521\n",
      "\n",
      "Epoch 19/20\n",
      "Batch 50/437 - Loss: 0.1060\n",
      "Batch 100/437 - Loss: 0.1951\n",
      "Batch 150/437 - Loss: 0.0224\n",
      "Batch 200/437 - Loss: 0.1250\n",
      "Batch 250/437 - Loss: 0.0950\n",
      "Batch 300/437 - Loss: 0.1646\n",
      "Batch 350/437 - Loss: 0.1881\n",
      "Batch 400/437 - Loss: 0.2101\n",
      "\n",
      "=== Top 5 Worst Samples by CER ===\n",
      "[1] CER: 1.222\n",
      "   Predicted: ra<EOS><EOS><EOS>\n",
      "   Ground Truth: ring<EOS>\n",
      "\n",
      "[2] CER: 1.083\n",
      "   Predicted: takak<EOS><EOS><EOS>\n",
      "   Ground Truth: tanhana<EOS>\n",
      "\n",
      "[3] CER: 1.000\n",
      "   Predicted: pad\n",
      "   Ground Truth: di<EOS>\n",
      "\n",
      "[4] CER: 1.000\n",
      "   Predicted: le\n",
      "   Ground Truth: 2<EOS>\n",
      "\n",
      "[5] CER: 1.000\n",
      "   Predicted: iru<EOS><EOS><EOS>\n",
      "   Ground Truth: irung<EOS>\n",
      "\n",
      "[19/20] Train Loss: 0.1246, Train CER: 0.0311 | Val Loss: 0.2778, Val CER: 0.0667\n",
      "\n",
      "Epoch 20/20\n",
      "Batch 50/437 - Loss: 0.0488\n",
      "Batch 100/437 - Loss: 0.2771\n",
      "Batch 150/437 - Loss: 0.0768\n",
      "Batch 200/437 - Loss: 0.0502\n",
      "Batch 250/437 - Loss: 0.1438\n",
      "Batch 300/437 - Loss: 0.1475\n",
      "Batch 350/437 - Loss: 0.0735\n",
      "Batch 400/437 - Loss: 0.0170\n",
      "\n",
      "=== Top 5 Worst Samples by CER ===\n",
      "[1] CER: 1.222\n",
      "   Predicted: ra<EOS><EOS><EOS>\n",
      "   Ground Truth: ring<EOS>\n",
      "\n",
      "[2] CER: 1.000\n",
      "   Predicted: wad\n",
      "   Ground Truth: di<EOS>\n",
      "\n",
      "[3] CER: 1.000\n",
      "   Predicted: ng\n",
      "   Ground Truth: 2<EOS>\n",
      "\n",
      "[4] CER: 1.000\n",
      "   Predicted: iru<EOS><EOS><EOS>\n",
      "   Ground Truth: irung<EOS>\n",
      "\n",
      "[5] CER: 1.000\n",
      "   Predicted: wa\n",
      "   Ground Truth: n<EOS>\n",
      "\n",
      "[20/20] Train Loss: 0.1072, Train CER: 0.0249 | Val Loss: 0.2402, Val CER: 0.0544\n",
      "\n",
      "Training completed in 2h 7m.\n",
      "\n",
      "Results have been written to: training_results.csv\n",
      "Encoder and decoder models saved: encoder_vit_large_patch16_224.pth, decoder_vit_large_patch16_224.pth\n"
     ]
    }
   ],
   "source": [
    "trainer_large = ImageCaptioningTrainer(\n",
    "    encoder=encoder_vit_large,\n",
    "    decoder=decoder_vit_large,\n",
    "    criterion=criterion,                    \n",
    "    encoder_optimizer=encoder_optimizer_large,\n",
    "    decoder_optimizer=decoder_optimizer_large,\n",
    "    train_loader=train_loader,              \n",
    "    val_loader=val_loader,\n",
    "    device=device,\n",
    "    char_to_idx=char_to_idx,\n",
    "    idx_to_char=idx_to_char,\n",
    "    max_label_length=max_label_length,\n",
    "    model_name=\"vit_large_patch16_224\"     \n",
    ")\n",
    "\n",
    "num_epochs = 20\n",
    "trainer_large.fit(num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e3d1c3c0-8596-4856-89fd-adc41b2ca20e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory cleared\n"
     ]
    }
   ],
   "source": [
    "# Manually delete references to free GPU memory\n",
    "del encoder_vit_large\n",
    "del decoder_vit_large\n",
    "del trainer_large\n",
    "\n",
    "# Empty the PyTorch CUDA cache\n",
    "torch.cuda.empty_cache()\n",
    "print(\"Memory cleared\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "58f94879-7a3d-41a8-98cf-c64a0dc83d1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unknown characters in test labels: set()\n"
     ]
    }
   ],
   "source": [
    "test_ground_truth_path = os.path.join(base_dir, 'balinese_transliteration_test.txt')\n",
    "test_images_dir        = os.path.join(base_dir, 'balinese_word_test')\n",
    "\n",
    "test_filenames = []\n",
    "test_labels    = []\n",
    "\n",
    "with open(test_ground_truth_path, 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        line = line.strip()\n",
    "        if line:\n",
    "            parts = line.split(';')\n",
    "            if len(parts) == 2:\n",
    "                filename, label = parts\n",
    "                label = label.lower()\n",
    "                test_filenames.append(filename)\n",
    "                test_labels.append(label)\n",
    "            else:\n",
    "                print(f\"Skipping malformed line: {line}\")\n",
    "\n",
    "test_data = pd.DataFrame({\n",
    "    'filename': test_filenames,\n",
    "    'label': test_labels\n",
    "})\n",
    "\n",
    "# Check for unknown chars in test set\n",
    "test_chars = set(''.join(test_data['label']))\n",
    "unknown_chars = test_chars - set(char_to_idx.keys())\n",
    "print(f\"Unknown characters in test labels: {unknown_chars}\")\n",
    "\n",
    "# Encode test labels\n",
    "max_label_length_test = max(len(lbl) for lbl in test_data['label']) + 2\n",
    "def encode_label_test(label, char_to_idx, max_length):\n",
    "    encoded = (\n",
    "        [char_to_idx['<SOS>']] +\n",
    "        [char_to_idx.get(ch, char_to_idx['<UNK>']) for ch in label] +\n",
    "        [char_to_idx['<EOS>']]\n",
    "    )\n",
    "    if len(encoded) > max_length:\n",
    "        encoded = encoded[:max_length]\n",
    "    else:\n",
    "        encoded += [char_to_idx['<PAD>']] * (max_length - len(encoded))\n",
    "    return encoded\n",
    "\n",
    "test_data['encoded_label'] = test_data['label'].apply(lambda x: encode_label_test(x, char_to_idx, max_label_length_test))\n",
    "test_data['label_length']  = test_data['label'].apply(len)\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=(0.5, 0.5, 0.5),\n",
    "        std=(0.5, 0.5, 0.5)\n",
    "    )\n",
    "])\n",
    "\n",
    "test_dataset = BalineseDataset(test_data, test_images_dir, transform=test_transform)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "250eea54-506c-42fe-93a7-9077c9138cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(encoder, decoder, data_loader, device, char_to_idx, idx_to_char, max_seq_length, test_data):\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "\n",
    "    eos_idx = char_to_idx['<EOS>']\n",
    "    results = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (images, labels, label_lengths) in enumerate(data_loader):\n",
    "            images = images.to(device, non_blocking=True)\n",
    "            labels = labels.to(device, non_blocking=True)\n",
    "\n",
    "            batch_size  = images.size(0)\n",
    "            encoder_out = encoder(images)  # [B, num_patches, encoder_dim]\n",
    "\n",
    "            # Init LSTM state\n",
    "            h1, c1, h2, c2 = decoder.init_hidden_state(encoder_out)\n",
    "\n",
    "            # Start tokens (all <SOS>)\n",
    "            inputs = torch.full(\n",
    "                (batch_size,),\n",
    "                fill_value=char_to_idx['<SOS>'],\n",
    "                dtype=torch.long,\n",
    "                device=device\n",
    "            )\n",
    "\n",
    "            all_preds = []\n",
    "\n",
    "            for _ in range(max_seq_length):\n",
    "                # 1) Embedding\n",
    "                embeddings = decoder.embedding(inputs)\n",
    "\n",
    "                # 2) Attention\n",
    "                attention_weighted_encoding, _ = decoder.attention(encoder_out, h1)\n",
    "\n",
    "                # 3) Gating\n",
    "                gate = decoder.sigmoid(decoder.f_beta(h1))\n",
    "                attention_weighted_encoding = gate * attention_weighted_encoding\n",
    "\n",
    "                # 4) Pass through LSTM layers\n",
    "                h1, c1 = decoder.lstm1(\n",
    "                    torch.cat([embeddings, attention_weighted_encoding], dim=1),\n",
    "                    (h1, c1)\n",
    "                )\n",
    "                h2, c2 = decoder.lstm2(h1, (h2, c2))\n",
    "\n",
    "                # 5) Get predicted token\n",
    "                preds = decoder.fc(decoder.dropout(h2))  # [batch_size, vocab_size]\n",
    "                _, preds_idx = preds.max(dim=1)\n",
    "\n",
    "                # Feed next token\n",
    "                all_preds.append(preds_idx.cpu().numpy())\n",
    "                inputs = preds_idx\n",
    "\n",
    "            # Reformat predictions to [batch_size, max_seq_length]\n",
    "            all_preds = np.array(all_preds).T\n",
    "\n",
    "            for i in range(batch_size):\n",
    "                pred_indices = all_preds[i]\n",
    "\n",
    "                # Stop at <EOS> if present\n",
    "                if eos_idx in pred_indices:\n",
    "                    first_eos = np.where(pred_indices == eos_idx)[0][0]\n",
    "                    pred_indices = pred_indices[:first_eos]\n",
    "\n",
    "                # Convert token indices -> string\n",
    "                pred_chars = [idx_to_char.get(idx, '') for idx in pred_indices]\n",
    "                pred_str   = ''.join(pred_chars)\n",
    "\n",
    "                # Process ground truth\n",
    "                label_indices = labels[i].cpu().numpy()\n",
    "                # remove <SOS>\n",
    "                label_indices = label_indices[1:]\n",
    "\n",
    "                if eos_idx in label_indices:\n",
    "                    eos_pos = np.where(label_indices == eos_idx)[0][0]\n",
    "                    label_indices = label_indices[:eos_pos]\n",
    "                else:\n",
    "                    # remove <PAD> if present\n",
    "                    label_indices = label_indices[label_indices != char_to_idx['<PAD>']]\n",
    "\n",
    "                label_chars = [idx_to_char.get(idx, '') for idx in label_indices]\n",
    "                label_str   = ''.join(label_chars)\n",
    "\n",
    "                global_idx    = batch_idx * batch_size + i\n",
    "                image_filename= test_data.iloc[global_idx]['filename']\n",
    "\n",
    "                results.append({\n",
    "                    'image_filename': image_filename,\n",
    "                    'predicted_caption': pred_str,\n",
    "                    'ground_truth_caption': label_str\n",
    "                })\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "129f05ea-c521-4adb-bb29-087530f8d5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_global_cer(results):\n",
    "    total_ed   = 0\n",
    "    total_refs = 0\n",
    "    for r in results:\n",
    "        ref = r['ground_truth_caption']\n",
    "        hyp = r['predicted_caption']\n",
    "        dist = editdistance.eval(ref, hyp)\n",
    "        total_ed   += dist\n",
    "        total_refs += len(ref)\n",
    "    if total_refs == 0:\n",
    "        return 0.0\n",
    "    return total_ed / total_refs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "563ae92a-a81e-44c8-98d2-0f54bebb2044",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_top_worst_samples(results, n=5):\n",
    "    # Calculate CER for each sample\n",
    "    results_with_cer = []\n",
    "    for r in results:\n",
    "        ref = r['ground_truth_caption']\n",
    "        hyp = r['predicted_caption']\n",
    "        dist = editdistance.eval(ref, hyp)\n",
    "        length = len(ref)\n",
    "        cer = dist / length if length > 0 else 0\n",
    "        # Copy the record and add cer\n",
    "        new_r = r.copy()\n",
    "        new_r['cer'] = cer\n",
    "        results_with_cer.append(new_r)\n",
    "\n",
    "    # Sort by CER (descending) and take the top N\n",
    "    results_with_cer.sort(key=lambda x: x['cer'], reverse=True)\n",
    "    worst_samples = results_with_cer[:n]\n",
    "\n",
    "    print(f\"\\n=== Top {n} Worst Samples by CER ===\")\n",
    "    for i, sample in enumerate(worst_samples, start=1):\n",
    "        print(f\"{i}) Image: {sample['image_filename']}\")\n",
    "        print(f\"   CER: {sample['cer']:.4f}\")\n",
    "        print(f\"   Predicted       : {sample['predicted_caption']}\")\n",
    "        print(f\"   Ground Truth    : {sample['ground_truth_caption']}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f1db6414-0221-4426-8dd9-1251f74ffaf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/apps/miniforge3/24.3.0-0/miniforge3/envs/dsks_2024.06/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/apps/miniforge3/24.3.0-0/miniforge3/envs/dsks_2024.06/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global CER on test set: 0.1687\n",
      "\n",
      "=== Top 5 Worst Samples by CER ===\n",
      "1) Image: test1330.png\n",
      "   CER: 7.0000\n",
      "   Predicted       : angiala\n",
      "   Ground Truth    : .\n",
      "\n",
      "2) Image: test726.png\n",
      "   CER: 4.0000\n",
      "   Predicted       : bang\n",
      "   Ground Truth    : 6\n",
      "\n",
      "3) Image: test3142.png\n",
      "   CER: 4.0000\n",
      "   Predicted       : sang\n",
      "   Ground Truth    : i\n",
      "\n",
      "4) Image: test5688.png\n",
      "   CER: 4.0000\n",
      "   Predicted       : deng\n",
      "   Ground Truth    : i\n",
      "\n",
      "5) Image: test7679.png\n",
      "   CER: 4.0000\n",
      "   Predicted       : ling\n",
      "   Ground Truth    : 7\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cnn_encoder = ResNet18Encoder(pretrained=True)\n",
    "cnn_decoder = DecoderRNN(\n",
    "    attention_dim=256,\n",
    "    embed_dim=256,\n",
    "    decoder_dim=512,\n",
    "    vocab_size=vocab_size,\n",
    "    encoder_dim=cnn_encoder.encoder_dim,  # 512 for ResNet18\n",
    "    teacher_forcing_ratio=0.5\n",
    ")\n",
    "\n",
    "encoder_ckpt = \"encoder_resnet18_encoder.pth\"\n",
    "decoder_ckpt = \"decoder_resnet18_encoder.pth\"\n",
    "\n",
    "cnn_encoder.load_state_dict(torch.load(encoder_ckpt, map_location=device))\n",
    "cnn_decoder.load_state_dict(torch.load(decoder_ckpt, map_location=device))\n",
    "\n",
    "cnn_encoder = cnn_encoder.to(device)\n",
    "cnn_decoder = cnn_decoder.to(device)\n",
    "\n",
    "cnn_encoder.eval()\n",
    "cnn_decoder.eval()\n",
    "\n",
    "test_results = inference(\n",
    "    encoder=cnn_encoder,\n",
    "    decoder=cnn_decoder,\n",
    "    data_loader=test_loader,             \n",
    "    device=device,\n",
    "    char_to_idx=char_to_idx,\n",
    "    idx_to_char=idx_to_char,\n",
    "    max_seq_length=max_label_length_test, \n",
    "    test_data=test_data                   \n",
    ")\n",
    "\n",
    "global_cer = calculate_global_cer(test_results)\n",
    "print(f\"Global CER on test set: {global_cer:.4f}\")\n",
    "\n",
    "print_top_worst_samples(test_results, n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b8d3aabc-9daa-4163-95db-911d4351f98c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/apps/miniforge3/24.3.0-0/miniforge3/envs/dsks_2024.06/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global CER on test set: 4.7277\n",
      "\n",
      "=== Top 5 Worst Samples by CER ===\n",
      "1) Image: test5693.png\n",
      "   CER: 75.0000\n",
      "   Predicted       : yyggg<UNK><UNK><UNK><UNK><UNK><UNK><UNK><UNK><UNK><UNK><UNK><UNK><UNK><UNK>\n",
      "   Ground Truth    : .\n",
      "\n",
      "2) Image: test6904.png\n",
      "   CER: 75.0000\n",
      "   Predicted       : nnngg<UNK><UNK><UNK><UNK><UNK><UNK><UNK><UNK><UNK><UNK><UNK><UNK><UNK><UNK>\n",
      "   Ground Truth    : .\n",
      "\n",
      "3) Image: test7927.png\n",
      "   CER: 75.0000\n",
      "   Predicted       : ggggg<UNK><UNK><UNK><UNK><UNK><UNK><UNK><UNK><UNK><UNK><UNK><UNK><UNK><UNK>\n",
      "   Ground Truth    : .\n",
      "\n",
      "4) Image: test8503.png\n",
      "   CER: 71.0000\n",
      "   Predicted       : gggggg<UNK><UNK><UNK><UNK><UNK><UNK><UNK><UNK><UNK><UNK><UNK><UNK><UNK>\n",
      "   Ground Truth    : i\n",
      "\n",
      "5) Image: test6352.png\n",
      "   CER: 37.5000\n",
      "   Predicted       : ggggg<UNK><UNK><UNK><UNK><UNK><UNK><UNK><UNK><UNK><UNK><UNK><UNK><UNK><UNK>\n",
      "   Ground Truth    : ta\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cnn_encoder_50 = ResNet50Encoder(pretrained=True)\n",
    "cnn_decoder_50 = DecoderRNN(\n",
    "    attention_dim=256,\n",
    "    embed_dim=256,\n",
    "    decoder_dim=512,\n",
    "    vocab_size=vocab_size,\n",
    "    encoder_dim=cnn_encoder_50.encoder_dim,  # 2048 for ResNet50\n",
    "    teacher_forcing_ratio=0.5\n",
    ")\n",
    "\n",
    "encoder_ckpt = \"encoder_resnet50_encoder.pth\"\n",
    "decoder_ckpt = \"decoder_resnet50_encoder.pth\"\n",
    "\n",
    "cnn_encoder_50.load_state_dict(torch.load(encoder_ckpt, map_location=device))\n",
    "cnn_decoder_50.load_state_dict(torch.load(decoder_ckpt, map_location=device))\n",
    "\n",
    "# Move to device\n",
    "cnn_encoder_50 = cnn_encoder_50.to(device)\n",
    "cnn_decoder_50 = cnn_decoder_50.to(device)\n",
    "\n",
    "cnn_encoder_50.eval()\n",
    "cnn_decoder_50.eval()\n",
    "\n",
    "test_results = inference(\n",
    "    encoder=cnn_encoder_50,\n",
    "    decoder=cnn_decoder_50,\n",
    "    data_loader=test_loader,\n",
    "    device=device,\n",
    "    char_to_idx=char_to_idx,\n",
    "    idx_to_char=idx_to_char,\n",
    "    max_seq_length=max_label_length_test,\n",
    "    test_data=test_data\n",
    ")\n",
    "\n",
    "global_cer = calculate_global_cer(test_results)\n",
    "print(f\"Global CER on test set: {global_cer:.4f}\")\n",
    "\n",
    "print_top_worst_samples(test_results, n=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "48de36fc-280d-4686-b978-202edc0c2f06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global CER on Test Set (ViT-Base) : 0.1535\n",
      "\n",
      "=== Top 5 Worst Samples by CER ===\n",
      "1) Image: test1330.png\n",
      "   CER: 9.0000\n",
      "   Predicted       : tangsaara\n",
      "   Ground Truth    : .\n",
      "\n",
      "2) Image: test580.png\n",
      "   CER: 8.0000\n",
      "   Predicted       : lualenia\n",
      "   Ground Truth    : .\n",
      "\n",
      "3) Image: test6274.png\n",
      "   CER: 8.0000\n",
      "   Predicted       : bualanta\n",
      "   Ground Truth    : .\n",
      "\n",
      "4) Image: test8296.png\n",
      "   CER: 7.0000\n",
      "   Predicted       : lelenta\n",
      "   Ground Truth    : .\n",
      "\n",
      "5) Image: test10070.png\n",
      "   CER: 7.0000\n",
      "   Predicted       : lapanta\n",
      "   Ground Truth    : .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "encoder = ViTEncoder(model_name=\"vit_base_patch16_224\", pretrained=True).to(device)\n",
    "decoder = DecoderRNN(\n",
    "    attention_dim=256,\n",
    "    embed_dim=256,\n",
    "    decoder_dim=512,\n",
    "    vocab_size=vocab_size,\n",
    "    encoder_dim=encoder.encoder_dim, \n",
    "    teacher_forcing_ratio=0.5\n",
    ").to(device)\n",
    "\n",
    "encoder.load_state_dict(torch.load(\"encoder_vit_base_patch16_224.pth\", map_location=device))\n",
    "decoder.load_state_dict(torch.load(\"decoder_vit_base_patch16_224.pth\", map_location=device))\n",
    "\n",
    "test_results = inference(\n",
    "    encoder=encoder,\n",
    "    decoder=decoder,\n",
    "    data_loader=test_loader,\n",
    "    device=device,\n",
    "    char_to_idx=char_to_idx,\n",
    "    idx_to_char=idx_to_char,\n",
    "    max_seq_length=max_label_length_test,\n",
    "    test_data=test_data\n",
    ")\n",
    "\n",
    "global_cer = calculate_global_cer(test_results)\n",
    "print(f\"Global CER on Test Set (ViT-Base) : {global_cer:.4f}\")\n",
    "\n",
    "print_top_worst_samples(test_results, n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5fe815c8-2f18-4d95-b6c0-c589c3097d6e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Global CER on Test Set (ViT-Large): 0.1295\n",
      "\n",
      "=== Top 5 Worst Samples by CER ===\n",
      "1) Image: test1330.png\n",
      "   CER: 8.0000\n",
      "   Predicted       : sangiana\n",
      "   Ground Truth    : .\n",
      "\n",
      "2) Image: test580.png\n",
      "   CER: 5.0000\n",
      "   Predicted       : siate\n",
      "   Ground Truth    : .\n",
      "\n",
      "3) Image: test1335.png\n",
      "   CER: 5.0000\n",
      "   Predicted       : panti\n",
      "   Ground Truth    : .\n",
      "\n",
      "4) Image: test2690.png\n",
      "   CER: 5.0000\n",
      "   Predicted       : jonga\n",
      "   Ground Truth    : b\n",
      "\n",
      "5) Image: test6297.png\n",
      "   CER: 5.0000\n",
      "   Predicted       : panti\n",
      "   Ground Truth    : .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "encoder_large = ViTEncoder(model_name=\"vit_large_patch16_224\", pretrained=True).to(device)\n",
    "decoder_large = DecoderRNN(\n",
    "    attention_dim=256,\n",
    "    embed_dim=256,\n",
    "    decoder_dim=512,\n",
    "    vocab_size=vocab_size,\n",
    "    encoder_dim=encoder_large.encoder_dim,\n",
    "    teacher_forcing_ratio=0.5\n",
    ").to(device)\n",
    "\n",
    "encoder_large.load_state_dict(torch.load(\"encoder_vit_large_patch16_224.pth\", map_location=device))\n",
    "decoder_large.load_state_dict(torch.load(\"decoder_vit_large_patch16_224.pth\", map_location=device))\n",
    "\n",
    "test_results_large = inference(\n",
    "    encoder=encoder_large,\n",
    "    decoder=decoder_large,\n",
    "    data_loader=test_loader,\n",
    "    device=device,\n",
    "    char_to_idx=char_to_idx,\n",
    "    idx_to_char=idx_to_char,\n",
    "    max_seq_length=max_label_length_test,\n",
    "    test_data=test_data\n",
    ")\n",
    "\n",
    "global_cer_large = calculate_global_cer(test_results_large)\n",
    "print(f\"\\nGlobal CER on Test Set (ViT-Large): {global_cer_large:.4f}\")\n",
    "\n",
    "print_top_worst_samples(test_results_large, n=5)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
