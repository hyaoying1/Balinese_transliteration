{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9284512d-f54b-45ac-a73d-ad35969d623a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "===== System Resource Check =====\n",
      "Model: microsoft/trocr-small-handwritten\n",
      "Free Disk Space: 24.06 GB (Need: 0.59 GB)\n",
      "GPU: NVIDIA A40\n",
      "GPU Memory: 45416.12 MB (Need: 1200.00 MB)\n",
      "Disk Space: SUFFICIENT\n",
      "GPU Memory: SUFFICIENT\n",
      "Loaded 15022 image-text pairs\n",
      "Unique labels: 4702\n",
      "Training size: 12922; Validation size: 1050; Test size: 1050\n",
      "Memory cleaned up\n",
      "Loading TrOCR model and processor...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yhuang1/.local/lib/python3.9/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "2025-03-28 12:46:49.884671: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-03-28 12:46:49.998070: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-03-28 12:46:50.026452: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-03-28 12:46:50.778956: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/11.0/extras/CUPTI/lib64:/usr/local/cuda/11.0/lib64:/usr/local/cuda/11.0/lib:/usr/local/cuda/11.0/lib64/stubs\n",
      "2025-03-28 12:46:50.779022: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/11.0/extras/CUPTI/lib64:/usr/local/cuda/11.0/lib64:/usr/local/cuda/11.0/lib:/usr/local/cuda/11.0/lib64/stubs\n",
      "2025-03-28 12:46:50.779028: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "Some weights of VisionEncoderDecoderModel were not initialized from the model checkpoint at microsoft/trocr-small-handwritten and are newly initialized: ['encoder.pooler.dense.weight', 'encoder.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/15\n",
      "Batch 50/1616, Loss: 14.4848\n",
      "Batch 100/1616, Loss: 1.9431\n",
      "Batch 150/1616, Loss: 0.5997\n",
      "Batch 200/1616, Loss: 0.4329\n",
      "Batch 250/1616, Loss: 0.4850\n",
      "Batch 300/1616, Loss: 0.3088\n",
      "Batch 350/1616, Loss: 0.2684\n",
      "Batch 400/1616, Loss: 0.2979\n",
      "Batch 450/1616, Loss: 0.1864\n",
      "Batch 500/1616, Loss: 0.2160\n",
      "Batch 550/1616, Loss: 0.1454\n",
      "Batch 600/1616, Loss: 0.1908\n",
      "Batch 650/1616, Loss: 0.1282\n",
      "Batch 700/1616, Loss: 0.0913\n",
      "Batch 750/1616, Loss: 0.1236\n",
      "Batch 800/1616, Loss: 0.1153\n",
      "Batch 850/1616, Loss: 0.1203\n",
      "Batch 900/1616, Loss: 0.1266\n",
      "Batch 950/1616, Loss: 0.1162\n",
      "Batch 1000/1616, Loss: 0.1137\n",
      "Batch 1050/1616, Loss: 0.0884\n",
      "Batch 1100/1616, Loss: 0.1314\n",
      "Batch 1150/1616, Loss: 0.1028\n",
      "Batch 1200/1616, Loss: 0.0487\n",
      "Batch 1250/1616, Loss: 0.0333\n",
      "Batch 1300/1616, Loss: 0.0982\n",
      "Batch 1350/1616, Loss: 0.1533\n",
      "Batch 1400/1616, Loss: 0.1326\n",
      "Batch 1450/1616, Loss: 0.1077\n",
      "Batch 1500/1616, Loss: 0.1184\n",
      "Batch 1550/1616, Loss: 0.0896\n",
      "Batch 1600/1616, Loss: 0.1277\n",
      "Epoch 1: Train Loss = 0.9813, Val Loss = 0.0584\n",
      "Saved best model at epoch 1\n",
      "\n",
      "Epoch 2/15\n",
      "Batch 50/1616, Loss: 0.1062\n",
      "Batch 100/1616, Loss: 0.0526\n",
      "Batch 150/1616, Loss: 0.1126\n",
      "Batch 200/1616, Loss: 0.0781\n",
      "Batch 250/1616, Loss: 0.0748\n",
      "Batch 300/1616, Loss: 0.1033\n",
      "Batch 350/1616, Loss: 0.0822\n",
      "Batch 400/1616, Loss: 0.0859\n",
      "Batch 450/1616, Loss: 0.0533\n",
      "Batch 500/1616, Loss: 0.0784\n",
      "Batch 550/1616, Loss: 0.0626\n",
      "Batch 600/1616, Loss: 0.0744\n",
      "Batch 650/1616, Loss: 0.0828\n",
      "Batch 700/1616, Loss: 0.0436\n",
      "Batch 750/1616, Loss: 0.0694\n",
      "Batch 800/1616, Loss: 0.1013\n",
      "Batch 850/1616, Loss: 0.1000\n",
      "Batch 900/1616, Loss: 0.0806\n",
      "Batch 950/1616, Loss: 0.0859\n",
      "Batch 1000/1616, Loss: 0.1016\n",
      "Batch 1050/1616, Loss: 0.0484\n",
      "Batch 1100/1616, Loss: 0.0560\n",
      "Batch 1150/1616, Loss: 0.0530\n",
      "Batch 1200/1616, Loss: 0.0891\n",
      "Batch 1250/1616, Loss: 0.0837\n",
      "Batch 1300/1616, Loss: 0.0758\n",
      "Batch 1350/1616, Loss: 0.0913\n",
      "Batch 1400/1616, Loss: 0.0738\n",
      "Batch 1450/1616, Loss: 0.0955\n",
      "Batch 1500/1616, Loss: 0.0843\n",
      "Batch 1550/1616, Loss: 0.0538\n",
      "Batch 1600/1616, Loss: 0.1253\n",
      "Epoch 2: Train Loss = 0.0787, Val Loss = 0.0446\n",
      "Saved best model at epoch 2\n",
      "\n",
      "Epoch 3/15\n",
      "Batch 50/1616, Loss: 0.0542\n",
      "Batch 100/1616, Loss: 0.0596\n",
      "Batch 150/1616, Loss: 0.0808\n",
      "Batch 200/1616, Loss: 0.0593\n",
      "Batch 250/1616, Loss: 0.0731\n",
      "Batch 300/1616, Loss: 0.0908\n",
      "Batch 350/1616, Loss: 0.0697\n",
      "Batch 400/1616, Loss: 0.0953\n",
      "Batch 450/1616, Loss: 0.0740\n",
      "Batch 500/1616, Loss: 0.0391\n",
      "Batch 550/1616, Loss: 0.0493\n",
      "Batch 600/1616, Loss: 0.0804\n",
      "Batch 650/1616, Loss: 0.0740\n",
      "Batch 700/1616, Loss: 0.0606\n",
      "Batch 750/1616, Loss: 0.0591\n",
      "Batch 800/1616, Loss: 0.0673\n",
      "Batch 850/1616, Loss: 0.0688\n",
      "Batch 900/1616, Loss: 0.0866\n",
      "Batch 950/1616, Loss: 0.0670\n",
      "Batch 1000/1616, Loss: 0.0676\n",
      "Batch 1050/1616, Loss: 0.0595\n",
      "Batch 1100/1616, Loss: 0.0635\n",
      "Batch 1150/1616, Loss: 0.0683\n",
      "Batch 1200/1616, Loss: 0.0785\n",
      "Batch 1250/1616, Loss: 0.0254\n",
      "Batch 1300/1616, Loss: 0.0701\n",
      "Batch 1350/1616, Loss: 0.0247\n",
      "Batch 1400/1616, Loss: 0.0670\n",
      "Batch 1450/1616, Loss: 0.0214\n",
      "Batch 1500/1616, Loss: 0.0474\n",
      "Batch 1550/1616, Loss: 0.0251\n",
      "Batch 1600/1616, Loss: 0.0440\n",
      "Epoch 3: Train Loss = 0.0649, Val Loss = 0.0315\n",
      "Saved best model at epoch 3\n",
      "\n",
      "Epoch 4/15\n",
      "Batch 50/1616, Loss: 0.0451\n",
      "Batch 100/1616, Loss: 0.0400\n",
      "Batch 150/1616, Loss: 0.0883\n",
      "Batch 200/1616, Loss: 0.0376\n",
      "Batch 250/1616, Loss: 0.0462\n",
      "Batch 300/1616, Loss: 0.0544\n",
      "Batch 350/1616, Loss: 0.0447\n",
      "Batch 400/1616, Loss: 0.0371\n",
      "Batch 450/1616, Loss: 0.0587\n",
      "Batch 500/1616, Loss: 0.0661\n",
      "Batch 550/1616, Loss: 0.0276\n",
      "Batch 600/1616, Loss: 0.0608\n",
      "Batch 650/1616, Loss: 0.0417\n",
      "Batch 700/1616, Loss: 0.0536\n",
      "Batch 750/1616, Loss: 0.0240\n",
      "Batch 800/1616, Loss: 0.0261\n",
      "Batch 850/1616, Loss: 0.0570\n",
      "Batch 900/1616, Loss: 0.0806\n",
      "Batch 950/1616, Loss: 0.0655\n",
      "Batch 1000/1616, Loss: 0.0639\n",
      "Batch 1050/1616, Loss: 0.0258\n",
      "Batch 1100/1616, Loss: 0.0805\n",
      "Batch 1150/1616, Loss: 0.0714\n",
      "Batch 1200/1616, Loss: 0.0536\n",
      "Batch 1250/1616, Loss: 0.0210\n",
      "Batch 1300/1616, Loss: 0.0589\n",
      "Batch 1350/1616, Loss: 0.0384\n",
      "Batch 1400/1616, Loss: 0.0538\n",
      "Batch 1450/1616, Loss: 0.0704\n",
      "Batch 1500/1616, Loss: 0.0304\n",
      "Batch 1550/1616, Loss: 0.0467\n",
      "Batch 1600/1616, Loss: 0.0295\n",
      "Epoch 4: Train Loss = 0.0476, Val Loss = 0.0210\n",
      "Saved best model at epoch 4\n",
      "\n",
      "Epoch 5/15\n",
      "Batch 50/1616, Loss: 0.0346\n",
      "Batch 100/1616, Loss: 0.0199\n",
      "Batch 150/1616, Loss: 0.0506\n",
      "Batch 200/1616, Loss: 0.0333\n",
      "Batch 250/1616, Loss: 0.0624\n",
      "Batch 300/1616, Loss: 0.0259\n",
      "Batch 350/1616, Loss: 0.0350\n",
      "Batch 400/1616, Loss: 0.0123\n",
      "Batch 450/1616, Loss: 0.0256\n",
      "Batch 500/1616, Loss: 0.0515\n",
      "Batch 550/1616, Loss: 0.0111\n",
      "Batch 600/1616, Loss: 0.0554\n",
      "Batch 650/1616, Loss: 0.0507\n",
      "Batch 700/1616, Loss: 0.0488\n",
      "Batch 750/1616, Loss: 0.0193\n",
      "Batch 800/1616, Loss: 0.0369\n",
      "Batch 850/1616, Loss: 0.0296\n",
      "Batch 900/1616, Loss: 0.0571\n",
      "Batch 950/1616, Loss: 0.0296\n",
      "Batch 1000/1616, Loss: 0.0254\n",
      "Batch 1050/1616, Loss: 0.0493\n",
      "Batch 1100/1616, Loss: 0.0267\n",
      "Batch 1150/1616, Loss: 0.0351\n",
      "Batch 1200/1616, Loss: 0.0174\n",
      "Batch 1250/1616, Loss: 0.0325\n",
      "Batch 1300/1616, Loss: 0.0153\n",
      "Batch 1350/1616, Loss: 0.0258\n",
      "Batch 1400/1616, Loss: 0.0539\n",
      "Batch 1450/1616, Loss: 0.0080\n",
      "Batch 1500/1616, Loss: 0.0161\n",
      "Batch 1550/1616, Loss: 0.0348\n",
      "Batch 1600/1616, Loss: 0.0877\n",
      "Epoch 5: Train Loss = 0.0362, Val Loss = 0.0151\n",
      "Saved best model at epoch 5\n",
      "\n",
      "Epoch 6/15\n",
      "Batch 50/1616, Loss: 0.0125\n",
      "Batch 100/1616, Loss: 0.0248\n",
      "Batch 150/1616, Loss: 0.0295\n",
      "Batch 200/1616, Loss: 0.0252\n",
      "Batch 250/1616, Loss: 0.0204\n",
      "Batch 300/1616, Loss: 0.0548\n",
      "Batch 350/1616, Loss: 0.0197\n",
      "Batch 400/1616, Loss: 0.0233\n",
      "Batch 450/1616, Loss: 0.0238\n",
      "Batch 500/1616, Loss: 0.0161\n",
      "Batch 550/1616, Loss: 0.0454\n",
      "Batch 600/1616, Loss: 0.0361\n",
      "Batch 650/1616, Loss: 0.0273\n",
      "Batch 700/1616, Loss: 0.0229\n",
      "Batch 750/1616, Loss: 0.0024\n",
      "Batch 800/1616, Loss: 0.0246\n",
      "Batch 850/1616, Loss: 0.0389\n",
      "Batch 900/1616, Loss: 0.0336\n",
      "Batch 950/1616, Loss: 0.0501\n",
      "Batch 1000/1616, Loss: 0.0236\n",
      "Batch 1050/1616, Loss: 0.0234\n",
      "Batch 1100/1616, Loss: 0.0190\n",
      "Batch 1150/1616, Loss: 0.0095\n",
      "Batch 1200/1616, Loss: 0.0359\n",
      "Batch 1250/1616, Loss: 0.0115\n",
      "Batch 1300/1616, Loss: 0.0392\n",
      "Batch 1350/1616, Loss: 0.0236\n",
      "Batch 1400/1616, Loss: 0.0167\n",
      "Batch 1450/1616, Loss: 0.0235\n",
      "Batch 1500/1616, Loss: 0.0105\n",
      "Batch 1550/1616, Loss: 0.0188\n",
      "Batch 1600/1616, Loss: 0.0055\n",
      "Epoch 6: Train Loss = 0.0272, Val Loss = 0.0121\n",
      "Saved best model at epoch 6\n",
      "\n",
      "Epoch 7/15\n",
      "Batch 50/1616, Loss: 0.0169\n",
      "Batch 100/1616, Loss: 0.0100\n",
      "Batch 150/1616, Loss: 0.0287\n",
      "Batch 200/1616, Loss: 0.0068\n",
      "Batch 250/1616, Loss: 0.0073\n",
      "Batch 300/1616, Loss: 0.0388\n",
      "Batch 350/1616, Loss: 0.0130\n",
      "Batch 400/1616, Loss: 0.0043\n",
      "Batch 450/1616, Loss: 0.0329\n",
      "Batch 500/1616, Loss: 0.0267\n",
      "Batch 550/1616, Loss: 0.0256\n",
      "Batch 600/1616, Loss: 0.0249\n",
      "Batch 650/1616, Loss: 0.0329\n",
      "Batch 700/1616, Loss: 0.0093\n",
      "Batch 750/1616, Loss: 0.0188\n",
      "Batch 800/1616, Loss: 0.0128\n",
      "Batch 850/1616, Loss: 0.0178\n",
      "Batch 900/1616, Loss: 0.0268\n",
      "Batch 950/1616, Loss: 0.0330\n",
      "Batch 1000/1616, Loss: 0.0101\n",
      "Batch 1050/1616, Loss: 0.0298\n",
      "Batch 1100/1616, Loss: 0.0302\n",
      "Batch 1150/1616, Loss: 0.0099\n",
      "Batch 1200/1616, Loss: 0.0162\n",
      "Batch 1250/1616, Loss: 0.0236\n",
      "Batch 1300/1616, Loss: 0.0188\n",
      "Batch 1350/1616, Loss: 0.0174\n",
      "Batch 1400/1616, Loss: 0.0145\n",
      "Batch 1450/1616, Loss: 0.0165\n",
      "Batch 1500/1616, Loss: 0.0293\n",
      "Batch 1550/1616, Loss: 0.0318\n",
      "Batch 1600/1616, Loss: 0.0144\n",
      "Epoch 7: Train Loss = 0.0211, Val Loss = 0.0095\n",
      "Saved best model at epoch 7\n",
      "\n",
      "Epoch 8/15\n",
      "Batch 50/1616, Loss: 0.0248\n",
      "Batch 100/1616, Loss: 0.0394\n",
      "Batch 150/1616, Loss: 0.0205\n",
      "Batch 200/1616, Loss: 0.0224\n",
      "Batch 250/1616, Loss: 0.0035\n",
      "Batch 300/1616, Loss: 0.0112\n",
      "Batch 350/1616, Loss: 0.0156\n",
      "Batch 400/1616, Loss: 0.0134\n",
      "Batch 450/1616, Loss: 0.0226\n",
      "Batch 500/1616, Loss: 0.0384\n",
      "Batch 550/1616, Loss: 0.0132\n",
      "Batch 600/1616, Loss: 0.0203\n",
      "Batch 650/1616, Loss: 0.0034\n",
      "Batch 700/1616, Loss: 0.0171\n",
      "Batch 750/1616, Loss: 0.0134\n",
      "Batch 800/1616, Loss: 0.0126\n",
      "Batch 850/1616, Loss: 0.0095\n",
      "Batch 900/1616, Loss: 0.0115\n",
      "Batch 950/1616, Loss: 0.0263\n",
      "Batch 1000/1616, Loss: 0.0051\n",
      "Batch 1050/1616, Loss: 0.0207\n",
      "Batch 1100/1616, Loss: 0.0105\n",
      "Batch 1150/1616, Loss: 0.0105\n",
      "Batch 1200/1616, Loss: 0.0274\n",
      "Batch 1250/1616, Loss: 0.0033\n",
      "Batch 1300/1616, Loss: 0.0059\n",
      "Batch 1350/1616, Loss: 0.0031\n",
      "Batch 1400/1616, Loss: 0.0036\n",
      "Batch 1450/1616, Loss: 0.0053\n",
      "Batch 1500/1616, Loss: 0.0203\n",
      "Batch 1550/1616, Loss: 0.0120\n",
      "Batch 1600/1616, Loss: 0.0298\n",
      "Epoch 8: Train Loss = 0.0172, Val Loss = 0.0085\n",
      "Saved best model at epoch 8\n",
      "\n",
      "Epoch 9/15\n",
      "Batch 50/1616, Loss: 0.0122\n",
      "Batch 100/1616, Loss: 0.0132\n",
      "Batch 150/1616, Loss: 0.0012\n",
      "Batch 200/1616, Loss: 0.0124\n",
      "Batch 250/1616, Loss: 0.0170\n",
      "Batch 300/1616, Loss: 0.0114\n",
      "Batch 350/1616, Loss: 0.0097\n",
      "Batch 400/1616, Loss: 0.0100\n",
      "Batch 450/1616, Loss: 0.0160\n",
      "Batch 500/1616, Loss: 0.0068\n",
      "Batch 550/1616, Loss: 0.0039\n",
      "Batch 600/1616, Loss: 0.0219\n",
      "Batch 650/1616, Loss: 0.0017\n",
      "Batch 700/1616, Loss: 0.0136\n",
      "Batch 750/1616, Loss: 0.0086\n",
      "Batch 800/1616, Loss: 0.0141\n",
      "Batch 850/1616, Loss: 0.0084\n",
      "Batch 900/1616, Loss: 0.0324\n",
      "Batch 950/1616, Loss: 0.0142\n",
      "Batch 1000/1616, Loss: 0.0194\n",
      "Batch 1050/1616, Loss: 0.0114\n",
      "Batch 1100/1616, Loss: 0.0069\n",
      "Batch 1150/1616, Loss: 0.0122\n",
      "Batch 1200/1616, Loss: 0.0078\n",
      "Batch 1250/1616, Loss: 0.0045\n",
      "Batch 1300/1616, Loss: 0.0030\n",
      "Batch 1350/1616, Loss: 0.0090\n",
      "Batch 1400/1616, Loss: 0.0044\n",
      "Batch 1450/1616, Loss: 0.0275\n",
      "Batch 1500/1616, Loss: 0.0209\n",
      "Batch 1550/1616, Loss: 0.0198\n",
      "Batch 1600/1616, Loss: 0.0164\n",
      "Epoch 9: Train Loss = 0.0132, Val Loss = 0.0069\n",
      "Saved best model at epoch 9\n",
      "\n",
      "Epoch 10/15\n",
      "Batch 50/1616, Loss: 0.0062\n",
      "Batch 100/1616, Loss: 0.0009\n",
      "Batch 150/1616, Loss: 0.0085\n",
      "Batch 200/1616, Loss: 0.0098\n",
      "Batch 250/1616, Loss: 0.0032\n",
      "Batch 300/1616, Loss: 0.0206\n",
      "Batch 350/1616, Loss: 0.0029\n",
      "Batch 400/1616, Loss: 0.0134\n",
      "Batch 450/1616, Loss: 0.0071\n",
      "Batch 500/1616, Loss: 0.0121\n",
      "Batch 550/1616, Loss: 0.0015\n",
      "Batch 600/1616, Loss: 0.0105\n",
      "Batch 650/1616, Loss: 0.0028\n",
      "Batch 700/1616, Loss: 0.0132\n",
      "Batch 750/1616, Loss: 0.0023\n",
      "Batch 800/1616, Loss: 0.0152\n",
      "Batch 850/1616, Loss: 0.0040\n",
      "Batch 900/1616, Loss: 0.0115\n",
      "Batch 950/1616, Loss: 0.0151\n",
      "Batch 1000/1616, Loss: 0.0179\n",
      "Batch 1050/1616, Loss: 0.0094\n",
      "Batch 1100/1616, Loss: 0.0095\n",
      "Batch 1150/1616, Loss: 0.0009\n",
      "Batch 1200/1616, Loss: 0.0272\n",
      "Batch 1250/1616, Loss: 0.0204\n",
      "Batch 1300/1616, Loss: 0.0283\n",
      "Batch 1350/1616, Loss: 0.0040\n",
      "Batch 1400/1616, Loss: 0.0079\n",
      "Batch 1450/1616, Loss: 0.0035\n",
      "Batch 1500/1616, Loss: 0.0064\n",
      "Batch 1550/1616, Loss: 0.0100\n",
      "Batch 1600/1616, Loss: 0.0092\n",
      "Epoch 10: Train Loss = 0.0099, Val Loss = 0.0062\n",
      "Saved best model at epoch 10\n",
      "\n",
      "Epoch 11/15\n",
      "Batch 50/1616, Loss: 0.0093\n",
      "Batch 100/1616, Loss: 0.0060\n",
      "Batch 150/1616, Loss: 0.0019\n",
      "Batch 200/1616, Loss: 0.0273\n",
      "Batch 250/1616, Loss: 0.0053\n",
      "Batch 300/1616, Loss: 0.0100\n",
      "Batch 350/1616, Loss: 0.0044\n",
      "Batch 400/1616, Loss: 0.0096\n",
      "Batch 450/1616, Loss: 0.0182\n",
      "Batch 500/1616, Loss: 0.0039\n",
      "Batch 550/1616, Loss: 0.0092\n",
      "Batch 600/1616, Loss: 0.0061\n",
      "Batch 650/1616, Loss: 0.0074\n",
      "Batch 700/1616, Loss: 0.0074\n",
      "Batch 750/1616, Loss: 0.0093\n",
      "Batch 800/1616, Loss: 0.0149\n",
      "Batch 850/1616, Loss: 0.0143\n",
      "Batch 900/1616, Loss: 0.0085\n",
      "Batch 950/1616, Loss: 0.0013\n",
      "Batch 1000/1616, Loss: 0.0021\n",
      "Batch 1050/1616, Loss: 0.0064\n",
      "Batch 1100/1616, Loss: 0.0213\n",
      "Batch 1150/1616, Loss: 0.0053\n",
      "Batch 1200/1616, Loss: 0.0059\n",
      "Batch 1250/1616, Loss: 0.0070\n",
      "Batch 1300/1616, Loss: 0.0049\n",
      "Batch 1350/1616, Loss: 0.0079\n",
      "Batch 1400/1616, Loss: 0.0045\n",
      "Batch 1450/1616, Loss: 0.0064\n",
      "Batch 1500/1616, Loss: 0.0124\n",
      "Batch 1550/1616, Loss: 0.0027\n",
      "Batch 1600/1616, Loss: 0.0121\n",
      "Epoch 11: Train Loss = 0.0074, Val Loss = 0.0060\n",
      "Saved best model at epoch 11\n",
      "\n",
      "Epoch 12/15\n",
      "Batch 50/1616, Loss: 0.0010\n",
      "Batch 100/1616, Loss: 0.0066\n",
      "Batch 150/1616, Loss: 0.0017\n",
      "Batch 200/1616, Loss: 0.0038\n",
      "Batch 250/1616, Loss: 0.0084\n",
      "Batch 300/1616, Loss: 0.0035\n",
      "Batch 350/1616, Loss: 0.0053\n",
      "Batch 400/1616, Loss: 0.0036\n",
      "Batch 450/1616, Loss: 0.0011\n",
      "Batch 500/1616, Loss: 0.0023\n",
      "Batch 550/1616, Loss: 0.0068\n",
      "Batch 600/1616, Loss: 0.0012\n",
      "Batch 650/1616, Loss: 0.0009\n",
      "Batch 700/1616, Loss: 0.0014\n",
      "Batch 750/1616, Loss: 0.0020\n",
      "Batch 800/1616, Loss: 0.0008\n",
      "Batch 850/1616, Loss: 0.0117\n",
      "Batch 900/1616, Loss: 0.0013\n",
      "Batch 950/1616, Loss: 0.0211\n",
      "Batch 1000/1616, Loss: 0.0048\n",
      "Batch 1050/1616, Loss: 0.0036\n",
      "Batch 1100/1616, Loss: 0.0026\n",
      "Batch 1150/1616, Loss: 0.0021\n",
      "Batch 1200/1616, Loss: 0.0030\n",
      "Batch 1250/1616, Loss: 0.0034\n",
      "Batch 1300/1616, Loss: 0.0055\n",
      "Batch 1350/1616, Loss: 0.0066\n",
      "Batch 1400/1616, Loss: 0.0088\n",
      "Batch 1450/1616, Loss: 0.0012\n",
      "Batch 1500/1616, Loss: 0.0059\n",
      "Batch 1550/1616, Loss: 0.0038\n",
      "Batch 1600/1616, Loss: 0.0113\n",
      "Epoch 12: Train Loss = 0.0052, Val Loss = 0.0063\n",
      "\n",
      "Epoch 13/15\n",
      "Batch 50/1616, Loss: 0.0012\n",
      "Batch 100/1616, Loss: 0.0024\n",
      "Batch 150/1616, Loss: 0.0057\n",
      "Batch 200/1616, Loss: 0.0005\n",
      "Batch 250/1616, Loss: 0.0047\n",
      "Batch 300/1616, Loss: 0.0041\n",
      "Batch 350/1616, Loss: 0.0012\n",
      "Batch 400/1616, Loss: 0.0017\n",
      "Batch 450/1616, Loss: 0.0048\n",
      "Batch 500/1616, Loss: 0.0029\n",
      "Batch 550/1616, Loss: 0.0029\n",
      "Batch 600/1616, Loss: 0.0051\n",
      "Batch 650/1616, Loss: 0.0011\n",
      "Batch 700/1616, Loss: 0.0018\n",
      "Batch 750/1616, Loss: 0.0087\n",
      "Batch 800/1616, Loss: 0.0012\n",
      "Batch 850/1616, Loss: 0.0003\n",
      "Batch 900/1616, Loss: 0.0008\n",
      "Batch 950/1616, Loss: 0.0012\n",
      "Batch 1000/1616, Loss: 0.0030\n",
      "Batch 1050/1616, Loss: 0.0011\n",
      "Batch 1100/1616, Loss: 0.0014\n",
      "Batch 1150/1616, Loss: 0.0012\n",
      "Batch 1200/1616, Loss: 0.0057\n",
      "Batch 1250/1616, Loss: 0.0006\n",
      "Batch 1300/1616, Loss: 0.0008\n",
      "Batch 1350/1616, Loss: 0.0045\n",
      "Batch 1400/1616, Loss: 0.0041\n",
      "Batch 1450/1616, Loss: 0.0036\n",
      "Batch 1500/1616, Loss: 0.0127\n",
      "Batch 1550/1616, Loss: 0.0056\n",
      "Batch 1600/1616, Loss: 0.0103\n",
      "Epoch 13: Train Loss = 0.0034, Val Loss = 0.0060\n",
      "\n",
      "Epoch 14/15\n",
      "Batch 50/1616, Loss: 0.0008\n",
      "Batch 100/1616, Loss: 0.0013\n",
      "Batch 150/1616, Loss: 0.0011\n",
      "Batch 200/1616, Loss: 0.0016\n",
      "Batch 250/1616, Loss: 0.0014\n",
      "Batch 300/1616, Loss: 0.0012\n",
      "Batch 350/1616, Loss: 0.0011\n",
      "Batch 400/1616, Loss: 0.0003\n",
      "Batch 450/1616, Loss: 0.0005\n",
      "Batch 500/1616, Loss: 0.0035\n",
      "Batch 550/1616, Loss: 0.0033\n",
      "Batch 600/1616, Loss: 0.0018\n",
      "Batch 650/1616, Loss: 0.0013\n",
      "Batch 700/1616, Loss: 0.0008\n",
      "Batch 750/1616, Loss: 0.0045\n",
      "Batch 800/1616, Loss: 0.0014\n",
      "Batch 850/1616, Loss: 0.0034\n",
      "Batch 900/1616, Loss: 0.0004\n",
      "Batch 950/1616, Loss: 0.0026\n",
      "Batch 1000/1616, Loss: 0.0022\n",
      "Batch 1050/1616, Loss: 0.0026\n",
      "Batch 1100/1616, Loss: 0.0069\n",
      "Batch 1150/1616, Loss: 0.0064\n",
      "Batch 1200/1616, Loss: 0.0005\n",
      "Batch 1250/1616, Loss: 0.0020\n",
      "Batch 1300/1616, Loss: 0.0005\n",
      "Batch 1350/1616, Loss: 0.0009\n",
      "Batch 1400/1616, Loss: 0.0038\n",
      "Batch 1450/1616, Loss: 0.0022\n",
      "Batch 1500/1616, Loss: 0.0019\n",
      "Batch 1550/1616, Loss: 0.0004\n",
      "Batch 1600/1616, Loss: 0.0022\n",
      "Epoch 14: Train Loss = 0.0021, Val Loss = 0.0058\n",
      "Saved best model at epoch 14\n",
      "\n",
      "Epoch 15/15\n",
      "Batch 50/1616, Loss: 0.0015\n",
      "Batch 100/1616, Loss: 0.0023\n",
      "Batch 150/1616, Loss: 0.0019\n",
      "Batch 200/1616, Loss: 0.0006\n",
      "Batch 250/1616, Loss: 0.0010\n",
      "Batch 300/1616, Loss: 0.0001\n",
      "Batch 350/1616, Loss: 0.0006\n",
      "Batch 400/1616, Loss: 0.0008\n",
      "Batch 450/1616, Loss: 0.0013\n",
      "Batch 500/1616, Loss: 0.0011\n",
      "Batch 550/1616, Loss: 0.0002\n",
      "Batch 600/1616, Loss: 0.0002\n",
      "Batch 650/1616, Loss: 0.0017\n",
      "Batch 700/1616, Loss: 0.0014\n",
      "Batch 750/1616, Loss: 0.0006\n",
      "Batch 800/1616, Loss: 0.0005\n",
      "Batch 850/1616, Loss: 0.0006\n",
      "Batch 900/1616, Loss: 0.0020\n",
      "Batch 950/1616, Loss: 0.0043\n",
      "Batch 1000/1616, Loss: 0.0009\n",
      "Batch 1050/1616, Loss: 0.0002\n",
      "Batch 1100/1616, Loss: 0.0061\n",
      "Batch 1150/1616, Loss: 0.0017\n",
      "Batch 1200/1616, Loss: 0.0014\n",
      "Batch 1250/1616, Loss: 0.0003\n",
      "Batch 1300/1616, Loss: 0.0020\n",
      "Batch 1350/1616, Loss: 0.0019\n",
      "Batch 1400/1616, Loss: 0.0015\n",
      "Batch 1450/1616, Loss: 0.0012\n",
      "Batch 1500/1616, Loss: 0.0025\n",
      "Batch 1550/1616, Loss: 0.0015\n",
      "Batch 1600/1616, Loss: 0.0062\n",
      "Epoch 15: Train Loss = 0.0014, Val Loss = 0.0058\n",
      "Saved best model at epoch 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yhuang1/.local/lib/python3.9/site-packages/transformers/generation/utils.py:1255: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n",
      "  warnings.warn(\n",
      "/home/yhuang1/.local/lib/python3.9/site-packages/transformers/generation/utils.py:1346: UserWarning: Using `max_length`'s default (64) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test CER: 0.0823\n",
      "\n",
      "Worst 5 predictions:\n",
      "1. Pred: 'turandupa' | GT: '.' | CER: 9.0000\n",
      "2. Pred: 'sang' | GT: '8' | CER: 4.0000\n",
      "3. Pred: 'tena' | GT: '2' | CER: 4.0000\n",
      "4. Pred: 'len' | GT: '.' | CER: 3.0000\n",
      "5. Pred: 'sul' | GT: 's' | CER: 2.0000\n",
      "Results saved to test_cer_results.csv\n",
      "Memory cleaned up\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import editdistance\n",
    "import time \n",
    "import warnings\n",
    "from PIL import Image\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "# For system resource checks\n",
    "def check_system_resources(model_name=\"microsoft/trocr-small-handwritten\"):\n",
    "    \"\"\"Check system resources to ensure we have enough space for TrOCR\"\"\"\n",
    "    try:\n",
    "        # Disk space check\n",
    "        total, used, free = shutil.disk_usage('/')\n",
    "        free_gb = free / (1024**3)  # Convert bytes to GB\n",
    "        \n",
    "        # Estimate model size based on variant\n",
    "        estimated_size_mb = {\n",
    "            \"small\": 300,    # ~300 MB for trocr-small\n",
    "            \"base\": 600,     # ~600 MB for trocr-base\n",
    "            \"large\": 1200    # ~1.2 GB for trocr-large\n",
    "        }\n",
    "        \n",
    "        # Determine model size category\n",
    "        size_category = 'small' if 'small' in model_name.lower() else \\\n",
    "                        'base' if 'base' in model_name.lower() else \\\n",
    "                        'large'\n",
    "        \n",
    "        model_size_mb = estimated_size_mb.get(size_category, 300)\n",
    "        model_size_gb = model_size_mb / 1024\n",
    "        \n",
    "        # GPU memory check\n",
    "        if torch.cuda.is_available():\n",
    "            gpu_info = torch.cuda.get_device_properties(0)\n",
    "            gpu_name = gpu_info.name\n",
    "            total_gpu_memory_mb = gpu_info.total_memory / (1024 * 1024)\n",
    "            \n",
    "            # Get current free memory\n",
    "            torch.cuda.empty_cache()\n",
    "            free_gpu_memory_mb = torch.cuda.memory_reserved(0) / (1024 * 1024)\n",
    "            \n",
    "            # Estimate required memory (very rough estimate)\n",
    "            required_gpu_mb = model_size_mb * 4  # Model size x4 for training overhead\n",
    "        else:\n",
    "            gpu_name = \"No GPU detected\"\n",
    "            total_gpu_memory_mb = 0\n",
    "            free_gpu_memory_mb = 0\n",
    "            required_gpu_mb = model_size_mb * 3\n",
    "        \n",
    "        # Check if system meets requirements\n",
    "        disk_sufficient = free_gb > (model_size_gb * 2)  # Need extra space for cache\n",
    "        gpu_sufficient = total_gpu_memory_mb > required_gpu_mb if torch.cuda.is_available() else False\n",
    "        \n",
    "        resources = {\n",
    "            \"free_disk_gb\": free_gb,\n",
    "            \"required_disk_gb\": model_size_gb * 2,\n",
    "            \"disk_sufficient\": disk_sufficient,\n",
    "            \"gpu_name\": gpu_name,\n",
    "            \"total_gpu_memory_mb\": total_gpu_memory_mb,\n",
    "            \"required_gpu_mb\": required_gpu_mb,\n",
    "            \"gpu_sufficient\": gpu_sufficient\n",
    "        }\n",
    "        \n",
    "        # Print results\n",
    "        print(\"\\n===== System Resource Check =====\")\n",
    "        print(f\"Model: {model_name}\")\n",
    "        print(f\"Free Disk Space: {free_gb:.2f} GB (Need: {model_size_gb * 2:.2f} GB)\")\n",
    "        print(f\"GPU: {gpu_name}\")\n",
    "        print(f\"GPU Memory: {total_gpu_memory_mb:.2f} MB (Need: {required_gpu_mb:.2f} MB)\")\n",
    "        print(f\"Disk Space: {'SUFFICIENT' if disk_sufficient else 'INSUFFICIENT'}\")\n",
    "        print(f\"GPU Memory: {'SUFFICIENT' if gpu_sufficient else 'INSUFFICIENT'}\")\n",
    "        \n",
    "        if not disk_sufficient or not gpu_sufficient:\n",
    "            print(\"\\n⚠️ WARNING: System resources may be insufficient for TrOCR model\")\n",
    "            if not disk_sufficient:\n",
    "                print(f\"  - Need {model_size_gb * 2:.2f} GB disk space, but only have {free_gb:.2f} GB\")\n",
    "            if not gpu_sufficient and torch.cuda.is_available():\n",
    "                print(f\"  - Need {required_gpu_mb:.2f} MB GPU memory, but only have {total_gpu_memory_mb:.2f} MB\")\n",
    "        \n",
    "        return resources\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error checking system resources: {e}\")\n",
    "        return None\n",
    "\n",
    "# Clean up function to free memory\n",
    "def cleanup():\n",
    "    \"\"\"Clean up memory and GPU cache\"\"\"\n",
    "    # Clean up global variables if they exist\n",
    "    for var in ['model', 'processor', 'tokenizer', 'feature_extractor']:\n",
    "        if var in globals():\n",
    "            del globals()[var]\n",
    "    \n",
    "    # Run garbage collection\n",
    "    gc.collect()\n",
    "    \n",
    "    # Clean CUDA cache if available\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    print(\"Memory cleaned up\")\n",
    "\n",
    "# Set up device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Check system resources before proceeding\n",
    "resources = check_system_resources(\"microsoft/trocr-small-handwritten\")\n",
    "if resources and (not resources[\"disk_sufficient\"] or not resources[\"gpu_sufficient\"]):\n",
    "    raise RuntimeError(\"System resources insufficient for TrOCR. See warnings above.\")\n",
    "\n",
    "# Base directory and paths\n",
    "base_dir = os.getcwd()\n",
    "ground_truth_path = os.path.join(base_dir, 'balinese_transliteration_train.txt') \n",
    "images_dir = os.path.join(base_dir, 'balinese_word_train')\n",
    "\n",
    "# Data loading function\n",
    "def load_data():\n",
    "    \"\"\"Load and prepare Balinese transliteration data\"\"\"\n",
    "    filenames = []\n",
    "    labels = []\n",
    "\n",
    "    with open(ground_truth_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            line = line.strip()\n",
    "            if line:  # Ensure the line is not empty\n",
    "                parts = line.split(';')\n",
    "                if len(parts) == 2:\n",
    "                    filename, label = parts\n",
    "                    label = label.lower()\n",
    "                    filenames.append(filename)\n",
    "                    labels.append(label)\n",
    "                else:\n",
    "                    print(f\"Skipping malformed line: {line}\")\n",
    "\n",
    "    data = pd.DataFrame({\n",
    "        'filename': filenames,\n",
    "        'label': labels\n",
    "    })\n",
    "    \n",
    "    print(f\"Loaded {len(data)} image-text pairs\")\n",
    "    \n",
    "    # Calculate label statistics\n",
    "    label_counts = data['label'].value_counts()\n",
    "    print(f\"Unique labels: {len(label_counts)}\")\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Load the data\n",
    "data = load_data()\n",
    "\n",
    "# Custom split function for train/val/test\n",
    "def custom_split(df, rare_label_threshold=3, val_size=0.1, test_size=0.1, random_state=42):\n",
    "    \"\"\"\n",
    "    Split data ensuring rare words are only in training set\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with 'label' column\n",
    "        rare_label_threshold: Words appearing fewer than this many times are considered rare\n",
    "        val_size: Proportion for validation set\n",
    "        test_size: Proportion for test set\n",
    "        random_state: Random seed for reproducibility\n",
    "    \n",
    "    Returns:\n",
    "        train_df, val_df, test_df\n",
    "    \"\"\"\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    \n",
    "    # Identify rare labels\n",
    "    label_counts = df['label'].value_counts()\n",
    "    rare_labels = label_counts[label_counts < rare_label_threshold].index\n",
    "    \n",
    "    # Separate rare and common words\n",
    "    rare_df = df[df['label'].isin(rare_labels)]\n",
    "    common_df = df[~df['label'].isin(rare_labels)]\n",
    "    \n",
    "    # First split common words into train+val and test\n",
    "    common_trainval, test_df = train_test_split(\n",
    "        common_df, \n",
    "        test_size=test_size, \n",
    "        random_state=random_state,\n",
    "        stratify=common_df['label'] if len(common_df) > 100 else None\n",
    "    )\n",
    "    \n",
    "    # Split train+val into train and val\n",
    "    train_common, val_df = train_test_split(\n",
    "        common_trainval,\n",
    "        test_size=val_size / (1 - test_size),  # Adjust to get correct proportion\n",
    "        random_state=random_state,\n",
    "        stratify=common_trainval['label'] if len(common_trainval) > 100 else None\n",
    "    )\n",
    "    \n",
    "    # Add rare words to train set\n",
    "    train_df = pd.concat([train_common, rare_df], ignore_index=True)\n",
    "    \n",
    "    # Shuffle all sets\n",
    "    train_df = train_df.sample(frac=1, random_state=random_state).reset_index(drop=True)\n",
    "    val_df = val_df.reset_index(drop=True)\n",
    "    test_df = test_df.reset_index(drop=True)\n",
    "    \n",
    "    print(f\"Training size: {len(train_df)}; Validation size: {len(val_df)}; Test size: {len(test_df)}\")\n",
    "    \n",
    "    return train_df, val_df, test_df\n",
    "\n",
    "# Split the data\n",
    "train_data, val_data, test_data = custom_split(data)\n",
    "\n",
    "# Create a fixed collate function for batching\n",
    "def collate_fn(batch):\n",
    "    \"\"\"Custom collate function to handle variable length inputs\"\"\"\n",
    "    pixel_values = torch.stack([item[\"pixel_values\"] for item in batch])\n",
    "    labels = torch.stack([item[\"labels\"] for item in batch])\n",
    "    return {\"pixel_values\": pixel_values, \"labels\": labels}\n",
    "\n",
    "# Create dataset class for TrOCR - CORRECTED VERSION\n",
    "class BalineseOCRDataset(Dataset):\n",
    "    def __init__(self, df, images_dir, processor=None, max_target_length=128):\n",
    "        self.data = df.reset_index(drop=True)\n",
    "        self.images_dir = images_dir\n",
    "        self.processor = processor\n",
    "        self.max_target_length = max_target_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.data.loc[idx, 'filename']\n",
    "        text = self.data.loc[idx, 'label']  # Raw text, not encoded\n",
    "        \n",
    "        img_path = os.path.join(self.images_dir, img_name)\n",
    "        # Load as PIL image - don't transform it here\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "        if self.processor:\n",
    "            # Process image and text separately\n",
    "            pixel_values = self.processor.image_processor(image, return_tensors=\"pt\").pixel_values\n",
    "            labels = self.processor.tokenizer(\n",
    "                text,\n",
    "                padding=\"max_length\",\n",
    "                max_length=self.max_target_length,\n",
    "                truncation=True,\n",
    "                return_tensors=\"pt\"\n",
    "            ).input_ids\n",
    "            \n",
    "            # Combine in a dict\n",
    "            encoding = {\n",
    "                \"pixel_values\": pixel_values.squeeze(),\n",
    "                \"labels\": labels.squeeze()\n",
    "            }\n",
    "            return encoding\n",
    "        else:\n",
    "            return image, text\n",
    "\n",
    "# TrOCR implementation\n",
    "def setup_trocr():\n",
    "    \"\"\"Set up TrOCR model, processor, and tokenizer\"\"\"\n",
    "    try:\n",
    "        # First check for sentencepiece\n",
    "        try:\n",
    "            import sentencepiece\n",
    "        except ImportError:\n",
    "            print(\"Installing sentencepiece...\")\n",
    "            import subprocess\n",
    "            subprocess.check_call(['pip', 'install', 'sentencepiece'])\n",
    "            import sentencepiece\n",
    "            \n",
    "        # Import necessary components\n",
    "        from transformers import TrOCRProcessor, VisionEncoderDecoderModel\n",
    "        \n",
    "        print(\"Loading TrOCR model and processor...\")\n",
    "        \n",
    "        # Load model and processor\n",
    "        processor = TrOCRProcessor.from_pretrained(\"microsoft/trocr-small-handwritten\")\n",
    "        model = VisionEncoderDecoderModel.from_pretrained(\"microsoft/trocr-small-handwritten\")\n",
    "        \n",
    "        # Configure for generation\n",
    "        model.config.decoder_start_token_id = processor.tokenizer.cls_token_id\n",
    "        model.config.pad_token_id = processor.tokenizer.pad_token_id\n",
    "        model.config.vocab_size = model.config.decoder.vocab_size\n",
    "        \n",
    "        # Set decoding parameters\n",
    "        model.config.eos_token_id = processor.tokenizer.sep_token_id\n",
    "        model.config.max_length = 64\n",
    "        model.config.early_stopping = True\n",
    "        model.config.no_repeat_ngram_size = 3\n",
    "        model.config.length_penalty = 2.0\n",
    "        model.config.num_beams = 4\n",
    "        \n",
    "        return model, processor\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error setting up TrOCR: {e}\")\n",
    "        return None, None\n",
    "\n",
    "# Create datasets and dataloaders\n",
    "def create_dataloaders(processor, batch_sizes=(16, 32, 32)):\n",
    "    \"\"\"Create datasets and dataloaders for training, validation and testing\"\"\"\n",
    "    # Create datasets - NO CUSTOM TRANSFORMS\n",
    "    train_dataset = BalineseOCRDataset(\n",
    "        train_data, \n",
    "        images_dir, \n",
    "        processor=processor\n",
    "    )\n",
    "    \n",
    "    val_dataset = BalineseOCRDataset(\n",
    "        val_data, \n",
    "        images_dir, \n",
    "        processor=processor\n",
    "    )\n",
    "    \n",
    "    test_dataset = BalineseOCRDataset(\n",
    "        test_data, \n",
    "        images_dir, \n",
    "        processor=processor\n",
    "    )\n",
    "    \n",
    "    # Create dataloaders with custom collate function\n",
    "    train_batch_size, val_batch_size, test_batch_size = batch_sizes\n",
    "    \n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=train_batch_size, \n",
    "        shuffle=True, \n",
    "        num_workers=2,\n",
    "        collate_fn=collate_fn\n",
    "    )\n",
    "    \n",
    "    val_dataloader = DataLoader(\n",
    "        val_dataset, \n",
    "        batch_size=val_batch_size, \n",
    "        shuffle=False, \n",
    "        num_workers=2,\n",
    "        collate_fn=collate_fn\n",
    "    )\n",
    "    \n",
    "    test_dataloader = DataLoader(\n",
    "        test_dataset, \n",
    "        batch_size=test_batch_size, \n",
    "        shuffle=False, \n",
    "        num_workers=2,\n",
    "        collate_fn=collate_fn\n",
    "    )\n",
    "    \n",
    "    return train_dataloader, val_dataloader, test_dataloader\n",
    "\n",
    "# Training function\n",
    "def train_trocr(model, processor, train_dataloader, val_dataloader, num_epochs=3, save_path=\"trocr_balinese\"):\n",
    "    \"\"\"Train TrOCR model on Balinese data\"\"\"\n",
    "    # Move model to device\n",
    "    model.to(device)\n",
    "    \n",
    "    # Set up optimizer\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=5e-5)\n",
    "    \n",
    "    # Set up scheduler (linear warmup then decay)\n",
    "    from transformers import get_linear_schedule_with_warmup\n",
    "    num_training_steps = len(train_dataloader) * num_epochs\n",
    "    num_warmup_steps = num_training_steps // 10\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer, \n",
    "        num_warmup_steps=num_warmup_steps, \n",
    "        num_training_steps=num_training_steps\n",
    "    )\n",
    "    \n",
    "    # Training loop\n",
    "    best_val_loss = float('inf')\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        \n",
    "        print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "        \n",
    "        for batch_idx, batch in enumerate(train_dataloader):\n",
    "            # Extract inputs and move to device\n",
    "            pixel_values = batch[\"pixel_values\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(pixel_values=pixel_values, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            \n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            \n",
    "            if (batch_idx + 1) % 50 == 0:\n",
    "                print(f\"Batch {batch_idx+1}/{len(train_dataloader)}, Loss: {loss.item():.4f}\")\n",
    "        \n",
    "        avg_train_loss = train_loss / len(train_dataloader)\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_idx, batch in enumerate(val_dataloader):\n",
    "                pixel_values = batch[\"pixel_values\"].to(device)\n",
    "                labels = batch[\"labels\"].to(device)\n",
    "                \n",
    "                outputs = model(pixel_values=pixel_values, labels=labels)\n",
    "                loss = outputs.loss\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "        \n",
    "        avg_val_loss = val_loss / len(val_dataloader)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}: Train Loss = {avg_train_loss:.4f}, Val Loss = {avg_val_loss:.4f}\")\n",
    "        \n",
    "        # Save best model\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            model.save_pretrained(f\"{save_path}_best\")\n",
    "            print(f\"Saved best model at epoch {epoch+1}\")\n",
    "    \n",
    "    # Save final model\n",
    "    model.save_pretrained(save_path)\n",
    "    processor.save_pretrained(save_path)\n",
    "    \n",
    "    return model, processor\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate_trocr(model, processor, test_dataloader):\n",
    "    \"\"\"Evaluate TrOCR model and compute CER\"\"\"\n",
    "    model.eval()\n",
    "    total_cer = 0\n",
    "    total_samples = 0\n",
    "    predictions = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch in enumerate(test_dataloader):\n",
    "            # Get images\n",
    "            pixel_values = batch[\"pixel_values\"].to(device)\n",
    "            \n",
    "            # Generate text\n",
    "            generated_ids = model.generate(pixel_values)\n",
    "            \n",
    "            # Decode text\n",
    "            generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "            \n",
    "            # Get ground truth\n",
    "            labels = batch[\"labels\"]\n",
    "            ground_truth = processor.batch_decode(labels, skip_special_tokens=True)\n",
    "            \n",
    "            # Compute CER\n",
    "            for pred, gt in zip(generated_text, ground_truth):\n",
    "                edit_distance = editdistance.eval(pred, gt)\n",
    "                cer = edit_distance / max(len(gt), 1)\n",
    "                \n",
    "                total_cer += cer\n",
    "                total_samples += 1\n",
    "                \n",
    "                predictions.append({\n",
    "                    \"prediction\": pred,\n",
    "                    \"ground_truth\": gt,\n",
    "                    \"cer\": cer\n",
    "                })\n",
    "    \n",
    "    # Calculate average CER\n",
    "    avg_cer = total_cer / total_samples if total_samples > 0 else 0\n",
    "    \n",
    "    print(f\"\\nTest CER: {avg_cer:.4f}\")\n",
    "    \n",
    "    # Display worst examples\n",
    "    predictions.sort(key=lambda x: x[\"cer\"], reverse=True)\n",
    "    print(\"\\nWorst 5 predictions:\")\n",
    "    for i, pred in enumerate(predictions[:5]):\n",
    "        print(f\"{i+1}. Pred: '{pred['prediction']}' | GT: '{pred['ground_truth']}' | CER: {pred['cer']:.4f}\")\n",
    "    \n",
    "    # Save results to CSV\n",
    "    csv_file = \"test_cer_results.csv\"\n",
    "    if os.path.exists(csv_file):\n",
    "        df = pd.read_csv(csv_file)\n",
    "    else:\n",
    "        df = pd.DataFrame(columns=[\"model_name\", \"test_cer\"])\n",
    "    \n",
    "    model_name = \"trocr-small-handwritten\"\n",
    "    if model_name in df[\"model_name\"].values:\n",
    "        df.loc[df[\"model_name\"] == model_name, \"test_cer\"] = avg_cer\n",
    "    else:\n",
    "        new_row = pd.DataFrame({\"model_name\": [model_name], \"test_cer\": [avg_cer]})\n",
    "        df = pd.concat([df, new_row], ignore_index=True)\n",
    "    \n",
    "    df.to_csv(csv_file, index=False)\n",
    "    print(f\"Results saved to {csv_file}\")\n",
    "    \n",
    "    return avg_cer, predictions\n",
    "\n",
    "# Main execution function\n",
    "def run_trocr_pipeline(num_epochs=3, batch_size=8):\n",
    "    \"\"\"Run complete TrOCR pipeline from setup to evaluation\"\"\"\n",
    "    try:\n",
    "        # Clean up before starting\n",
    "        cleanup()\n",
    "        \n",
    "        # Set up TrOCR\n",
    "        model, processor = setup_trocr()\n",
    "        \n",
    "        if not model or not processor:\n",
    "            raise RuntimeError(\"Failed to initialize TrOCR model or processor\")\n",
    "        \n",
    "        # Create dataloaders\n",
    "        train_dataloader, val_dataloader, test_dataloader = create_dataloaders(\n",
    "            processor, \n",
    "            batch_sizes=(batch_size, batch_size, batch_size)\n",
    "        )\n",
    "        \n",
    "        # Train model\n",
    "        model, processor = train_trocr(\n",
    "            model, \n",
    "            processor, \n",
    "            train_dataloader, \n",
    "            val_dataloader, \n",
    "            num_epochs=num_epochs, \n",
    "            save_path=\"trocr_balinese\"\n",
    "        )\n",
    "        \n",
    "        # Evaluate model\n",
    "        cer, predictions = evaluate_trocr(model, processor, test_dataloader)\n",
    "        \n",
    "        # Final cleanup\n",
    "        cleanup()\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error in TrOCR pipeline: {e}\")\n",
    "        cleanup()\n",
    "        return False\n",
    "\n",
    "# Run the pipeline if script is executed directly\n",
    "if __name__ == \"__main__\":\n",
    "    run_trocr_pipeline(num_epochs=15, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d734ddbf-dd33-4519-bbd8-8977a3fe25e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
