{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61ebf14-0121-4af5-9ba6-1486f71d9994",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import editdistance\n",
    "import time \n",
    "\n",
    "\n",
    "from PIL import Image\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "import timm  \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from transformers import TrOCRProcessor, VisionEncoderDecoderModel\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "base_dir = os.getcwd()\n",
    "\n",
    "# Same paths as your original code\n",
    "ground_truth_path = os.path.join(base_dir, 'balinese_transliteration_train.txt') \n",
    "images_dir        = os.path.join(base_dir, 'balinese_word_train')\n",
    "\n",
    "filenames = []\n",
    "labels    = []\n",
    "\n",
    "with open(ground_truth_path, 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        line = line.strip()\n",
    "        if line:  # Ensure the line is not empty\n",
    "            parts = line.split(';')\n",
    "            if len(parts) == 2:\n",
    "                filename, label = parts\n",
    "                label = label.lower()\n",
    "                filenames.append(filename)\n",
    "                labels.append(label)\n",
    "            else:\n",
    "                print(f\"Skipping malformed line: {line}\")\n",
    "\n",
    "data = pd.DataFrame({\n",
    "    'filename': filenames,\n",
    "    'label': labels\n",
    "})\n",
    "\n",
    "label_counts = data['label'].value_counts()\n",
    "\n",
    "all_text = ''.join(data['label'])\n",
    "unique_chars = sorted(list(set(all_text)))\n",
    "\n",
    "# Create character->index starting from 1\n",
    "char_to_idx = {char: idx + 1 for idx, char in enumerate(unique_chars)}\n",
    "# Add special tokens\n",
    "char_to_idx['<PAD>'] = 0\n",
    "char_to_idx['<UNK>'] = len(char_to_idx)\n",
    "char_to_idx['<SOS>'] = len(char_to_idx)\n",
    "char_to_idx['<EOS>'] = len(char_to_idx)\n",
    "\n",
    "# Reverse mapping\n",
    "idx_to_char = {v: k for k, v in char_to_idx.items()}\n",
    "\n",
    "vocab_size = len(char_to_idx)\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "\n",
    "def encode_label(label, char_to_idx, max_length):\n",
    "    \"\"\"\n",
    "    Converts a label (string) into a list of indices with <SOS>, <EOS>, padding, etc.\n",
    "    \"\"\"\n",
    "    encoded = (\n",
    "        [char_to_idx['<SOS>']] +\n",
    "        [char_to_idx.get(ch, char_to_idx['<UNK>']) for ch in label] +\n",
    "        [char_to_idx['<EOS>']]\n",
    "    )\n",
    "    # Pad if needed\n",
    "    if len(encoded) < max_length:\n",
    "        encoded += [char_to_idx['<PAD>']] * (max_length - len(encoded))\n",
    "    else:\n",
    "        encoded = encoded[:max_length]\n",
    "    return encoded\n",
    "\n",
    "max_label_length = max(len(label) for label in data['label']) + 2  # +2 for <SOS> and <EOS>\n",
    "data['encoded_label'] = data['label'].apply(lambda x: encode_label(x, char_to_idx, max_label_length))\n",
    "data['label_length']  = data['label'].apply(len)\n",
    "\n",
    "rare_labels = label_counts[label_counts < 3].index  # NEW: words that appear <3 times\n",
    "\n",
    "def custom_split(df, rare_label_list, test_size=0.1, random_state=42):\n",
    "    # Separate rare words from frequent ones\n",
    "    rare_df     = df[df['label'].isin(rare_label_list)]\n",
    "    non_rare_df = df[~df['label'].isin(rare_label_list)]\n",
    "\n",
    "    #  train/val split for non-rare\n",
    "    train_nr, val_nr = train_test_split(non_rare_df, test_size=test_size, \n",
    "                                        random_state=random_state)\n",
    "\n",
    "    # Combine rare samples entirely into training\n",
    "    train_df = pd.concat([train_nr, rare_df], ignore_index=True)\n",
    "    # Shuffle after combining\n",
    "    train_df = train_df.sample(frac=1, random_state=random_state).reset_index(drop=True)\n",
    "\n",
    "    val_df = val_nr.reset_index(drop=True)\n",
    "    return train_df, val_df\n",
    "\n",
    "# Call custom_split instead of direct train_test_split\n",
    "train_data, val_data = custom_split(data, rare_labels, test_size=0.1, random_state=42) \n",
    "\n",
    "print(f\"Training size: {len(train_data)}; Validation size: {len(val_data)}\")\n",
    "Vocabulary size: 39\n",
    "Training size: 13972; Validation size: 1050\n",
    "class BalineseDataset(Dataset):\n",
    "    def __init__(self, df, images_dir, transform=None):\n",
    "        self.data       = df.reset_index(drop=True)\n",
    "        self.images_dir = images_dir\n",
    "        self.transform  = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name     = self.data.loc[idx, 'filename']\n",
    "        label        = self.data.loc[idx, 'encoded_label']\n",
    "        label_length = self.data.loc[idx, 'label_length']\n",
    "\n",
    "        img_path = os.path.join(self.images_dir, img_name)\n",
    "        image    = Image.open(img_path).convert('RGB')\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        label = torch.tensor(label, dtype=torch.long)\n",
    "        return image, label, torch.tensor(label_length, dtype=torch.long)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=(0.5, 0.5, 0.5),\n",
    "        std=(0.5, 0.5, 0.5)\n",
    "    )\n",
    "])\n",
    "\n",
    "train_dataset = BalineseDataset(train_data, images_dir, transform=transform)\n",
    "val_dataset   = BalineseDataset(val_data,   images_dir, transform=transform)\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,  num_workers=2, pin_memory=True)\n",
    "val_loader   = DataLoader(val_dataset,   batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a785f311-2a23-4aa8-b1db-7b9ac375fce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageCaptioningTrainer:\n",
    "    def __init__(self, encoder, decoder, \n",
    "                 criterion, encoder_optimizer, decoder_optimizer, \n",
    "                 train_loader, val_loader, device, \n",
    "                 char_to_idx, idx_to_char, max_label_length,\n",
    "                 model_name, csv_filename=\"training_results.csv\"):\n",
    "        self.encoder = encoder.to(device)\n",
    "        self.decoder             = decoder.to(device)\n",
    "        self.criterion           = criterion\n",
    "        self.encoder_optimizer   = encoder_optimizer\n",
    "        self.decoder_optimizer   = decoder_optimizer\n",
    "        self.train_loader        = train_loader\n",
    "        self.val_loader          = val_loader\n",
    "        self.device              = device\n",
    "        self.char_to_idx         = char_to_idx\n",
    "        self.idx_to_char         = idx_to_char\n",
    "        self.max_label_length    = max_label_length\n",
    "        self.model_name = model_name\n",
    "        self.csv_filename = csv_filename\n",
    "\n",
    "        self.train_losses = []\n",
    "        self.val_losses   = []\n",
    "\n",
    "        self.train_cers   = []\n",
    "        self.val_cers     = []\n",
    "\n",
    "    def fit(self, num_epochs):\n",
    "        start_time = time.time()\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
    "            train_loss, train_cer = self.train_one_epoch()\n",
    "            val_loss,   val_cer   = self.validate_one_epoch(top_n=5)\n",
    "\n",
    "            print(f\"[{epoch+1}/{num_epochs}] \"\n",
    "                  f\"Train Loss: {train_loss:.4f}, Train CER: {train_cer:.4f} | \"\n",
    "                  f\"Val Loss: {val_loss:.4f}, Val CER: {val_cer:.4f}\")\n",
    "\n",
    "            # Store epoch results\n",
    "            self.train_losses.append(train_loss)\n",
    "            self.val_losses.append(val_loss)\n",
    "            self.train_cers.append(train_cer)\n",
    "            self.val_cers.append(val_cer)\n",
    "            \n",
    "        \n",
    "        # Calculate total training time\n",
    "        end_time = time.time() \n",
    "        total_time = end_time - start_time\n",
    "        hours = int(total_time // 3600)\n",
    "        minutes = int((total_time % 3600) // 60)\n",
    "\n",
    "        print(f\"\\nTraining completed in {hours}h {minutes}m.\")\n",
    "\n",
    "        num_epochs = len(self.train_losses)\n",
    "        epoch_cols = [f\"epoch{i+1}\" for i in range(num_epochs)]\n",
    "\n",
    "        # Create the new data block to insert\n",
    "        new_rows = pd.DataFrame([\n",
    "            [self.model_name, \"training loss\"] + self.train_losses,\n",
    "            [self.model_name, \"validation loss\"] + self.val_losses,\n",
    "            [self.model_name, \"training cer\"] + self.train_cers,\n",
    "            [self.model_name, \"validation cer\"] + self.val_cers\n",
    "        ], columns=[\"model_name\", \"mode\"] + epoch_cols)\n",
    "        \n",
    "        # Check if CSV already exists\n",
    "        if os.path.exists(self.csv_filename):\n",
    "            df_existing = pd.read_csv(self.csv_filename)\n",
    "            df_existing = df_existing[df_existing[\"model_name\"] != self.model_name]\n",
    "            df_updated = pd.concat([df_existing, new_rows], ignore_index=True)\n",
    "        else:\n",
    "            df_updated = new_rows\n",
    "        \n",
    "        # Save the updated CSV\n",
    "        df_updated.to_csv(self.csv_filename, index=False)\n",
    "        print(f\"\\nResults have been written to: {self.csv_filename}\")\n",
    "\n",
    "\n",
    "        # Save model weights\n",
    "        # torch.save(self.encoder.state_dict(), f\"encoder_{self.model_name}.pth\")\n",
    "        # torch.save(self.decoder.state_dict(), f\"decoder_{self.model_name}.pth\")\n",
    "        # print(f\"Encoder and decoder models saved: encoder_{self.model_name}.pth, decoder_{self.model_name}.pth\")\n",
    "        \n",
    "\n",
    "    def train_one_epoch(self):\n",
    "        self.encoder.train()\n",
    "        self.decoder.train()\n",
    "        running_loss           = 0.0\n",
    "        total_edit_distance    = 0\n",
    "        total_ref_length       = 0\n",
    "\n",
    "        for batch_idx, (images, labels, label_lengths) in enumerate(self.train_loader):\n",
    "            images        = images.to(self.device, non_blocking=True)\n",
    "            labels        = labels.to(self.device, non_blocking=True)\n",
    "            label_lengths = label_lengths.to(self.device, non_blocking=True)\n",
    "\n",
    "            self.encoder_optimizer.zero_grad()\n",
    "            self.decoder_optimizer.zero_grad()\n",
    "\n",
    "            encoder_out   = self.encoder(images)\n",
    "            caption_lengths = torch.tensor(\n",
    "                [self.max_label_length] * labels.size(0)\n",
    "            ).unsqueeze(1).to(self.device)\n",
    "\n",
    "            outputs, encoded_captions, decode_lengths, alphas, sort_ind = self.decoder(\n",
    "                encoder_out, labels, caption_lengths\n",
    "            )\n",
    "\n",
    "            # Targets = encoded captions without the <SOS>\n",
    "            targets = encoded_captions[:, 1:]\n",
    "\n",
    "            # Flatten for loss\n",
    "            outputs_flat = outputs.view(-1, self.decoder.fc.out_features)\n",
    "            targets_flat = targets.contiguous().view(-1)\n",
    "\n",
    "            loss = self.criterion(outputs_flat, targets_flat)\n",
    "            loss.backward()\n",
    "\n",
    "            self.decoder_optimizer.step()\n",
    "            self.encoder_optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            # Compute CER for the batch (global style)\n",
    "            batch_size = labels.size(0)\n",
    "            _, preds_flat = torch.max(outputs_flat, dim=1)\n",
    "            preds_seq = preds_flat.view(batch_size, -1)\n",
    "\n",
    "            for i in range(batch_size):\n",
    "                pred_indices   = preds_seq[i].detach().cpu().numpy()\n",
    "                target_indices = targets[i].detach().cpu().numpy()\n",
    "\n",
    "                mask          = (target_indices != self.char_to_idx['<PAD>'])\n",
    "                pred_indices  = pred_indices[mask]\n",
    "                target_indices= target_indices[mask]\n",
    "\n",
    "                pred_chars    = [self.idx_to_char.get(idx, '') for idx in pred_indices]\n",
    "                target_chars  = [self.idx_to_char.get(idx, '') for idx in target_indices]\n",
    "                pred_str      = ''.join(pred_chars)\n",
    "                target_str    = ''.join(target_chars)\n",
    "\n",
    "                edit_dist           = editdistance.eval(pred_str, target_str)\n",
    "                total_edit_distance += edit_dist\n",
    "                total_ref_length    += len(target_str)\n",
    "\n",
    "            # if (batch_idx + 1) % 50 == 0:\n",
    "            #     print(f'Batch {batch_idx + 1}/{len(self.train_loader)} - Loss: {loss.item():.4f}')\n",
    "\n",
    "        avg_loss = running_loss / len(self.train_loader)\n",
    "        avg_cer  = total_edit_distance / total_ref_length if total_ref_length > 0 else 0.0\n",
    "        return avg_loss, avg_cer\n",
    "\n",
    "    def validate_one_epoch(self, top_n=5):\n",
    "        self.encoder.eval()\n",
    "        self.decoder.eval()\n",
    "        running_loss         = 0.0\n",
    "        total_edit_distance  = 0\n",
    "        total_ref_length     = 0\n",
    "\n",
    "        # each sample’s CER\n",
    "        sample_cer_info = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (images, labels, label_lengths) in enumerate(self.val_loader):\n",
    "                images        = images.to(self.device, non_blocking=True)\n",
    "                labels        = labels.to(self.device, non_blocking=True)\n",
    "                label_lengths = label_lengths.to(self.device, non_blocking=True)\n",
    "\n",
    "                encoder_out = self.encoder(images)\n",
    "                caption_lengths = torch.tensor(\n",
    "                    [self.max_label_length] * labels.size(0)\n",
    "                ).unsqueeze(1).to(self.device)\n",
    "\n",
    "                outputs, encoded_captions, decode_lengths, alphas, sort_ind = self.decoder(\n",
    "                    encoder_out, labels, caption_lengths\n",
    "                )\n",
    "                targets = encoded_captions[:, 1:]\n",
    "\n",
    "                outputs_flat = outputs.view(-1, self.decoder.fc.out_features)\n",
    "                targets_flat = targets.contiguous().view(-1)\n",
    "\n",
    "                loss = self.criterion(outputs_flat, targets_flat)\n",
    "                running_loss += loss.item()\n",
    "\n",
    "                batch_size = labels.size(0)\n",
    "                _, preds_flat = torch.max(outputs_flat, dim=1)\n",
    "                preds_seq = preds_flat.view(batch_size, -1)\n",
    "\n",
    "                for i in range(batch_size):\n",
    "                    pred_indices   = preds_seq[i].detach().cpu().numpy()\n",
    "                    target_indices = targets[i].detach().cpu().numpy()\n",
    "\n",
    "                    mask           = (target_indices != self.char_to_idx['<PAD>'])\n",
    "                    pred_indices   = pred_indices[mask]\n",
    "                    target_indices = target_indices[mask]\n",
    "\n",
    "                    pred_chars   = [self.idx_to_char.get(idx, '') for idx in pred_indices]\n",
    "                    target_chars = [self.idx_to_char.get(idx, '') for idx in target_indices]\n",
    "                    pred_str     = ''.join(pred_chars)\n",
    "                    target_str   = ''.join(target_chars)\n",
    "\n",
    "                    edit_dist = editdistance.eval(pred_str, target_str)\n",
    "                    ref_len   = len(target_str)\n",
    "                    cer       = edit_dist / ref_len if ref_len > 0 else 0\n",
    "    \n",
    "                    total_edit_distance += edit_dist\n",
    "                    total_ref_length    += ref_len\n",
    "    \n",
    "                    # Store sample info\n",
    "                    # sample_cer_info.append({\n",
    "                    #     \"pred\": pred_str,\n",
    "                    #     \"gt\": target_str,\n",
    "                    #     \"cer\": cer\n",
    "                    # })\n",
    "\n",
    "                    # Print a few samples from the 1st batch\n",
    "                    # if batch_idx == 0 and i < 3:\n",
    "                    #     print(f\"Sample {i + 1}:\")\n",
    "                    #     print(f\"Predicted: {pred_str}\")\n",
    "                    #     print(f\"Target   : {target_str}\\n\")\n",
    "\n",
    "        avg_loss = running_loss / len(self.val_loader)\n",
    "        avg_cer  = total_edit_distance / total_ref_length if total_ref_length > 0 else 0.0\n",
    "\n",
    "        # Sort by CER descending\n",
    "        sample_cer_info.sort(key=lambda x: x[\"cer\"], reverse=True)\n",
    "        # Take top_n\n",
    "        worst_samples = sample_cer_info[:top_n]\n",
    "    \n",
    "        # print(f\"\\n=== Top {top_n} Worst Samples by CER ===\")\n",
    "        # for idx, sample in enumerate(worst_samples):\n",
    "        #     print(f\"[{idx+1}] CER: {sample['cer']:.3f}\")\n",
    "        #     print(f\"   Predicted: {sample['pred']}\")\n",
    "        #     print(f\"   Ground Truth: {sample['gt']}\\n\")\n",
    "       \n",
    "        return avg_loss, avg_cer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d9b2b6-c2a1-4d12-ade5-4f2eefe4a46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ground_truth_path = os.path.join(base_dir, 'balinese_transliteration_test.txt')\n",
    "test_images_dir        = os.path.join(base_dir, 'balinese_word_test')\n",
    "\n",
    "test_filenames = []\n",
    "test_labels    = []\n",
    "\n",
    "with open(test_ground_truth_path, 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        line = line.strip()\n",
    "        if line:\n",
    "            parts = line.split(';')\n",
    "            if len(parts) == 2:\n",
    "                filename, label = parts\n",
    "                label = label.lower()\n",
    "                test_filenames.append(filename)\n",
    "                test_labels.append(label)\n",
    "            else:\n",
    "                print(f\"Skipping malformed line: {line}\")\n",
    "\n",
    "test_data = pd.DataFrame({\n",
    "    'filename': test_filenames,\n",
    "    'label': test_labels\n",
    "})\n",
    "\n",
    "# Check for unknown chars in test set\n",
    "test_chars = set(''.join(test_data['label']))\n",
    "unknown_chars = test_chars - set(char_to_idx.keys())\n",
    "print(f\"Unknown characters in test labels: {unknown_chars}\")\n",
    "\n",
    "# Encode test labels\n",
    "max_label_length_test = max(len(lbl) for lbl in test_data['label']) + 2\n",
    "def encode_label_test(label, char_to_idx, max_length):\n",
    "    encoded = (\n",
    "        [char_to_idx['<SOS>']] +\n",
    "        [char_to_idx.get(ch, char_to_idx['<UNK>']) for ch in label] +\n",
    "        [char_to_idx['<EOS>']]\n",
    "    )\n",
    "    if len(encoded) > max_length:\n",
    "        encoded = encoded[:max_length]\n",
    "    else:\n",
    "        encoded += [char_to_idx['<PAD>']] * (max_length - len(encoded))\n",
    "    return encoded\n",
    "\n",
    "test_data['encoded_label'] = test_data['label'].apply(lambda x: encode_label_test(x, char_to_idx, max_label_length_test))\n",
    "test_data['label_length']  = test_data['label'].apply(len)\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=(0.5, 0.5, 0.5),\n",
    "        std=(0.5, 0.5, 0.5)\n",
    "    )\n",
    "])\n",
    "\n",
    "test_dataset = BalineseDataset(test_data, test_images_dir, transform=test_transform)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=0)\n",
    "Unknown characters in test labels: set()\n",
    "def inference(encoder, decoder, data_loader, device, char_to_idx, idx_to_char, max_seq_length, test_data):\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "\n",
    "    eos_idx = char_to_idx['<EOS>']\n",
    "    results = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (images, labels, label_lengths) in enumerate(data_loader):\n",
    "            images = images.to(device, non_blocking=True)\n",
    "            labels = labels.to(device, non_blocking=True)\n",
    "\n",
    "            batch_size  = images.size(0)\n",
    "            encoder_out = encoder(images)  # [B, num_patches, encoder_dim]\n",
    "\n",
    "            # Init LSTM state\n",
    "            h1, c1, h2, c2 = decoder.init_hidden_state(encoder_out)\n",
    "\n",
    "            # Start tokens (all <SOS>)\n",
    "            inputs = torch.full(\n",
    "                (batch_size,),\n",
    "                fill_value=char_to_idx['<SOS>'],\n",
    "                dtype=torch.long,\n",
    "                device=device\n",
    "            )\n",
    "\n",
    "            all_preds = []\n",
    "\n",
    "            for _ in range(max_seq_length):\n",
    "                # Embedding\n",
    "                embeddings = decoder.embedding(inputs)\n",
    "\n",
    "                # Attention\n",
    "                attention_weighted_encoding, _ = decoder.attention(encoder_out, h1)\n",
    "\n",
    "                # Gating\n",
    "                gate = decoder.sigmoid(decoder.f_beta(h1))\n",
    "                attention_weighted_encoding = gate * attention_weighted_encoding\n",
    "\n",
    "                # Pass through LSTM layers\n",
    "                h1, c1 = decoder.lstm1(\n",
    "                    torch.cat([embeddings, attention_weighted_encoding], dim=1),\n",
    "                    (h1, c1)\n",
    "                )\n",
    "                h2, c2 = decoder.lstm2(h1, (h2, c2))\n",
    "\n",
    "                # Get predicted token\n",
    "                preds = decoder.fc(decoder.dropout(h2))  # [batch_size, vocab_size]\n",
    "                _, preds_idx = preds.max(dim=1)\n",
    "\n",
    "                # Feed next token\n",
    "                all_preds.append(preds_idx.cpu().numpy())\n",
    "                inputs = preds_idx\n",
    "\n",
    "            # Reformat predictions to [batch_size, max_seq_length]\n",
    "            all_preds = np.array(all_preds).T\n",
    "\n",
    "            for i in range(batch_size):\n",
    "                pred_indices = all_preds[i]\n",
    "\n",
    "                # Stop at <EOS> if present\n",
    "                if eos_idx in pred_indices:\n",
    "                    first_eos = np.where(pred_indices == eos_idx)[0][0]\n",
    "                    pred_indices = pred_indices[:first_eos]\n",
    "\n",
    "                # Convert token indices -> string\n",
    "                pred_chars = [idx_to_char.get(idx, '') for idx in pred_indices]\n",
    "                pred_str   = ''.join(pred_chars)\n",
    "\n",
    "                # Process ground truth\n",
    "                label_indices = labels[i].cpu().numpy()\n",
    "                # remove <SOS>\n",
    "                label_indices = label_indices[1:]\n",
    "\n",
    "                if eos_idx in label_indices:\n",
    "                    eos_pos = np.where(label_indices == eos_idx)[0][0]\n",
    "                    label_indices = label_indices[:eos_pos]\n",
    "                else:\n",
    "                    # remove <PAD> if present\n",
    "                    label_indices = label_indices[label_indices != char_to_idx['<PAD>']]\n",
    "\n",
    "                label_chars = [idx_to_char.get(idx, '') for idx in label_indices]\n",
    "                label_str   = ''.join(label_chars)\n",
    "\n",
    "                global_idx    = batch_idx * batch_size + i\n",
    "                image_filename= test_data.iloc[global_idx]['filename']\n",
    "\n",
    "                results.append({\n",
    "                    'image_filename': image_filename,\n",
    "                    'predicted_caption': pred_str,\n",
    "                    'ground_truth_caption': label_str\n",
    "                })\n",
    "\n",
    "    return results\n",
    "def calculate_global_cer(results):\n",
    "    total_ed   = 0\n",
    "    total_refs = 0\n",
    "    for r in results:\n",
    "        ref = r['ground_truth_caption']\n",
    "        hyp = r['predicted_caption']\n",
    "        dist = editdistance.eval(ref, hyp)\n",
    "        total_ed   += dist\n",
    "        total_refs += len(ref)\n",
    "    if total_refs == 0:\n",
    "        return 0.0\n",
    "    return total_ed / total_refs\n",
    "def print_top_worst_samples(results, n=5):\n",
    "    # Calculate CER for each sample\n",
    "    results_with_cer = []\n",
    "    for r in results:\n",
    "        ref = r['ground_truth_caption']\n",
    "        hyp = r['predicted_caption']\n",
    "        dist = editdistance.eval(ref, hyp)\n",
    "        length = len(ref)\n",
    "        cer = dist / length if length > 0 else 0\n",
    "        # Copy the record and add cer\n",
    "        new_r = r.copy()\n",
    "        new_r['cer'] = cer\n",
    "        results_with_cer.append(new_r)\n",
    "\n",
    "    # Sort by CER (descending) and take the top N\n",
    "    results_with_cer.sort(key=lambda x: x['cer'], reverse=True)\n",
    "    worst_samples = results_with_cer[:n]\n",
    "\n",
    "    print(f\"\\n=== Top {n} Worst Samples by CER ===\")\n",
    "    for i, sample in enumerate(worst_samples, start=1):\n",
    "        print(f\"{i}) Image: {sample['image_filename']}\")\n",
    "        print(f\"   CER: {sample['cer']:.4f}\")\n",
    "        print(f\"   Predicted       : {sample['predicted_caption']}\")\n",
    "        print(f\"   Ground Truth    : {sample['ground_truth_caption']}\")\n",
    "        print()\n",
    "training_csv = \"training_results.csv\"\n",
    "if not os.path.exists(training_csv) or os.path.getsize(training_csv) == 0:\n",
    "    pd.DataFrame(columns=[\"model_name\", \"mode\", \"epoch1\", \"epoch2\"]).to_csv(training_csv, index=False)\n",
    "\n",
    "csv_file = \"test_cer_results.csv\"\n",
    "if not os.path.exists(csv_file) or os.path.getsize(csv_file) == 0:\n",
    "    pd.DataFrame(columns=[\"model_name\", \"test_cer\"]).to_csv(csv_file, index=False)\n",
    "\n",
    "def log_test_cer(model_name, cer_value):\n",
    "    df = pd.read_csv(csv_file)\n",
    "    # Check if model_name exists\n",
    "    if model_name in df['model_name'].values:\n",
    "        # Update existing row\n",
    "        df.loc[df['model_name'] == model_name, 'test_cer'] = cer_value\n",
    "    else:\n",
    "        # Add new row - use concat instead of append\n",
    "        new_row = pd.DataFrame({\"model_name\": [model_name], \"test_cer\": [cer_value]})\n",
    "        df = pd.concat([df, new_row], ignore_index=True)\n",
    "    \n",
    "    df.to_csv(csv_file, index=False)\n",
    "    print(f\"Logged {model_name}: {cer_value:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
